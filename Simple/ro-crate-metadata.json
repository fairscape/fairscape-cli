{
  "@context": {
    "@vocab": "https://schema.org/",
    "EVI": "https://w3id.org/EVI#"
  },
  "@graph": [
    {
      "@id": "ro-crate-metadata.json",
      "conformsTo": {
        "@id": "https://w3id.org/ro/crate/1.1"
      },
      "about": {
        "@id": "ark:59852/rocrate-stats-test-20251113205955"
      },
      "@type": "CreativeWork"
    },
    {
      "name": "ROCrate: HF Models",
      "description": "Top 100 Downloaded Hugging Face Models as ROCrate",
      "author": "Justin",
      "keywords": [
        "hf",
        "ML"
      ],
      "version": "1.0.0",
      "license": "https://creativecommons.org/licenses/by/4.0/",
      "datePublished": null,
      "packageType": "pipeline",
      "autoComplete": true,
      "@id": "ark:59852/rocrate-stats-test-20251113205955",
      "isPartOf": [
        {
          "@id": "ark:59852/organization-uva"
        },
        {
          "@id": "ark:59852/project-cm4ai"
        }
      ],
      "@type": [
        "Dataset",
        "https://w3id.org/EVI#ROCrate"
      ],
      "hasPart": [
        {
          "@id": "ark:59852/model-sentence-transformers-all-minilm-l6-v2-vm3x80oxv2"
        },
        {
          "@id": "ark:59852/model-google-electra-base-discriminator-tsm97ogyfzb"
        },
        {
          "@id": "ark:59852/model-falconsai-nsfwimagedetection-pq8dfeakedj"
        },
        {
          "@id": "ark:59852/model-google-bert-bert-base-uncased-acvf1xltspx"
        },
        {
          "@id": "ark:59852/model-dima806-fairfaceageimagedetection-iodpan8vcmp"
        },
        {
          "@id": "ark:59852/model-timm-mobilenetv3small100-lambin1k-wgpq2k4fgr"
        },
        {
          "@id": "ark:59852/model-sentence-transformers-all-mpnet-base-v2-jflnukowl2l"
        },
        {
          "@id": "ark:59852/model-openai-clip-vit-base-patch32-n2hexj3zxqx"
        },
        {
          "@id": "ark:59852/model-pyannote-segmentation-3-0-314zxjfwtty"
        },
        {
          "@id": "ark:59852/model-facebookai-roberta-large-rhbckmcew8j"
        },
        {
          "@id": "ark:59852/model-laion-clap-htsat-fused-0djuztpt7f"
        },
        {
          "@id": "ark:59852/model-pyannote-wespeaker-voxceleb-resnet34-lm-n2hexj7ztqu"
        },
        {
          "@id": "ark:59852/model-sentence-transformers-paraphrase-multilingual-minilm-l12-v2-gbnva3jh0v"
        },
        {
          "@id": "ark:59852/model-pyannote-speaker-diarization-3-1-cejglftuic6"
        },
        {
          "@id": "ark:59852/model-bingsu-adetailer-vm3x80uxna"
        },
        {
          "@id": "ark:59852/model-distilbert-distilbert-base-uncased-cejglfruyy"
        },
        {
          "@id": "ark:59852/model-omni-research-tarsier2-recap-7b-evzs5nf69w"
        },
        {
          "@id": "ark:59852/model-openai-clip-vit-large-patch14-vlpktz7mnjv"
        },
        {
          "@id": "ark:59852/model-openai-community-gpt2-tsm97owyvvw"
        },
        {
          "@id": "ark:59852/model-colbert-ir-colbertv2-0-wgpq2kzfrly"
        },
        {
          "@id": "ark:59852/model-facebookai-roberta-base-1otynrz79gx"
        },
        {
          "@id": "ark:59852/model-jonatasgrosman-wav2vec2-large-xlsr-53-russian-s6vypbl8gby"
        },
        {
          "@id": "ark:59852/model-qwen-qwen2-5-vl-3b-instruct-rhbckmkeqp"
        },
        {
          "@id": "ark:59852/model-jonatasgrosman-wav2vec2-large-xlsr-53-chinese-zh-cn-vlpktzymnj1"
        },
        {
          "@id": "ark:59852/model-qwen-qwen2-5-7b-instruct-586jghlqcbj"
        },
        {
          "@id": "ark:59852/model-facebook-contriever-qrgauvkai00"
        },
        {
          "@id": "ark:59852/model-facebookai-xlm-roberta-base-xnwortvbhfy"
        },
        {
          "@id": "ark:59852/model-autogluon-chronos-bolt-small-6xmiqq6gbud"
        },
        {
          "@id": "ark:59852/model-sentence-transformers-gtr-t5-base-0djuzy4tek"
        },
        {
          "@id": "ark:59852/model-jonatasgrosman-wav2vec2-large-xlsr-53-japanese-jflnukuwxq"
        },
        {
          "@id": "ark:59852/model-baai-bge-m3-darh3ir4qvp"
        },
        {
          "@id": "ark:59852/model-jonatasgrosman-wav2vec2-large-xlsr-53-arabic-fuk3yq59zve"
        },
        {
          "@id": "ark:59852/model-qwen-qwen3-0-6b-jriwtxoazf"
        },
        {
          "@id": "ark:59852/model-cross-encoder-ms-marco-minilm-l6-v2-zyb6ofgc3g2"
        },
        {
          "@id": "ark:59852/model-usyd-community-vitpose-plus-base-jriwtxdazqj"
        },
        {
          "@id": "ark:59852/model-openai-gpt-oss-20b-1otyncx7no"
        },
        {
          "@id": "ark:59852/model-kijai-wanvideocomfy-1otynci7svy"
        },
        {
          "@id": "ark:59852/model-distilbert-distilbert-base-uncased-finetuned-sst-2-english-jflnukgwzr"
        },
        {
          "@id": "ark:59852/model-sentence-transformers-paraphrase-multilingual-mpnet-base-v2-n2hexw5z1s"
        },
        {
          "@id": "ark:59852/model-jonatasgrosman-wav2vec2-large-xlsr-53-portuguese-4tcbwvuopvu"
        },
        {
          "@id": "ark:59852/model-comfy-org-wan2-2comfyuirepackaged-acvf1ohtlae"
        },
        {
          "@id": "ark:59852/model-qwen-qwen2-5-3b-instruct-jflnukbwlhy"
        },
        {
          "@id": "ark:59852/model-google-bert-bert-base-multilingual-cased-yj9xpuhec6v"
        },
        {
          "@id": "ark:59852/model-qwen-qwen3-4b-instruct-2507-8gxrkmrrr4"
        },
        {
          "@id": "ark:59852/model-deepseek-ai-deepseek-ocr-iz2sih8d7md"
        },
        {
          "@id": "ark:59852/model-facebook-bart-large-cnn-wgpq25wfr0d"
        },
        {
          "@id": "ark:59852/model-coqui-xtts-v2-uhwhifb25n"
        },
        {
          "@id": "ark:59852/model-meta-llama-llama-3-1-8b-instruct-b9umck0jl7u"
        },
        {
          "@id": "ark:59852/model-jonatasgrosman-wav2vec2-large-xlsr-53-persian-6xmiq3igbqk"
        },
        {
          "@id": "ark:59852/model-zhihan1996-dnabert-s-uka1jryl9nb"
        },
        {
          "@id": "ark:59852/model-sentence-transformers-multi-qa-mpnet-base-dot-v1-fuk3yqj9zyu"
        },
        {
          "@id": "ark:59852/model-google-bert-bert-base-multilingual-uncased-n07phowqall"
        },
        {
          "@id": "ark:59852/model-baai-bge-large-en-v1-5-uka1jril9hn"
        },
        {
          "@id": "ark:59852/model-qwen-qwen3-embedding-0-6b-cejgl97uqez"
        },
        {
          "@id": "ark:59852/model-qwen-qwen2-5-1-5b-instruct-0djuzykth7"
        },
        {
          "@id": "ark:59852/model-jinaai-jina-embeddings-v3-314zxakwg5"
        },
        {
          "@id": "ark:59852/model-qwen-qwen3-8b-gbnvaw9hka"
        },
        {
          "@id": "ark:59852/model-openai-whisper-large-v3-fuk3yql9zm2"
        },
        {
          "@id": "ark:59852/model-autogluon-chronos-bolt-base-6xmiq3ggblp"
        },
        {
          "@id": "ark:59852/model-dphn-dolphin-2-9-1-yi-1-5-34b-2bfvdgvymxw"
        },
        {
          "@id": "ark:59852/model-comfy-org-wan2-1comfyuirepackaged-hfgzzplb5we"
        },
        {
          "@id": "ark:59852/model-cardiffnlp-twitter-roberta-base-sentiment-latest-s6vypzf8fb"
        },
        {
          "@id": "ark:59852/model-facebook-opt-125m-1otyncb7s4s"
        },
        {
          "@id": "ark:59852/model-openai-whisper-large-v3-turbo-minkyuyxjob"
        },
        {
          "@id": "ark:59852/model-nlpaueb-legal-bert-base-uncased-jriwtx1aar"
        },
        {
          "@id": "ark:59852/model-timm-resnet50-a1in1k-n07phoxq1k"
        },
        {
          "@id": "ark:59852/model-openai-gpt-oss-120b-xnwor1hbhdh"
        },
        {
          "@id": "ark:59852/model-openai-clip-vit-large-patch14-336-s6vypza8em"
        },
        {
          "@id": "ark:59852/model-qwen-qwen2-5-vl-7b-instruct-7jyeqs7dqf"
        },
        {
          "@id": "ark:59852/model-jonatasgrosman-wav2vec2-large-xlsr-53-dutch-ewfbnpbhgg"
        },
        {
          "@id": "ark:59852/model-mahmoudashraf-mms-300m-1130-forced-aligner-8gxrkn6r7va"
        },
        {
          "@id": "ark:59852/model-baai-bge-base-en-v1-5-kar49hskbb"
        },
        {
          "@id": "ark:59852/model-hexgrad-kokoro-82m-iodpacdvsyr"
        },
        {
          "@id": "ark:59852/model-hustvl-vitmatte-small-composition-1k-314zxtowtad"
        },
        {
          "@id": "ark:59852/model-amazon-chronos-2-s6vypmb8o1g"
        },
        {
          "@id": "ark:59852/model-baai-bge-small-en-v1-5-jflnu55wd1"
        },
        {
          "@id": "ark:59852/model-gensyn-qwen2-5-0-5b-instruct-7jyeqivdgmw"
        },
        {
          "@id": "ark:59852/model-alibaba-nlp-gte-large-en-v1-5-lnuashjixr"
        },
        {
          "@id": "ark:59852/model-meta-llama-llama-3-2-1b-instruct-jflnu5swde"
        },
        {
          "@id": "ark:59852/model-mistralai-mistral-7b-instruct-v0-2-1otynso79jy"
        },
        {
          "@id": "ark:59852/model-nvidia-parakeet-tdt-0-6b-v2-iz2sibgd7uf"
        },
        {
          "@id": "ark:59852/model-google-bert-bert-base-cased-hfgzzkrb826"
        },
        {
          "@id": "ark:59852/model-facebook-bart-large-mnli-ewfbnprhsh"
        },
        {
          "@id": "ark:59852/model-trl-internal-testing-tiny-qwen2forcausallm-2-5-acvf1lytsgx"
        },
        {
          "@id": "ark:59852/model-google-vit-base-patch16-224-s6vypmo8gkm"
        },
        {
          "@id": "ark:59852/model-ggml-org-models-moved-iodpacgvcqt"
        },
        {
          "@id": "ark:59852/model-isotonic-distilbertfinetunedai4privacyv2-n2hexf8ztgd"
        },
        {
          "@id": "ark:59852/model-sentence-transformers-all-minilm-l12-v2-1otyns27ay"
        },
        {
          "@id": "ark:59852/model-meta-llama-llama-3-2-1b-vlpktukmnyk"
        },
        {
          "@id": "ark:59852/model-facebook-wav2vec2-base-960h-0djuzrytms5"
        },
        {
          "@id": "ark:59852/model-openai-whisper-small-b9umck3j3ix"
        },
        {
          "@id": "ark:59852/model-nvidia-parakeet-rnnt-0-6b-opcqceelwf"
        },
        {
          "@id": "ark:59852/model-facebookai-xlm-roberta-large-1otynsf7qt"
        },
        {
          "@id": "ark:59852/model-patrickjohncyh-fashion-clip-gkqtbrauypv"
        },
        {
          "@id": "ark:59852/model-sentence-transformers-msmarco-distilbert-base-tas-b-pq8dfgckr5"
        },
        {
          "@id": "ark:59852/model-openai-clip-vit-base-patch16-jflnu5oweg"
        },
        {
          "@id": "ark:59852/model-sentence-transformers-paraphrase-minilm-l6-v2-sxiljz6i97"
        },
        {
          "@id": "ark:59852/model-emilyalsentzer-bioclinicalbert-yj9xpldecaa"
        },
        {
          "@id": "ark:59852/model-w11wo-indonesian-roberta-base-posp-tagger-s6vypmn88n"
        }
      ],
      "associatedPublication": "",
      "conditionsOfAccess": "",
      "copyrightNotice": ""
    },
    {
      "@id": "ark:59852/model-sentence-transformers-all-minilm-l6-v2-vm3x80oxv2",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "sentence-transformers/all-MiniLM-L6-v2",
      "description": "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
      "author": "sentence-transformers",
      "keywords": [
        "sentence-transformers",
        "pytorch",
        "tf",
        "rust",
        "onnx",
        "safetensors",
        "openvino",
        "bert",
        "feature-extraction",
        "sentence-similarity",
        "transformers",
        "en",
        "dataset:s2orc",
        "dataset:flax-sentence-embeddings/stackexchange_xml",
        "dataset:ms_marco",
        "dataset:gooaq",
        "dataset:yahoo_answers_topics",
        "dataset:code_search_net",
        "dataset:search_qa",
        "dataset:eli5",
        "dataset:snli",
        "dataset:multi_nli",
        "dataset:wikihow",
        "dataset:natural_questions",
        "dataset:trivia_qa",
        "dataset:embedding-data/sentence-compression",
        "dataset:embedding-data/flickr30k-captions",
        "dataset:embedding-data/altlex",
        "dataset:embedding-data/simple-wiki",
        "dataset:embedding-data/QQP",
        "dataset:embedding-data/SPECTER",
        "dataset:embedding-data/PAQ_pairs",
        "dataset:embedding-data/WikiAnswers",
        "arxiv:1904.06472",
        "arxiv:2102.07033",
        "arxiv:2104.08727",
        "arxiv:1704.05179",
        "arxiv:1810.09305",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-embeddings-inference",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/s2orc"
        },
        {
          "@id": "https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml"
        },
        {
          "@id": "https://huggingface.co/datasets/ms_marco"
        },
        {
          "@id": "https://huggingface.co/datasets/gooaq"
        },
        {
          "@id": "https://huggingface.co/datasets/yahoo_answers_topics"
        },
        {
          "@id": "https://huggingface.co/datasets/code_search_net"
        },
        {
          "@id": "https://huggingface.co/datasets/search_qa"
        },
        {
          "@id": "https://huggingface.co/datasets/eli5"
        },
        {
          "@id": "https://huggingface.co/datasets/snli"
        },
        {
          "@id": "https://huggingface.co/datasets/multi_nli"
        },
        {
          "@id": "https://huggingface.co/datasets/wikihow"
        },
        {
          "@id": "https://huggingface.co/datasets/natural_questions"
        },
        {
          "@id": "https://huggingface.co/datasets/trivia_qa"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/sentence-compression"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/flickr30k-captions"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/altlex"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/simple-wiki"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/QQP"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/SPECTER"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/PAQ_pairs"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/WikiAnswers"
        }
      ],
      "intendedUseCase": "Our model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated.",
      "usageInformation": "Using this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F",
      "contentUrl": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/model.safetensors",
      "url": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n\n# all-MiniLM-L6-v2\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n## Usage (Sentence-Transformers)\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n------\n\n## Background\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We used the pretrained [`nreimers/MiniLM-L6-H384-uncased`](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developed this model during the \n[Community week using JAX/Flax for NLP & CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intended to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`nreimers/MiniLM-L6-H384-uncased`](https://huggingface.co/nreimers/MiniLM-L6-H384-uncased) model. Please refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n#### Hyper parameters\n\nWe trained our model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: `train_script.py`.\n\n#### Training data\n\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\n\n| Dataset                                                  | Paper                                    | Number of training tuples  |\n|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|\n| [Reddit comments (2015-2018)](https://github.com/PolyAI-LDN/conversational-datasets/tree/master/reddit) | [paper](https://arxiv.org/abs/1904.06472) | 726,484,430 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Abstracts) | [paper](https://aclanthology.org/2020.acl-main.447/) | 116,288,806 |\n| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https://doi.org/10.1145/2623330.2623677) | 77,427,422 |\n| [PAQ](https://github.com/facebookresearch/PAQ) (Question, Answer) pairs | [paper](https://arxiv.org/abs/2102.07033) | 64,371,441 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Titles) | [paper](https://aclanthology.org/2020.acl-main.447/) | 52,603,982 |\n| [S2ORC](https://github.com/allenai/s2orc) (Title, Abstract) | [paper](https://aclanthology.org/2020.acl-main.447/) | 41,769,185 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title+Body, Answer) pairs  | - | 21,396,559 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Answer) pairs  | - | 21,396,559 |\n| [MS MARCO](https://microsoft.github.io/msmarco/) triplets | [paper](https://doi.org/10.1145/3404835.3462804) | 9,144,553 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) | [paper](https://arxiv.org/pdf/2104.08727.pdf) | 3,012,496 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |\n| [Code Search](https://huggingface.co/datasets/code_search_net) | - | 1,151,414 |\n| [COCO](https://cocodataset.org/#home) Image captions | [paper](https://link.springer.com/chapter/10.1007%2F978-3-319-10602-1_48) | 828,395|\n| [SPECTER](https://github.com/allenai/specter) citation triplets | [paper](https://doi.org/10.18653/v1/2020.acl-main.207) | 684,100 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |\n| [SearchQA](https://huggingface.co/datasets/search_qa) | [paper](https://arxiv.org/abs/1704.05179) | 582,261 |\n| [Eli5](https://huggingface.co/datasets/eli5) | [paper](https://doi.org/10.18653/v1/p19-1346) | 325,475 |\n| [Flickr 30k](https://shannon.cs.illinois.edu/DenotationGraph/) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/229/33) | 317,695 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles) | | 304,525 |\n| AllNLI ([SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) | [paper SNLI](https://doi.org/10.18653/v1/d15-1075), [paper MultiNLI](https://doi.org/10.18653/v1/n18-1101) | 277,230 | \n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |\n| [Sentence Compression](https://github.com/google-research-datasets/sentence-compression) | [paper](https://www.aclweb.org/anthology/D13-1155/) | 180,000 |\n| [Wikihow](https://github.com/pvl/wikihow_pairs_dataset) | [paper](https://arxiv.org/abs/1810.09305) | 128,542 |\n| [Altlex](https://github.com/chridey/altlex/) | [paper](https://aclanthology.org/P16-1135.pdf) | 112,696 |\n| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |\n| [Simple Wikipedia](https://cs.pomona.edu/~dkauchak/simplification/) | [paper](https://www.aclweb.org/anthology/P11-2117/) | 102,225 |\n| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/1455) | 100,231 |\n| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) | [paper](https://aclanthology.org/P18-2124.pdf) | 87,599 |\n| [TriviaQA](https://huggingface.co/datasets/trivia_qa) | - | 73,346 |\n| **Total** | | **1,170,060,424** |"
    },
    {
      "@id": "ark:59852/model-google-electra-base-discriminator-tsm97ogyfzb",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "google/electra-base-discriminator",
      "description": "**ELECTRA** is a new method for self-supervised language representation learning. It can be used to pre-train transformer networks using relatively little compute. ELECTRA models are trained to distinguish \"real\" input tokens vs \"fake\" input tokens generated by another neural network, similar to the discriminator of a [GAN](https://arxiv.org/pdf/1406.2661.pdf). At small scale, ELECTRA achieves strong results even when trained on a single GPU. At large scale, ELECTRA achieves state-of-the-art results on the [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset.",
      "author": "google",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "rust",
        "electra",
        "pretraining",
        "en",
        "arxiv:1406.2661",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [],
      "usageInformation": "```python\nfrom transformers import ElectraForPreTraining, ElectraTokenizerFast\nimport torch\n\ndiscriminator = ElectraForPreTraining.from_pretrained(\"google/electra-base-discriminator\")\ntokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-base-discriminator\")\n\nsentence = \"The quick brown fox jumps over the lazy dog\"\nfake_sentence = \"The quick brown fox fake over the lazy dog\"\n\nfake_tokens = tokenizer.tokenize(fake_sentence)\nfake_inputs = tokenizer.encode(fake_sentence, return_tensors=\"pt\")\ndiscriminator_outputs = discriminator(fake_inputs)\npredictions = torch.round((torch.sign(discriminator_outputs[0]) + 1) / 2)\n\n[print(\"%7s\" % token, end=\"\") for token in fake_tokens]\n\n[print(\"%7s\" % int(prediction), end=\"\") for prediction in predictions.tolist()]\n```",
      "contentUrl": "https://huggingface.co/google/electra-base-discriminator/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/google/electra-base-discriminator",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n## ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\n\n**ELECTRA** is a new method for self-supervised language representation learning. It can be used to pre-train transformer networks using relatively little compute. ELECTRA models are trained to distinguish \"real\" input tokens vs \"fake\" input tokens generated by another neural network, similar to the discriminator of a [GAN](https://arxiv.org/pdf/1406.2661.pdf). At small scale, ELECTRA achieves strong results even when trained on a single GPU. At large scale, ELECTRA achieves state-of-the-art results on the [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset.\n\nFor a detailed description and experimental results, please refer to our paper [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/pdf?id=r1xMH1BtvB).\n\nThis repository contains code to pre-train ELECTRA, including small ELECTRA models on a single GPU. It also supports fine-tuning ELECTRA on downstream tasks including classification tasks (e.g,. [GLUE](https://gluebenchmark.com/)), QA tasks (e.g., [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)), and sequence tagging tasks (e.g., [text chunking](https://www.clips.uantwerpen.be/conll2000/chunking/)).\n\n## How to use the discriminator in `transformers`\n\n```python\nfrom transformers import ElectraForPreTraining, ElectraTokenizerFast\nimport torch\n\ndiscriminator = ElectraForPreTraining.from_pretrained(\"google/electra-base-discriminator\")\ntokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-base-discriminator\")\n\nsentence = \"The quick brown fox jumps over the lazy dog\"\nfake_sentence = \"The quick brown fox fake over the lazy dog\"\n\nfake_tokens = tokenizer.tokenize(fake_sentence)\nfake_inputs = tokenizer.encode(fake_sentence, return_tensors=\"pt\")\ndiscriminator_outputs = discriminator(fake_inputs)\npredictions = torch.round((torch.sign(discriminator_outputs[0]) + 1) / 2)\n\n[print(\"%7s\" % token, end=\"\") for token in fake_tokens]\n\n[print(\"%7s\" % int(prediction), end=\"\") for prediction in predictions.tolist()]\n```\n"
    },
    {
      "@id": "ark:59852/model-falconsai-nsfwimagedetection-pq8dfeakedj",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "Falconsai/nsfw_image_detection",
      "description": "The **Fine-Tuned Vision Transformer (ViT)** is a variant of the transformer encoder architecture, similar to BERT, that has been adapted for image classification tasks. This specific model, named \"google/vit-base-patch16-224-in21k,\" is pre-trained on a substantial collection of images in a supervised manner, leveraging the ImageNet-21k dataset. The images in the pre-training dataset are resized to a resolution of 224x224 pixels, making it suitable for a wide range of image recognition tasks.",
      "author": "Falconsai",
      "keywords": [
        "transformers",
        "pytorch",
        "safetensors",
        "vit",
        "image-classification",
        "arxiv:2010.11929",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "image-classification",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "hasBias": "The overarching objective of this meticulous training process was to impart the model with a deep understanding of visual cues, ensuring its robustness and competence in tackling the specific task of NSFW image classification. The result is a model that stands ready to contribute significantly to content safety and moderation, all while maintaining the highest standards of accuracy and reliability.\n## Intended Uses & Limitations",
      "intendedUseCase": "- **NSFW Image Classification**: The primary intended use of this model is for the classification of NSFW (Not Safe for Work) images. It has been fine-tuned for this purpose, making it suitable for filtering explicit or inappropriate content in various applications.",
      "usageInformation": "Here is how to use this model to classifiy an image based on 1 of 2 classes (normal,nsfw):\n\n```markdown",
      "contentUrl": "https://huggingface.co/Falconsai/nsfw_image_detection/resolve/main/model.safetensors",
      "url": "https://huggingface.co/Falconsai/nsfw_image_detection",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "# Model Card: Fine-Tuned Vision Transformer (ViT) for NSFW Image Classification\n\n## Model Description\n\nThe **Fine-Tuned Vision Transformer (ViT)** is a variant of the transformer encoder architecture, similar to BERT, that has been adapted for image classification tasks. This specific model, named \"google/vit-base-patch16-224-in21k,\" is pre-trained on a substantial collection of images in a supervised manner, leveraging the ImageNet-21k dataset. The images in the pre-training dataset are resized to a resolution of 224x224 pixels, making it suitable for a wide range of image recognition tasks.\n\nDuring the training phase, meticulous attention was given to hyperparameter settings to ensure optimal model performance. The model was fine-tuned with a judiciously chosen batch size of 16. This choice not only balanced computational efficiency but also allowed for the model to effectively process and learn from a diverse array of images.\n\nTo facilitate this fine-tuning process, a learning rate of 5e-5 was employed. The learning rate serves as a critical tuning parameter that dictates the magnitude of adjustments made to the model's parameters during training. In this case, a learning rate of 5e-5 was selected to strike a harmonious balance between rapid convergence and steady optimization, resulting in a model that not only learns swiftly but also steadily refines its capabilities throughout the training process.\n\nThis training phase was executed using a proprietary dataset containing an extensive collection of 80,000 images, each characterized by a substantial degree of variability. The dataset was thoughtfully curated to include two distinct classes, namely \"normal\" and \"nsfw.\" This diversity allowed the model to grasp nuanced visual patterns, equipping it with the competence to accurately differentiate between safe and explicit content.\n\nThe overarching objective of this meticulous training process was to impart the model with a deep understanding of visual cues, ensuring its robustness and competence in tackling the specific task of NSFW image classification. The result is a model that stands ready to contribute significantly to content safety and moderation, all while maintaining the highest standards of accuracy and reliability.\n## Intended Uses & Limitations\n\n### Intended Uses\n- **NSFW Image Classification**: The primary intended use of this model is for the classification of NSFW (Not Safe for Work) images. It has been fine-tuned for this purpose, making it suitable for filtering explicit or inappropriate content in various applications.\n\n### How to use\nHere is how to use this model to classifiy an image based on 1 of 2 classes (normal,nsfw):\n\n```markdown\n\n# Use a pipeline as a high-level helper\nfrom PIL import Image\nfrom transformers import pipeline\n\nimg = Image.open(\"<path_to_image_file>\")\nclassifier = pipeline(\"image-classification\", model=\"Falconsai/nsfw_image_detection\")\nclassifier(img)\n\n```\n\n<hr>\n\n``` markdown\n\n# Load model directly\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModelForImageClassification, ViTImageProcessor\n\nimg = Image.open(\"<path_to_image_file>\")\nmodel = AutoModelForImageClassification.from_pretrained(\"Falconsai/nsfw_image_detection\")\nprocessor = ViTImageProcessor.from_pretrained('Falconsai/nsfw_image_detection')\nwith torch.no_grad():\n    inputs = processor(images=img, return_tensors=\"pt\")\n    outputs = model(**inputs)\n    logits = outputs.logits\n\npredicted_label = logits.argmax(-1).item()\nmodel.config.id2label[predicted_label]\n\n```\n\n<hr>\nRun Yolo Version\n\n``` markdown\n\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport onnxruntime as ort\nimport json # Added import for json\n\n# Predict using YOLOv9 model\ndef predict_with_yolov9(image_path, model_path, labels_path, input_size):\n    \"\"\"\n    Run inference using the converted YOLOv9 model on a single image.\n\n    Args:\n        image_path (str): Path to the input image file.\n        model_path (str): Path to the ONNX model file.\n        labels_path (str): Path to the JSON file containing class labels.\n        input_size (tuple): The expected input size (height, width) for the model.\n\n    Returns:\n        str: The predicted class label.\n        PIL.Image.Image: The original loaded image.\n    \"\"\"\n    def load_json(file_path):\n        with open(file_path, \"r\") as f:\n            return json.load(f)\n\n    # Load labels\n    labels = load_json(labels_path)\n\n    # Preprocess image\n    original_image = Image.open(image_path).convert(\"RGB\")\n    image_resized = original_image.resize(input_size, Image.Resampling.BILINEAR)\n    image_np = np.array(image_resized, dtype=np.float32) / 255.0\n    image_np = np.transpose(image_np, (2, 0, 1))  # [C, H, W]\n    input_tensor = np.expand_dims(image_np, axis=0).astype(np.float32)\n\n    # Load YOLOv9 model\n    session = ort.InferenceSession(model_path)\n    input_name = session.get_inputs()[0].name\n    output_name = session.get_outputs()[0].name # Assuming classification output\n\n    # Run inference\n    outputs = session.run([output_name], {input_name: input_tensor})\n    predictions = outputs[0]\n\n    # Postprocess predictions (assuming classification output)\n    # Adapt this section if your model output is different (e.g., detection boxes)\n    predicted_index = np.argmax(predictions)\n    predicted_label = labels[str(predicted_index)] # Assumes labels are indexed by string numbers\n\n    return predicted_label, original_image\n\n# Display prediction for a single image\ndef display_single_prediction(image_path, model_path, labels_path, input_size):\n    \"\"\"\n    Predicts the class for a single image and displays the image with its prediction.\n\n    Args:\n        image_path (str): Path to the input image file.\n        model_path (str): Path to the ONNX model file.\n        labels_path (str): Path to the JSON file containing class labels.\n        input_size (tuple): The expected input size (height, width) for the model.\n    \"\"\"\n    try:\n        # Run prediction\n        prediction, img = predict_with_yolov9(image_path, model_path, labels_path, input_size)\n\n        # Display image and prediction\n        fig, ax = plt.subplots(1, 1, figsize=(8, 8)) # Create a single plot\n        ax.imshow(img)\n        ax.set_title(f\"Prediction: {prediction}\", fontsize=14)\n        ax.axis(\"off\") # Hide axes ticks and labels\n\n        plt.tight_layout()\n        plt.show()\n\n    except FileNotFoundError:\n        print(f\"Error: Image file not found at {image_path}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n\n# --- Main Execution ---\n\n# Paths and parameters - **MODIFY THESE**\nsingle_image_path = \"path/to/your/single_image.jpg\"  # <--- Replace with the actual path to your image file\nmodel_path = \"path/to/your/yolov9_model.onnx\"    # <--- Replace with the actual path to your ONNX model\nlabels_path = \"path/to/your/labels.json\"        # <--- Replace with the actual path to your labels JSON file\ninput_size = (224, 224)                         # Standard input size, adjust if your model differs\n\n# Check if the image file exists before proceeding (optional but recommended)\nif os.path.exists(single_image_path):\n    # Run prediction and display for the single image\n    display_single_prediction(single_image_path, model_path, labels_path, input_size)\nelse:\n    print(f\"Error: The specified image file does not exist: {single_image_path}\")\n\n```\n\n<hr>\n\n\n\n### Limitations\n- **Specialized Task Fine-Tuning**: While the model is adept at NSFW image classification, its performance may vary when applied to other tasks.\n- Users interested in employing this model for different tasks should explore fine-tuned versions available in the model hub for optimal results.\n\n## Training Data\n\nThe model's training data includes a proprietary dataset comprising approximately 80,000 images. This dataset encompasses a significant amount of variability and consists of two distinct classes: \"normal\" and \"nsfw.\" The training process on this data aimed to equip the model with the ability to distinguish between safe and explicit content effectively.\n\n### Training Stats\n``` markdown\n\n- 'eval_loss': 0.07463177293539047,\n- 'eval_accuracy': 0.980375, \n- 'eval_runtime': 304.9846, \n- 'eval_samples_per_second': 52.462, \n- 'eval_steps_per_second': 3.279\n\n```\n\n<hr>\n\n\n**Note:** It's essential to use this model responsibly and ethically, adhering to content guidelines and applicable regulations when implementing it in real-world applications, particularly those involving potentially sensitive content.\n\nFor more details on model fine-tuning and usage, please refer to the model's documentation and the model hub.\n\n## References\n\n- [Hugging Face Model Hub](https://huggingface.co/models)\n- [Vision Transformer (ViT) Paper](https://arxiv.org/abs/2010.11929)\n- [ImageNet-21k Dataset](http://www.image-net.org/)\n\n**Disclaimer:** The model's performance may be influenced by the quality and representativeness of the data it was fine-tuned on. Users are encouraged to assess the model's suitability for their specific applications and datasets."
    },
    {
      "@id": "ark:59852/model-google-bert-bert-base-uncased-acvf1xltspx",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "google-bert/bert-base-uncased",
      "description": "Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in",
      "author": "google-bert",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "rust",
        "coreml",
        "onnx",
        "safetensors",
        "bert",
        "fill-mask",
        "exbert",
        "en",
        "dataset:bookcorpus",
        "dataset:wikipedia",
        "arxiv:1810.04805",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/bookcorpus"
        },
        {
          "@id": "https://huggingface.co/datasets/wikipedia"
        }
      ],
      "hasBias": "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n  'score': 0.09747550636529922,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the man worked as a waiter. [SEP]',\n  'score': 0.0523831807076931,\n  'token': 15610,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] the man worked as a barber. [SEP]',\n  'score': 0.04962705448269844,\n  'token': 13362,\n  'token_str': 'barber'},\n {'sequence': '[CLS] the man worked as a mechanic. [SEP]',\n  'score': 0.03788609802722931,\n  'token': 15893,\n  'token_str': 'mechanic'},\n {'sequence': '[CLS] the man worked as a salesman. [SEP]',\n  'score': 0.037680890411138535,\n  'token': 18968,\n  'token_str': 'salesman'}]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the woman worked as a nurse. [SEP]',\n  'score': 0.21981462836265564,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the woman worked as a waitress. [SEP]',\n  'score': 0.1597415804862976,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the woman worked as a maid. [SEP]',\n  'score': 0.1154729500412941,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the woman worked as a prostitute. [SEP]',\n  'score': 0.037968918681144714,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the woman worked as a cook. [SEP]',\n  'score': 0.03042375110089779,\n  'token': 5660,\n  'token_str': 'cook'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.",
      "intendedUseCase": "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for\nfine-tuned versions of a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.1073106899857521,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.08774490654468536,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a new model. [SEP]\",\n  'score': 0.05338378623127937,\n  'token': 2047,\n  'token_str': 'new'},\n {'sequence': \"[CLS] hello i'm a super model. [SEP]\",\n  'score': 0.04667217284440994,\n  'token': 3565,\n  'token_str': 'super'},\n {'sequence': \"[CLS] hello i'm a fine model. [SEP]\",\n  'score': 0.027095865458250046,\n  'token': 2986,\n  'token_str': 'fine'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n  'score': 0.09747550636529922,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the man worked as a waiter. [SEP]',\n  'score': 0.0523831807076931,\n  'token': 15610,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] the man worked as a barber. [SEP]',\n  'score': 0.04962705448269844,\n  'token': 13362,\n  'token_str': 'barber'},\n {'sequence': '[CLS] the man worked as a mechanic. [SEP]',\n  'score': 0.03788609802722931,\n  'token': 15893,\n  'token_str': 'mechanic'},\n {'sequence': '[CLS] the man worked as a salesman. [SEP]',\n  'score': 0.037680890411138535,\n  'token': 18968,\n  'token_str': 'salesman'}]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the woman worked as a nurse. [SEP]',\n  'score': 0.21981462836265564,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the woman worked as a waitress. [SEP]',\n  'score': 0.1597415804862976,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the woman worked as a maid. [SEP]',\n  'score': 0.1154729500412941,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the woman worked as a prostitute. [SEP]',\n  'score': 0.037968918681144714,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the woman worked as a cook. [SEP]',\n  'score': 0.03042375110089779,\n  'token': 5660,\n  'token_str': 'cook'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.",
      "usageInformation": "You can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.1073106899857521,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.08774490654468536,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a new model. [SEP]\",\n  'score': 0.05338378623127937,\n  'token': 2047,\n  'token_str': 'new'},\n {'sequence': \"[CLS] hello i'm a super model. [SEP]\",\n  'score': 0.04667217284440994,\n  'token': 3565,\n  'token_str': 'super'},\n {'sequence': \"[CLS] hello i'm a fine model. [SEP]\",\n  'score': 0.027095865458250046,\n  'token': 2986,\n  'token_str': 'fine'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```",
      "contentUrl": "https://huggingface.co/google-bert/bert-base-uncased/resolve/main/model.safetensors",
      "url": "https://huggingface.co/google-bert/bert-base-uncased",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# BERT base model (uncased)\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1810.04805) and first released in\n[this repository](https://github.com/google-research/bert). This model is uncased: it does not make a difference\nbetween english and English.\n\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\nwas pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally masks the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences, for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Model variations\n\nBERT has originally been released in base and large variations, for cased and uncased input text. The uncased models also strips out an accent markers.  \nChinese and multilingual uncased and cased versions followed shortly after.  \nModified preprocessing with whole word masking has replaced subpiece masking in a following work, with the release of two models.  \nOther 24 smaller models are released afterward.  \n\nThe detailed release history can be found on the [google-research/bert readme](https://github.com/google-research/bert/blob/master/README.md) on github.\n\n| Model | #params | Language |\n|------------------------|--------------------------------|-------|\n| [`bert-base-uncased`](https://huggingface.co/bert-base-uncased) | 110M   | English |\n| [`bert-large-uncased`](https://huggingface.co/bert-large-uncased)              | 340M    | English | sub \n| [`bert-base-cased`](https://huggingface.co/bert-base-cased)        | 110M    | English |\n| [`bert-large-cased`](https://huggingface.co/bert-large-cased) | 340M    |  English |\n| [`bert-base-chinese`](https://huggingface.co/bert-base-chinese) | 110M    | Chinese |\n| [`bert-base-multilingual-cased`](https://huggingface.co/bert-base-multilingual-cased) | 110M | Multiple |\n| [`bert-large-uncased-whole-word-masking`](https://huggingface.co/bert-large-uncased-whole-word-masking) | 340M | English |\n| [`bert-large-cased-whole-word-masking`](https://huggingface.co/bert-large-cased-whole-word-masking) | 340M | English |\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for\nfine-tuned versions of a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.1073106899857521,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.08774490654468536,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a new model. [SEP]\",\n  'score': 0.05338378623127937,\n  'token': 2047,\n  'token_str': 'new'},\n {'sequence': \"[CLS] hello i'm a super model. [SEP]\",\n  'score': 0.04667217284440994,\n  'token': 3565,\n  'token_str': 'super'},\n {'sequence': \"[CLS] hello i'm a fine model. [SEP]\",\n  'score': 0.027095865458250046,\n  'token': 2986,\n  'token_str': 'fine'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n  'score': 0.09747550636529922,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the man worked as a waiter. [SEP]',\n  'score': 0.0523831807076931,\n  'token': 15610,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] the man worked as a barber. [SEP]',\n  'score': 0.04962705448269844,\n  'token': 13362,\n  'token_str': 'barber'},\n {'sequence': '[CLS] the man worked as a mechanic. [SEP]',\n  'score': 0.03788609802722931,\n  'token': 15893,\n  'token_str': 'mechanic'},\n {'sequence': '[CLS] the man worked as a salesman. [SEP]',\n  'score': 0.037680890411138535,\n  'token': 18968,\n  'token_str': 'salesman'}]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the woman worked as a nurse. [SEP]',\n  'score': 0.21981462836265564,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the woman worked as a waitress. [SEP]',\n  'score': 0.1597415804862976,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the woman worked as a maid. [SEP]',\n  'score': 0.1154729500412941,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the woman worked as a prostitute. [SEP]',\n  'score': 0.037968918681144714,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the woman worked as a cook. [SEP]',\n  'score': 0.03042375110089779,\n  'token': 5660,\n  'token_str': 'cook'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038\nunpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and\nheaders).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus, and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\nused is Adam with a learning rate of 1e-4, \\\\(\\beta_{1} = 0.9\\\\) and \\\\(\\beta_{2} = 0.999\\\\), a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI-(m/mm) | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  | Average |\n|:----:|:-----------:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|:-------:|\n|      | 84.6/83.4   | 71.2 | 90.5 | 93.5  | 52.1 | 85.8  | 88.9 | 66.4 | 79.6    |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=bert-base-uncased\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n"
    },
    {
      "@id": "ark:59852/model-dima806-fairfaceageimagedetection-iodpan8vcmp",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "dima806/fairface_age_image_detection",
      "description": "Detects age group with about 59% accuracy based on an image.",
      "author": "dima806",
      "keywords": [
        "transformers",
        "safetensors",
        "vit",
        "image-classification",
        "dataset:nateraw/fairface",
        "base_model:google/vit-base-patch16-224-in21k",
        "base_model:finetune:google/vit-base-patch16-224-in21k",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "image-classification",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/nateraw/fairface"
        }
      ],
      "baseModel": "google/vit-base-patch16-224-in21k",
      "contentUrl": "https://huggingface.co/dima806/fairface_age_image_detection/resolve/main/checkpoint-32/model.safetensors",
      "url": "https://huggingface.co/dima806/fairface_age_image_detection",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "Detects age group with about 59% accuracy based on an image.\n\nSee https://www.kaggle.com/code/dima806/age-group-image-classification-vit for details.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6449300e3adf50d864095b90/gvzsgTtWDOE4vxwugZF4P.png)\n\n```\nClassification report:\n\n              precision    recall  f1-score   support\n\n         0-2     0.7803    0.7500    0.7649       180\n         3-9     0.7998    0.7998    0.7998      1249\n       10-19     0.5361    0.4236    0.4733      1086\n       20-29     0.6402    0.7221    0.6787      3026\n       30-39     0.4935    0.5083    0.5008      2099\n       40-49     0.4848    0.4386    0.4606      1238\n       50-59     0.5000    0.4814    0.4905       725\n       60-69     0.4497    0.4685    0.4589       286\nmore than 70     0.6897    0.1802    0.2857       111\n\n    accuracy                         0.5892     10000\n   macro avg     0.5971    0.5303    0.5459     10000\nweighted avg     0.5863    0.5892    0.5844     10000\n```"
    },
    {
      "@id": "ark:59852/model-timm-mobilenetv3small100-lambin1k-wgpq2k4fgr",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "timm/mobilenetv3_small_100.lamb_in1k",
      "description": "A MobileNet-v3 image classification model. Trained on ImageNet-1k in `timm` using recipe template described below.",
      "author": "timm",
      "keywords": [
        "timm",
        "pytorch",
        "safetensors",
        "image-classification",
        "transformers",
        "dataset:imagenet-1k",
        "arxiv:2110.00476",
        "arxiv:1905.02244",
        "license:apache-2.0",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "image-classification",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/imagenet-1k"
        }
      ],
      "usageInformation": "### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('mobilenetv3_small_100.lamb_in1k', pretrained=True)\nmodel = model.eval()",
      "contentUrl": "https://huggingface.co/timm/mobilenetv3_small_100.lamb_in1k/resolve/main/model.safetensors",
      "url": "https://huggingface.co/timm/mobilenetv3_small_100.lamb_in1k",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "# Model card for mobilenetv3_small_100.lamb_in1k\n\nA MobileNet-v3 image classification model. Trained on ImageNet-1k in `timm` using recipe template described below.\n\nRecipe details:\n * A LAMB optimizer based recipe that is similar to [ResNet Strikes Back](https://arxiv.org/abs/2110.00476) `A2` but 50% longer with EMA weight averaging, no CutMix\n * Step (exponential decay w/ staircase) LR schedule with warmup\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 2.5\n  - GMACs: 0.1\n  - Activations (M): 1.4\n  - Image size: 224 x 224\n- **Papers:**\n  - Searching for MobileNetV3: https://arxiv.org/abs/1905.02244\n- **Dataset:** ImageNet-1k\n- **Original:** https://github.com/huggingface/pytorch-image-models\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('mobilenetv3_small_100.lamb_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'mobilenetv3_small_100.lamb_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 16, 112, 112])\n    #  torch.Size([1, 16, 56, 56])\n    #  torch.Size([1, 24, 28, 28])\n    #  torch.Size([1, 48, 14, 14])\n    #  torch.Size([1, 576, 7, 7])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'mobilenetv3_small_100.lamb_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 576, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n## Citation\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```\n```bibtex\n@inproceedings{howard2019searching,\n  title={Searching for mobilenetv3},\n  author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},\n  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},\n  pages={1314--1324},\n  year={2019}\n}\n```\n"
    },
    {
      "@id": "ark:59852/model-sentence-transformers-all-mpnet-base-v2-jflnukowl2l",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "sentence-transformers/all-mpnet-base-v2",
      "description": "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
      "author": "sentence-transformers",
      "keywords": [
        "sentence-transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "openvino",
        "mpnet",
        "fill-mask",
        "feature-extraction",
        "sentence-similarity",
        "transformers",
        "text-embeddings-inference",
        "en",
        "dataset:s2orc",
        "dataset:flax-sentence-embeddings/stackexchange_xml",
        "dataset:ms_marco",
        "dataset:gooaq",
        "dataset:yahoo_answers_topics",
        "dataset:code_search_net",
        "dataset:search_qa",
        "dataset:eli5",
        "dataset:snli",
        "dataset:multi_nli",
        "dataset:wikihow",
        "dataset:natural_questions",
        "dataset:trivia_qa",
        "dataset:embedding-data/sentence-compression",
        "dataset:embedding-data/flickr30k-captions",
        "dataset:embedding-data/altlex",
        "dataset:embedding-data/simple-wiki",
        "dataset:embedding-data/QQP",
        "dataset:embedding-data/SPECTER",
        "dataset:embedding-data/PAQ_pairs",
        "dataset:embedding-data/WikiAnswers",
        "arxiv:1904.06472",
        "arxiv:2102.07033",
        "arxiv:2104.08727",
        "arxiv:1704.05179",
        "arxiv:1810.09305",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/s2orc"
        },
        {
          "@id": "https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml"
        },
        {
          "@id": "https://huggingface.co/datasets/ms_marco"
        },
        {
          "@id": "https://huggingface.co/datasets/gooaq"
        },
        {
          "@id": "https://huggingface.co/datasets/yahoo_answers_topics"
        },
        {
          "@id": "https://huggingface.co/datasets/code_search_net"
        },
        {
          "@id": "https://huggingface.co/datasets/search_qa"
        },
        {
          "@id": "https://huggingface.co/datasets/eli5"
        },
        {
          "@id": "https://huggingface.co/datasets/snli"
        },
        {
          "@id": "https://huggingface.co/datasets/multi_nli"
        },
        {
          "@id": "https://huggingface.co/datasets/wikihow"
        },
        {
          "@id": "https://huggingface.co/datasets/natural_questions"
        },
        {
          "@id": "https://huggingface.co/datasets/trivia_qa"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/sentence-compression"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/flickr30k-captions"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/altlex"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/simple-wiki"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/QQP"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/SPECTER"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/PAQ_pairs"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/WikiAnswers"
        }
      ],
      "intendedUseCase": "Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 384 word pieces is truncated.",
      "usageInformation": "Using this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F",
      "contentUrl": "https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/model.safetensors",
      "url": "https://huggingface.co/sentence-transformers/all-mpnet-base-v2",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n\n# all-mpnet-base-v2\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n## Usage (Sentence-Transformers)\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n## Usage (Text Embeddings Inference (TEI))\n\n[Text Embeddings Inference (TEI)](https://github.com/huggingface/text-embeddings-inference) is a blazing fast inference solution for text embedding models.\n\n- CPU:\n```bash\ndocker run -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cpu-latest --model-id sentence-transformers/all-mpnet-base-v2 --pooling mean --dtype float16\n```\n\n- NVIDIA GPU:\n```bash\ndocker run --gpus all -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cuda-latest --model-id sentence-transformers/all-mpnet-base-v2 --pooling mean --dtype float16\n```\n\nSend a request to `/v1/embeddings` to generate embeddings via the [OpenAI Embeddings API](https://platform.openai.com/docs/api-reference/embeddings/create):\n```bash\ncurl http://localhost:8080/v1/embeddings \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n    \"model\": \"sentence-transformers/all-mpnet-base-v2\",\n    \"input\": [\"This is an example sentence\", \"Each sentence is converted\"]\n  }'\n```\n\nOr check the [Text Embeddings Inference API specification](https://huggingface.github.io/text-embeddings-inference/) instead.\n\n------\n\n## Background\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We used the pretrained [`microsoft/mpnet-base`](https://huggingface.co/microsoft/mpnet-base) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developed this model during the \n[Community week using JAX/Flax for NLP & CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intented to be used as a sentence and short paragraph encoder. Given an input text, it outputs a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 384 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`microsoft/mpnet-base`](https://huggingface.co/microsoft/mpnet-base) model. Please refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n#### Hyper parameters\n\nWe trained our model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: `train_script.py`.\n\n#### Training data\n\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\n\n| Dataset                                                  | Paper                                    | Number of training tuples  |\n|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|\n| [Reddit comments (2015-2018)](https://github.com/PolyAI-LDN/conversational-datasets/tree/master/reddit) | [paper](https://arxiv.org/abs/1904.06472) | 726,484,430 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Abstracts) | [paper](https://aclanthology.org/2020.acl-main.447/) | 116,288,806 |\n| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https://doi.org/10.1145/2623330.2623677) | 77,427,422 |\n| [PAQ](https://github.com/facebookresearch/PAQ) (Question, Answer) pairs | [paper](https://arxiv.org/abs/2102.07033) | 64,371,441 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Titles) | [paper](https://aclanthology.org/2020.acl-main.447/) | 52,603,982 |\n| [S2ORC](https://github.com/allenai/s2orc) (Title, Abstract) | [paper](https://aclanthology.org/2020.acl-main.447/) | 41,769,185 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title+Body, Answer) pairs  | - | 21,396,559 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Answer) pairs  | - | 21,396,559 |\n| [MS MARCO](https://microsoft.github.io/msmarco/) triplets | [paper](https://doi.org/10.1145/3404835.3462804) | 9,144,553 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) | [paper](https://arxiv.org/pdf/2104.08727.pdf) | 3,012,496 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |\n| [Code Search](https://huggingface.co/datasets/code_search_net) | - | 1,151,414 |\n| [COCO](https://cocodataset.org/#home) Image captions | [paper](https://link.springer.com/chapter/10.1007%2F978-3-319-10602-1_48) | 828,395|\n| [SPECTER](https://github.com/allenai/specter) citation triplets | [paper](https://doi.org/10.18653/v1/2020.acl-main.207) | 684,100 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |\n| [SearchQA](https://huggingface.co/datasets/search_qa) | [paper](https://arxiv.org/abs/1704.05179) | 582,261 |\n| [Eli5](https://huggingface.co/datasets/eli5) | [paper](https://doi.org/10.18653/v1/p19-1346) | 325,475 |\n| [Flickr 30k](https://shannon.cs.illinois.edu/DenotationGraph/) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/229/33) | 317,695 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles) | | 304,525 |\n| AllNLI ([SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) | [paper SNLI](https://doi.org/10.18653/v1/d15-1075), [paper MultiNLI](https://doi.org/10.18653/v1/n18-1101) | 277,230 | \n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |\n| [Sentence Compression](https://github.com/google-research-datasets/sentence-compression) | [paper](https://www.aclweb.org/anthology/D13-1155/) | 180,000 |\n| [Wikihow](https://github.com/pvl/wikihow_pairs_dataset) | [paper](https://arxiv.org/abs/1810.09305) | 128,542 |\n| [Altlex](https://github.com/chridey/altlex/) | [paper](https://aclanthology.org/P16-1135.pdf) | 112,696 |\n| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |\n| [Simple Wikipedia](https://cs.pomona.edu/~dkauchak/simplification/) | [paper](https://www.aclweb.org/anthology/P11-2117/) | 102,225 |\n| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/1455) | 100,231 |\n| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) | [paper](https://aclanthology.org/P18-2124.pdf) | 87,599 |\n| [TriviaQA](https://huggingface.co/datasets/trivia_qa) | - | 73,346 |\n| **Total** | | **1,170,060,424** |"
    },
    {
      "@id": "ark:59852/model-openai-clip-vit-base-patch32-n2hexj3zxqx",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "openai/clip-vit-base-patch32",
      "description": "Disclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md).",
      "author": "openai",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "clip",
        "zero-shot-image-classification",
        "vision",
        "arxiv:2103.00020",
        "arxiv:1908.04913",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [],
      "hasBias": "We find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with \u2018Middle Eastern\u2019 having the highest accuracy (98.4%) and \u2018White\u2019 having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.",
      "intendedUseCase": "The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n\nThe primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP\u2019s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.",
      "contentUrl": "https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/openai/clip-vit-base-patch32",
      "isPartOf": [],
      "README": "\n# Model Card: CLIP\n\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md).\n\n## Model Details\n\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they\u2019re being deployed within.\n\n### Model Date\n\nJanuary 2021\n\n### Model Type\n\nThe model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. \n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n\n- [Blog Post](https://openai.com/blog/clip/)\n- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n\n\n### Use with Transformers\n\n```python3\nfrom PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\n\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n#### Primary intended uses\n\nThe primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n### Out-of-Scope Use Cases\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP\u2019s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n\n## Data\n\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n### Data Mission Statement\n\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n\n## Performance and Limitations\n\n### Performance\n\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n## Limitations\n\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n### Bias and Fairness\n\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with \u2018Middle Eastern\u2019 having the highest accuracy (98.4%) and \u2018White\u2019 having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.\n\n\n\n## Feedback\n\n### Where to send questions or comments about the model\n\nPlease use [this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)"
    },
    {
      "@id": "ark:59852/model-pyannote-segmentation-3-0-314zxjfwtty",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "pyannote/segmentation-3.0",
      "description": "Using this open-source model in production?",
      "author": "pyannote",
      "keywords": [
        "pyannote-audio",
        "pytorch",
        "pyannote",
        "pyannote-audio-model",
        "audio",
        "voice",
        "speech",
        "speaker",
        "speaker-diarization",
        "speaker-change-detection",
        "speaker-segmentation",
        "voice-activity-detection",
        "overlapped-speech-detection",
        "resegmentation",
        "license:mit",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [],
      "usageInformation": "```python",
      "contentUrl": "https://huggingface.co/pyannote/segmentation-3.0/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/pyannote/segmentation-3.0",
      "license": "mit",
      "isPartOf": [],
      "README": "\nUsing this open-source model in production?  \nConsider switching to [pyannoteAI](https://www.pyannote.ai) for better and faster options.\n\n# \ud83c\udfb9 \"Powerset\" speaker segmentation\n\nThis model ingests 10 seconds of mono audio sampled at 16kHz and outputs speaker diarization as a (num_frames, num_classes) matrix where the 7 classes are _non-speech_, _speaker #1_, _speaker #2_, _speaker #3_, _speakers #1 and #2_, _speakers #1 and #3_, and _speakers #2 and #3_.\n\n![Example output](example.png)\n\n```python\n# waveform (first row)\nduration, sample_rate, num_channels = 10, 16000, 1\nwaveform = torch.randn(batch_size, num_channels, duration * sample_rate) \n\n# powerset multi-class encoding (second row)\npowerset_encoding = model(waveform)\n\n# multi-label encoding (third row)\nfrom pyannote.audio.utils.powerset import Powerset\nmax_speakers_per_chunk, max_speakers_per_frame = 3, 2\nto_multilabel = Powerset(\n    max_speakers_per_chunk, \n    max_speakers_per_frame).to_multilabel\nmultilabel_encoding = to_multilabel(powerset_encoding)\n```\n\nThe various concepts behind this model are described in details in this [paper](https://www.isca-speech.org/archive/interspeech_2023/plaquet23_interspeech.html).\n\nIt has been trained by S\u00e9verin Baroudi with [pyannote.audio](https://github.com/pyannote/pyannote-audio) `3.0.0` using the combination of the training sets of AISHELL, AliMeeting, AMI, AVA-AVD, DIHARD, Ego4D, MSDWild, REPERE, and VoxConverse.\n\nThis [companion repository](https://github.com/FrenchKrab/IS2023-powerset-diarization/) by [Alexis Plaquet](https://frenchkrab.github.io/) also provides instructions on how to train or finetune such a model on your own data.\n\n## Requirements\n\n1. Install [`pyannote.audio`](https://github.com/pyannote/pyannote-audio) `3.0` with `pip install pyannote.audio`\n2. Accept [`pyannote/segmentation-3.0`](https://hf.co/pyannote/segmentation-3.0) user conditions\n3. Create access token at [`hf.co/settings/tokens`](https://hf.co/settings/tokens).\n\n\n## Usage\n\n```python\n# instantiate the model\nfrom pyannote.audio import Model\nmodel = Model.from_pretrained(\n  \"pyannote/segmentation-3.0\", \n  use_auth_token=\"HUGGINGFACE_ACCESS_TOKEN_GOES_HERE\")\n```\n\n### Speaker diarization\n\nThis model cannot be used to perform speaker diarization of full recordings on its own (it only processes 10s chunks). \n\nSee [pyannote/speaker-diarization-3.0](https://hf.co/pyannote/speaker-diarization-3.0) pipeline that uses an additional speaker embedding model to perform full recording speaker diarization.\n\n### Voice activity detection\n\n```python\nfrom pyannote.audio.pipelines import VoiceActivityDetection\npipeline = VoiceActivityDetection(segmentation=model)\nHYPER_PARAMETERS = {\n  # remove speech regions shorter than that many seconds.\n  \"min_duration_on\": 0.0,\n  # fill non-speech regions shorter than that many seconds.\n  \"min_duration_off\": 0.0\n}\npipeline.instantiate(HYPER_PARAMETERS)\nvad = pipeline(\"audio.wav\")\n# `vad` is a pyannote.core.Annotation instance containing speech regions\n```\n\n### Overlapped speech detection\n\n```python\nfrom pyannote.audio.pipelines import OverlappedSpeechDetection\npipeline = OverlappedSpeechDetection(segmentation=model)\nHYPER_PARAMETERS = {\n  # remove overlapped speech regions shorter than that many seconds.\n  \"min_duration_on\": 0.0,\n  # fill non-overlapped speech regions shorter than that many seconds.\n  \"min_duration_off\": 0.0\n}\npipeline.instantiate(HYPER_PARAMETERS)\nosd = pipeline(\"audio.wav\")\n# `osd` is a pyannote.core.Annotation instance containing overlapped speech regions\n```\n\n## Citations\n\n```bibtex\n@inproceedings{Plaquet23,\n  author={Alexis Plaquet and Herv\u00e9 Bredin},\n  title={{Powerset multi-class cross entropy loss for neural speaker diarization}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}\n```\n\n```bibtex\n@inproceedings{Bredin23,\n  author={Herv\u00e9 Bredin},\n  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}\n```\n"
    },
    {
      "@id": "ark:59852/model-facebookai-roberta-large-rhbckmcew8j",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "FacebookAI/roberta-large",
      "description": "Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in",
      "author": "FacebookAI",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "onnx",
        "safetensors",
        "roberta",
        "fill-mask",
        "exbert",
        "en",
        "dataset:bookcorpus",
        "dataset:wikipedia",
        "arxiv:1907.11692",
        "arxiv:1806.02847",
        "license:mit",
        "autotrain_compatible",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/bookcorpus"
        },
        {
          "@id": "https://huggingface.co/datasets/wikipedia"
        }
      ],
      "hasBias": "The training data used for this model contains a lot of unfiltered content from the internet, which is far from\r\nneutral. Therefore, the model can have biased predictions:\r\n\r\n```python\r\n>>> from transformers import pipeline\r\n>>> unmasker = pipeline('fill-mask', model='roberta-large')\r\n>>> unmasker(\"The man worked as a <mask>.\")\r\n\r\n[{'sequence': '<s>The man worked as a mechanic.</s>',\r\n  'score': 0.08260300755500793,\r\n  'token': 25682,\r\n  'token_str': '\u0120mechanic'},\r\n {'sequence': '<s>The man worked as a driver.</s>',\r\n  'score': 0.05736079439520836,\r\n  'token': 1393,\r\n  'token_str': '\u0120driver'},\r\n {'sequence': '<s>The man worked as a teacher.</s>',\r\n  'score': 0.04709019884467125,\r\n  'token': 3254,\r\n  'token_str': '\u0120teacher'},\r\n {'sequence': '<s>The man worked as a bartender.</s>',\r\n  'score': 0.04641604796051979,\r\n  'token': 33080,\r\n  'token_str': '\u0120bartender'},\r\n {'sequence': '<s>The man worked as a waiter.</s>',\r\n  'score': 0.04239227622747421,\r\n  'token': 38233,\r\n  'token_str': '\u0120waiter'}]\r\n\r\n>>> unmasker(\"The woman worked as a <mask>.\")\r\n\r\n[{'sequence': '<s>The woman worked as a nurse.</s>',\r\n  'score': 0.2667474150657654,\r\n  'token': 9008,\r\n  'token_str': '\u0120nurse'},\r\n {'sequence': '<s>The woman worked as a waitress.</s>',\r\n  'score': 0.12280137836933136,\r\n  'token': 35698,\r\n  'token_str': '\u0120waitress'},\r\n {'sequence': '<s>The woman worked as a teacher.</s>',\r\n  'score': 0.09747499972581863,\r\n  'token': 3254,\r\n  'token_str': '\u0120teacher'},\r\n {'sequence': '<s>The woman worked as a secretary.</s>',\r\n  'score': 0.05783602222800255,\r\n  'token': 2971,\r\n  'token_str': '\u0120secretary'},\r\n {'sequence': '<s>The woman worked as a cleaner.</s>',\r\n  'score': 0.05576248839497566,\r\n  'token': 16126,\r\n  'token_str': '\u0120cleaner'}]\r\n```\r\n\r\nThis bias will also affect all fine-tuned versions of this model.",
      "intendedUseCase": "You can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task.\r\nSee the [model hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task that\r\ninterests you.\r\n\r\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\r\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\r\ngeneration you should look at model like GPT2.\r\n\r\n### How to use\r\n\r\nYou can use this model directly with a pipeline for masked language modeling:\r\n\r\n```python\r\n>>> from transformers import pipeline\r\n>>> unmasker = pipeline('fill-mask', model='roberta-large')\r\n>>> unmasker(\"Hello I'm a <mask> model.\")\r\n\r\n[{'sequence': \"<s>Hello I'm a male model.</s>\",\r\n  'score': 0.3317350447177887,\r\n  'token': 2943,\r\n  'token_str': '\u0120male'},\r\n {'sequence': \"<s>Hello I'm a fashion model.</s>\",\r\n  'score': 0.14171843230724335,\r\n  'token': 2734,\r\n  'token_str': '\u0120fashion'},\r\n {'sequence': \"<s>Hello I'm a professional model.</s>\",\r\n  'score': 0.04291723668575287,\r\n  'token': 2038,\r\n  'token_str': '\u0120professional'},\r\n {'sequence': \"<s>Hello I'm a freelance model.</s>\",\r\n  'score': 0.02134818211197853,\r\n  'token': 18150,\r\n  'token_str': '\u0120freelance'},\r\n {'sequence': \"<s>Hello I'm a young model.</s>\",\r\n  'score': 0.021098261699080467,\r\n  'token': 664,\r\n  'token_str': '\u0120young'}]\r\n```\r\n\r\nHere is how to use this model to get the features of a given text in PyTorch:\r\n\r\n```python\r\nfrom transformers import RobertaTokenizer, RobertaModel\r\ntokenizer = RobertaTokenizer.from_pretrained('roberta-large')\r\nmodel = RobertaModel.from_pretrained('roberta-large')\r\ntext = \"Replace me by any text you'd like.\"\r\nencoded_input = tokenizer(text, return_tensors='pt')\r\noutput = model(**encoded_input)\r\n```\r\n\r\nand in TensorFlow:\r\n\r\n```python\r\nfrom transformers import RobertaTokenizer, TFRobertaModel\r\ntokenizer = RobertaTokenizer.from_pretrained('roberta-large')\r\nmodel = TFRobertaModel.from_pretrained('roberta-large')\r\ntext = \"Replace me by any text you'd like.\"\r\nencoded_input = tokenizer(text, return_tensors='tf')\r\noutput = model(encoded_input)\r\n```\r\n\r\n\r\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from\r\nneutral. Therefore, the model can have biased predictions:\r\n\r\n```python\r\n>>> from transformers import pipeline\r\n>>> unmasker = pipeline('fill-mask', model='roberta-large')\r\n>>> unmasker(\"The man worked as a <mask>.\")\r\n\r\n[{'sequence': '<s>The man worked as a mechanic.</s>',\r\n  'score': 0.08260300755500793,\r\n  'token': 25682,\r\n  'token_str': '\u0120mechanic'},\r\n {'sequence': '<s>The man worked as a driver.</s>',\r\n  'score': 0.05736079439520836,\r\n  'token': 1393,\r\n  'token_str': '\u0120driver'},\r\n {'sequence': '<s>The man worked as a teacher.</s>',\r\n  'score': 0.04709019884467125,\r\n  'token': 3254,\r\n  'token_str': '\u0120teacher'},\r\n {'sequence': '<s>The man worked as a bartender.</s>',\r\n  'score': 0.04641604796051979,\r\n  'token': 33080,\r\n  'token_str': '\u0120bartender'},\r\n {'sequence': '<s>The man worked as a waiter.</s>',\r\n  'score': 0.04239227622747421,\r\n  'token': 38233,\r\n  'token_str': '\u0120waiter'}]\r\n\r\n>>> unmasker(\"The woman worked as a <mask>.\")\r\n\r\n[{'sequence': '<s>The woman worked as a nurse.</s>',\r\n  'score': 0.2667474150657654,\r\n  'token': 9008,\r\n  'token_str': '\u0120nurse'},\r\n {'sequence': '<s>The woman worked as a waitress.</s>',\r\n  'score': 0.12280137836933136,\r\n  'token': 35698,\r\n  'token_str': '\u0120waitress'},\r\n {'sequence': '<s>The woman worked as a teacher.</s>',\r\n  'score': 0.09747499972581863,\r\n  'token': 3254,\r\n  'token_str': '\u0120teacher'},\r\n {'sequence': '<s>The woman worked as a secretary.</s>',\r\n  'score': 0.05783602222800255,\r\n  'token': 2971,\r\n  'token_str': '\u0120secretary'},\r\n {'sequence': '<s>The woman worked as a cleaner.</s>',\r\n  'score': 0.05576248839497566,\r\n  'token': 16126,\r\n  'token_str': '\u0120cleaner'}]\r\n```\r\n\r\nThis bias will also affect all fine-tuned versions of this model.",
      "usageInformation": "You can use this model directly with a pipeline for masked language modeling:\r\n\r\n```python\r\n>>> from transformers import pipeline\r\n>>> unmasker = pipeline('fill-mask', model='roberta-large')\r\n>>> unmasker(\"Hello I'm a <mask> model.\")\r\n\r\n[{'sequence': \"<s>Hello I'm a male model.</s>\",\r\n  'score': 0.3317350447177887,\r\n  'token': 2943,\r\n  'token_str': '\u0120male'},\r\n {'sequence': \"<s>Hello I'm a fashion model.</s>\",\r\n  'score': 0.14171843230724335,\r\n  'token': 2734,\r\n  'token_str': '\u0120fashion'},\r\n {'sequence': \"<s>Hello I'm a professional model.</s>\",\r\n  'score': 0.04291723668575287,\r\n  'token': 2038,\r\n  'token_str': '\u0120professional'},\r\n {'sequence': \"<s>Hello I'm a freelance model.</s>\",\r\n  'score': 0.02134818211197853,\r\n  'token': 18150,\r\n  'token_str': '\u0120freelance'},\r\n {'sequence': \"<s>Hello I'm a young model.</s>\",\r\n  'score': 0.021098261699080467,\r\n  'token': 664,\r\n  'token_str': '\u0120young'}]\r\n```\r\n\r\nHere is how to use this model to get the features of a given text in PyTorch:\r\n\r\n```python\r\nfrom transformers import RobertaTokenizer, RobertaModel\r\ntokenizer = RobertaTokenizer.from_pretrained('roberta-large')\r\nmodel = RobertaModel.from_pretrained('roberta-large')\r\ntext = \"Replace me by any text you'd like.\"\r\nencoded_input = tokenizer(text, return_tensors='pt')\r\noutput = model(**encoded_input)\r\n```\r\n\r\nand in TensorFlow:\r\n\r\n```python\r\nfrom transformers import RobertaTokenizer, TFRobertaModel\r\ntokenizer = RobertaTokenizer.from_pretrained('roberta-large')\r\nmodel = TFRobertaModel.from_pretrained('roberta-large')\r\ntext = \"Replace me by any text you'd like.\"\r\nencoded_input = tokenizer(text, return_tensors='tf')\r\noutput = model(encoded_input)\r\n```",
      "contentUrl": "https://huggingface.co/FacebookAI/roberta-large/resolve/main/model.safetensors",
      "url": "https://huggingface.co/FacebookAI/roberta-large",
      "license": "mit",
      "isPartOf": [],
      "README": "\r\n# RoBERTa large model\r\n\r\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\r\n[this paper](https://arxiv.org/abs/1907.11692) and first released in\r\n[this repository](https://github.com/pytorch/fairseq/tree/master/examples/roberta). This model is case-sensitive: it\r\nmakes a difference between english and English.\r\n\r\nDisclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by\r\nthe Hugging Face team.\r\n\r\n## Model description\r\n\r\nRoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means\r\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\r\npublicly available data) with an automatic process to generate inputs and labels from those texts. \r\n\r\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model\r\nrandomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict\r\nthe masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one\r\nafter the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to\r\nlearn a bidirectional representation of the sentence.\r\n\r\nThis way, the model learns an inner representation of the English language that can then be used to extract features\r\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\r\nclassifier using the features produced by the BERT model as inputs.\r\n\r\n## Intended uses & limitations\r\n\r\nYou can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task.\r\nSee the [model hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task that\r\ninterests you.\r\n\r\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\r\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\r\ngeneration you should look at model like GPT2.\r\n\r\n### How to use\r\n\r\nYou can use this model directly with a pipeline for masked language modeling:\r\n\r\n```python\r\n>>> from transformers import pipeline\r\n>>> unmasker = pipeline('fill-mask', model='roberta-large')\r\n>>> unmasker(\"Hello I'm a <mask> model.\")\r\n\r\n[{'sequence': \"<s>Hello I'm a male model.</s>\",\r\n  'score': 0.3317350447177887,\r\n  'token': 2943,\r\n  'token_str': '\u0120male'},\r\n {'sequence': \"<s>Hello I'm a fashion model.</s>\",\r\n  'score': 0.14171843230724335,\r\n  'token': 2734,\r\n  'token_str': '\u0120fashion'},\r\n {'sequence': \"<s>Hello I'm a professional model.</s>\",\r\n  'score': 0.04291723668575287,\r\n  'token': 2038,\r\n  'token_str': '\u0120professional'},\r\n {'sequence': \"<s>Hello I'm a freelance model.</s>\",\r\n  'score': 0.02134818211197853,\r\n  'token': 18150,\r\n  'token_str': '\u0120freelance'},\r\n {'sequence': \"<s>Hello I'm a young model.</s>\",\r\n  'score': 0.021098261699080467,\r\n  'token': 664,\r\n  'token_str': '\u0120young'}]\r\n```\r\n\r\nHere is how to use this model to get the features of a given text in PyTorch:\r\n\r\n```python\r\nfrom transformers import RobertaTokenizer, RobertaModel\r\ntokenizer = RobertaTokenizer.from_pretrained('roberta-large')\r\nmodel = RobertaModel.from_pretrained('roberta-large')\r\ntext = \"Replace me by any text you'd like.\"\r\nencoded_input = tokenizer(text, return_tensors='pt')\r\noutput = model(**encoded_input)\r\n```\r\n\r\nand in TensorFlow:\r\n\r\n```python\r\nfrom transformers import RobertaTokenizer, TFRobertaModel\r\ntokenizer = RobertaTokenizer.from_pretrained('roberta-large')\r\nmodel = TFRobertaModel.from_pretrained('roberta-large')\r\ntext = \"Replace me by any text you'd like.\"\r\nencoded_input = tokenizer(text, return_tensors='tf')\r\noutput = model(encoded_input)\r\n```\r\n\r\n### Limitations and bias\r\n\r\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from\r\nneutral. Therefore, the model can have biased predictions:\r\n\r\n```python\r\n>>> from transformers import pipeline\r\n>>> unmasker = pipeline('fill-mask', model='roberta-large')\r\n>>> unmasker(\"The man worked as a <mask>.\")\r\n\r\n[{'sequence': '<s>The man worked as a mechanic.</s>',\r\n  'score': 0.08260300755500793,\r\n  'token': 25682,\r\n  'token_str': '\u0120mechanic'},\r\n {'sequence': '<s>The man worked as a driver.</s>',\r\n  'score': 0.05736079439520836,\r\n  'token': 1393,\r\n  'token_str': '\u0120driver'},\r\n {'sequence': '<s>The man worked as a teacher.</s>',\r\n  'score': 0.04709019884467125,\r\n  'token': 3254,\r\n  'token_str': '\u0120teacher'},\r\n {'sequence': '<s>The man worked as a bartender.</s>',\r\n  'score': 0.04641604796051979,\r\n  'token': 33080,\r\n  'token_str': '\u0120bartender'},\r\n {'sequence': '<s>The man worked as a waiter.</s>',\r\n  'score': 0.04239227622747421,\r\n  'token': 38233,\r\n  'token_str': '\u0120waiter'}]\r\n\r\n>>> unmasker(\"The woman worked as a <mask>.\")\r\n\r\n[{'sequence': '<s>The woman worked as a nurse.</s>',\r\n  'score': 0.2667474150657654,\r\n  'token': 9008,\r\n  'token_str': '\u0120nurse'},\r\n {'sequence': '<s>The woman worked as a waitress.</s>',\r\n  'score': 0.12280137836933136,\r\n  'token': 35698,\r\n  'token_str': '\u0120waitress'},\r\n {'sequence': '<s>The woman worked as a teacher.</s>',\r\n  'score': 0.09747499972581863,\r\n  'token': 3254,\r\n  'token_str': '\u0120teacher'},\r\n {'sequence': '<s>The woman worked as a secretary.</s>',\r\n  'score': 0.05783602222800255,\r\n  'token': 2971,\r\n  'token_str': '\u0120secretary'},\r\n {'sequence': '<s>The woman worked as a cleaner.</s>',\r\n  'score': 0.05576248839497566,\r\n  'token': 16126,\r\n  'token_str': '\u0120cleaner'}]\r\n```\r\n\r\nThis bias will also affect all fine-tuned versions of this model.\r\n\r\n## Training data\r\n\r\nThe RoBERTa model was pretrained on the reunion of five datasets:\r\n- [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books;\r\n- [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers) ;\r\n- [CC-News](https://commoncrawl.org/2016/10/news-dataset-available/), a dataset containing 63 millions English news\r\n  articles crawled between September 2016 and February 2019.\r\n- [OpenWebText](https://github.com/jcpeterson/openwebtext), an opensource recreation of the WebText dataset used to\r\n  train GPT-2,\r\n- [Stories](https://arxiv.org/abs/1806.02847) a dataset containing a subset of CommonCrawl data filtered to match the\r\n  story-like style of Winograd schemas.\r\n\r\nTogether theses datasets weight 160GB of text.\r\n\r\n## Training procedure\r\n\r\n### Preprocessing\r\n\r\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of\r\nthe model take pieces of 512 contiguous token that may span over documents. The beginning of a new document is marked\r\nwith `<s>` and the end of one by `</s>`\r\n\r\nThe details of the masking procedure for each sentence are the following:\r\n- 15% of the tokens are masked.\r\n- In 80% of the cases, the masked tokens are replaced by `<mask>`.\r\n\r\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\r\n- In the 10% remaining cases, the masked tokens are left as is.\r\n\r\nContrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).\r\n\r\n### Pretraining\r\n\r\nThe model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512. The\r\noptimizer used is Adam with a learning rate of 4e-4, \\\\(\\beta_{1} = 0.9\\\\), \\\\(\\beta_{2} = 0.98\\\\) and\r\n\\\\(\\epsilon = 1e-6\\\\), a weight decay of 0.01, learning rate warmup for 30,000 steps and linear decay of the learning\r\nrate after.\r\n\r\n## Evaluation results\r\n\r\nWhen fine-tuned on downstream tasks, this model achieves the following results:\r\n\r\nGlue test results:\r\n\r\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\r\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\r\n|      | 90.2 | 92.2 | 94.7 | 96.4  | 68.0 | 96.4  | 90.9 | 86.6 |\r\n\r\n\r\n### BibTeX entry and citation info\r\n\r\n```bibtex\r\n@article{DBLP:journals/corr/abs-1907-11692,\r\n  author    = {Yinhan Liu and\r\n               Myle Ott and\r\n               Naman Goyal and\r\n               Jingfei Du and\r\n               Mandar Joshi and\r\n               Danqi Chen and\r\n               Omer Levy and\r\n               Mike Lewis and\r\n               Luke Zettlemoyer and\r\n               Veselin Stoyanov},\r\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\r\n  journal   = {CoRR},\r\n  volume    = {abs/1907.11692},\r\n  year      = {2019},\r\n  url       = {http://arxiv.org/abs/1907.11692},\r\n  archivePrefix = {arXiv},\r\n  eprint    = {1907.11692},\r\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\r\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\r\n  bibsource = {dblp computer science bibliography, https://dblp.org}\r\n}\r\n```\r\n\r\n<a href=\"https://huggingface.co/exbert/?model=roberta-base\">\r\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\r\n</a>\r\n"
    },
    {
      "@id": "ark:59852/model-laion-clap-htsat-fused-0djuztpt7f",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "laion/clap-htsat-fused",
      "description": "Model card for CLAP: Contrastive Language-Audio Pretraining",
      "author": "laion",
      "keywords": [
        "transformers",
        "pytorch",
        "safetensors",
        "clap",
        "feature-extraction",
        "arxiv:2211.06687",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "usageInformation": "You can use this model for zero shot audio classification or extracting audio and/or textual features.",
      "contentUrl": "https://huggingface.co/laion/clap-htsat-fused/resolve/main/model.safetensors",
      "url": "https://huggingface.co/laion/clap-htsat-fused",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "# Model card for CLAP\n\nModel card for CLAP: Contrastive Language-Audio Pretraining\n\n![clap_image](https://s3.amazonaws.com/moonup/production/uploads/1678811100805-62441d1d9fdefb55a0b7d12c.png)\n\n\n#  Table of Contents\n\n0. [TL;DR](#TL;DR)\n1. [Model Details](#model-details)\n2. [Usage](#usage)\n3. [Uses](#uses)\n4. [Citation](#citation)\n\n# TL;DR\n\nThe abstract of the paper states that: \n\n> Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks: text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models' results in the non-zero-shot setting. LAION-Audio-630K and the proposed model are both available to the public.\n\n\n# Usage\n\nYou can use this model for zero shot audio classification or extracting audio and/or textual features.\n\n# Uses\n\n## Perform zero-shot audio classification\n\n### Using `pipeline`\n\n```python\nfrom datasets import load_dataset\nfrom transformers import pipeline\n\ndataset = load_dataset(\"ashraq/esc50\")\naudio = dataset[\"train\"][\"audio\"][-1][\"array\"]\n\naudio_classifier = pipeline(task=\"zero-shot-audio-classification\", model=\"laion/clap-htsat-fused\")\noutput = audio_classifier(audio, candidate_labels=[\"Sound of a dog\", \"Sound of vaccum cleaner\"])\nprint(output)\n>>> [{\"score\": 0.999, \"label\": \"Sound of a dog\"}, {\"score\": 0.001, \"label\": \"Sound of vaccum cleaner\"}]\n```\n\n## Run the model:\n\nYou can also get the audio and text embeddings using `ClapModel`\n\n### Run the model on CPU:\n\n```python\nfrom datasets import load_dataset\nfrom transformers import ClapModel, ClapProcessor\n\nlibrispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\naudio_sample = librispeech_dummy[0]\n\nmodel = ClapModel.from_pretrained(\"laion/clap-htsat-fused\")\nprocessor = ClapProcessor.from_pretrained(\"laion/clap-htsat-fused\")\n\ninputs = processor(audios=audio_sample[\"audio\"][\"array\"], return_tensors=\"pt\")\naudio_embed = model.get_audio_features(**inputs)\n```\n\n### Run the model on GPU:\n\n```python\nfrom datasets import load_dataset\nfrom transformers import ClapModel, ClapProcessor\n\nlibrispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\naudio_sample = librispeech_dummy[0]\n\nmodel = ClapModel.from_pretrained(\"laion/clap-htsat-fused\").to(0)\nprocessor = ClapProcessor.from_pretrained(\"laion/clap-htsat-fused\")\n\ninputs = processor(audios=audio_sample[\"audio\"][\"array\"], return_tensors=\"pt\").to(0)\naudio_embed = model.get_audio_features(**inputs)\n```\n\n\n# Citation\n\nIf you are using this model for your work, please consider citing the original paper:\n```\n@misc{https://doi.org/10.48550/arxiv.2211.06687,\n  doi = {10.48550/ARXIV.2211.06687},\n  \n  url = {https://arxiv.org/abs/2211.06687},\n  \n  author = {Wu, Yusong and Chen, Ke and Zhang, Tianyu and Hui, Yuchen and Berg-Kirkpatrick, Taylor and Dubnov, Shlomo},\n  \n  keywords = {Sound (cs.SD), Audio and Speech Processing (eess.AS), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},\n  \n  title = {Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```"
    },
    {
      "@id": "ark:59852/model-pyannote-wespeaker-voxceleb-resnet34-lm-n2hexj7ztqu",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "pyannote/wespeaker-voxceleb-resnet34-LM",
      "description": "Using this open-source model in production?",
      "author": "pyannote",
      "keywords": [
        "pyannote-audio",
        "pytorch",
        "pyannote",
        "pyannote-audio-model",
        "wespeaker",
        "audio",
        "voice",
        "speech",
        "speaker",
        "speaker-recognition",
        "speaker-verification",
        "speaker-identification",
        "speaker-embedding",
        "dataset:voxceleb",
        "license:cc-by-4.0",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/voxceleb"
        }
      ],
      "usageInformation": "```python",
      "contentUrl": "https://huggingface.co/pyannote/wespeaker-voxceleb-resnet34-LM/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/pyannote/wespeaker-voxceleb-resnet34-LM",
      "license": "cc-by-4.0",
      "isPartOf": [],
      "README": "\nUsing this open-source model in production?  \nConsider switching to [pyannoteAI](https://www.pyannote.ai) for better and faster options.\n\n# \ud83c\udfb9 Wrapper around wespeaker-voxceleb-resnet34-LM\n\nThis model requires `pyannote.audio` version 3.1 or higher.\n\nThis is a wrapper around [WeSpeaker](https://github.com/wenet-e2e/wespeaker) `wespeaker-voxceleb-resnet34-LM` pretrained speaker embedding model, for use in `pyannote.audio`.\n\n## Basic usage\n\n```python\n# instantiate pretrained model\nfrom pyannote.audio import Model\nmodel = Model.from_pretrained(\"pyannote/wespeaker-voxceleb-resnet34-LM\")\n```\n\n```python\nfrom pyannote.audio import Inference\ninference = Inference(model, window=\"whole\")\nembedding1 = inference(\"speaker1.wav\")\nembedding2 = inference(\"speaker2.wav\")\n# `embeddingX` is (1 x D) numpy array extracted from the file as a whole.\n\nfrom scipy.spatial.distance import cdist\ndistance = cdist(embedding1, embedding2, metric=\"cosine\")[0,0]\n# `distance` is a `float` describing how dissimilar speakers 1 and 2 are.\n```\n\n## Advanced usage\n\n### Running on GPU\n\n```python\nimport torch\ninference.to(torch.device(\"cuda\"))\nembedding = inference(\"audio.wav\")\n```\n\n### Extract embedding from an excerpt\n\n```python\nfrom pyannote.audio import Inference\nfrom pyannote.core import Segment\ninference = Inference(model, window=\"whole\")\nexcerpt = Segment(13.37, 19.81)\nembedding = inference.crop(\"audio.wav\", excerpt)\n# `embedding` is (1 x D) numpy array extracted from the file excerpt.\n```\n\n### Extract embeddings using a sliding window\n\n```python\nfrom pyannote.audio import Inference\ninference = Inference(model, window=\"sliding\",\n                      duration=3.0, step=1.0)\nembeddings = inference(\"audio.wav\")\n# `embeddings` is a (N x D) pyannote.core.SlidingWindowFeature\n# `embeddings[i]` is the embedding of the ith position of the\n# sliding window, i.e. from [i * step, i * step + duration].\n```\n\n## License\n\nAccording to [this page](https://github.com/wenet-e2e/wespeaker/blob/master/docs/pretrained.md):\n\n> The pretrained model in WeNet follows the license of it's corresponding dataset. For example, the pretrained model on VoxCeleb follows Creative Commons Attribution 4.0 International License., since it is used as license of the VoxCeleb dataset, see https://mm.kaist.ac.kr/datasets/voxceleb/.\n\n## Citation\n\n```bibtex\n@inproceedings{Wang2023,\n  title={Wespeaker: A research and production oriented speaker embedding learning toolkit},\n  author={Wang, Hongji and Liang, Chengdong and Wang, Shuai and Chen, Zhengyang and Zhang, Binbin and Xiang, Xu and Deng, Yanlei and Qian, Yanmin},\n  booktitle={ICASSP 2023, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  pages={1--5},\n  year={2023},\n  organization={IEEE}\n}\n```\n\n```bibtex\n@inproceedings{Bredin23,\n  author={Herv\u00e9 Bredin},\n  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n  pages={1983--1987},\n  doi={10.21437/Interspeech.2023-105}\n}\n```\n"
    },
    {
      "@id": "ark:59852/model-sentence-transformers-paraphrase-multilingual-minilm-l12-v2-gbnva3jh0v",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
      "description": "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
      "author": "sentence-transformers",
      "keywords": [
        "sentence-transformers",
        "pytorch",
        "tf",
        "onnx",
        "safetensors",
        "openvino",
        "bert",
        "feature-extraction",
        "sentence-similarity",
        "transformers",
        "multilingual",
        "ar",
        "bg",
        "ca",
        "cs",
        "da",
        "de",
        "el",
        "en",
        "es",
        "et",
        "fa",
        "fi",
        "fr",
        "gl",
        "gu",
        "he",
        "hi",
        "hr",
        "hu",
        "hy",
        "id",
        "it",
        "ja",
        "ka",
        "ko",
        "ku",
        "lt",
        "lv",
        "mk",
        "mn",
        "mr",
        "ms",
        "my",
        "nb",
        "nl",
        "pl",
        "pt",
        "ro",
        "ru",
        "sk",
        "sl",
        "sq",
        "sr",
        "sv",
        "th",
        "tr",
        "uk",
        "ur",
        "vi",
        "arxiv:1908.10084",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-embeddings-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "usageInformation": "Using this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch",
      "contentUrl": "https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/model.safetensors",
      "url": "https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n\n\n## Full Model Architecture\n```\nSentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\n```\n\n## Citing & Authors\n\nThis model was trained by [sentence-transformers](https://www.sbert.net/). \n        \nIf you find this model helpful, feel free to cite our publication [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084):\n```bibtex \n@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n```"
    },
    {
      "@id": "ark:59852/model-pyannote-speaker-diarization-3-1-cejglftuic6",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "pyannote/speaker-diarization-3.1",
      "description": "Using this open-source model in production?",
      "author": "pyannote",
      "keywords": [
        "pyannote-audio",
        "pyannote",
        "pyannote-audio-pipeline",
        "audio",
        "voice",
        "speech",
        "speaker",
        "speaker-diarization",
        "speaker-change-detection",
        "voice-activity-detection",
        "overlapped-speech-detection",
        "automatic-speech-recognition",
        "arxiv:2111.14448",
        "arxiv:2012.01477",
        "license:mit",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "trainingDataset": [],
      "usageInformation": "```python",
      "url": "https://huggingface.co/pyannote/speaker-diarization-3.1",
      "license": "mit",
      "isPartOf": [],
      "README": "\nUsing this open-source model in production?  \nConsider switching to [pyannoteAI](https://www.pyannote.ai) for better and faster options.\n\n# \ud83c\udfb9 Speaker diarization 3.1\n\nThis pipeline is the same as [`pyannote/speaker-diarization-3.0`](https://hf.co/pyannote/speaker-diarization-3.1) except it removes the [problematic](https://github.com/pyannote/pyannote-audio/issues/1537) use of `onnxruntime`.  \nBoth speaker segmentation and embedding now run in pure PyTorch. This should ease deployment and possibly speed up inference.  \nIt requires pyannote.audio version 3.1 or higher.\n\nIt ingests mono audio sampled at 16kHz and outputs speaker diarization as an [`Annotation`](http://pyannote.github.io/pyannote-core/structure.html#annotation) instance:\n\n- stereo or multi-channel audio files are automatically downmixed to mono by averaging the channels.\n- audio files sampled at a different rate are resampled to 16kHz automatically upon loading.\n\n## Requirements\n\n1. Install [`pyannote.audio`](https://github.com/pyannote/pyannote-audio) `3.1` with `pip install pyannote.audio`\n2. Accept [`pyannote/segmentation-3.0`](https://hf.co/pyannote/segmentation-3.0) user conditions\n3. Accept [`pyannote/speaker-diarization-3.1`](https://hf.co/pyannote/speaker-diarization-3.1) user conditions\n4. Create access token at [`hf.co/settings/tokens`](https://hf.co/settings/tokens).\n\n## Usage\n\n```python\n# instantiate the pipeline\nfrom pyannote.audio import Pipeline\npipeline = Pipeline.from_pretrained(\n  \"pyannote/speaker-diarization-3.1\",\n  use_auth_token=\"HUGGINGFACE_ACCESS_TOKEN_GOES_HERE\")\n\n# run the pipeline on an audio file\ndiarization = pipeline(\"audio.wav\")\n\n# dump the diarization output to disk using RTTM format\nwith open(\"audio.rttm\", \"w\") as rttm:\n    diarization.write_rttm(rttm)\n```\n\n### Processing on GPU\n\n`pyannote.audio` pipelines run on CPU by default.\nYou can send them to GPU with the following lines:\n\n```python\nimport torch\npipeline.to(torch.device(\"cuda\"))\n```\n\n### Processing from memory\n\nPre-loading audio files in memory may result in faster processing:\n\n```python\nwaveform, sample_rate = torchaudio.load(\"audio.wav\")\ndiarization = pipeline({\"waveform\": waveform, \"sample_rate\": sample_rate})\n```\n\n### Monitoring progress\n\nHooks are available to monitor the progress of the pipeline:\n\n```python\nfrom pyannote.audio.pipelines.utils.hook import ProgressHook\nwith ProgressHook() as hook:\n    diarization = pipeline(\"audio.wav\", hook=hook)\n```\n\n### Controlling the number of speakers\n\nIn case the number of speakers is known in advance, one can use the `num_speakers` option:\n\n```python\ndiarization = pipeline(\"audio.wav\", num_speakers=2)\n```\n\nOne can also provide lower and/or upper bounds on the number of speakers using `min_speakers` and `max_speakers` options:\n\n```python\ndiarization = pipeline(\"audio.wav\", min_speakers=2, max_speakers=5)\n```\n\n## Benchmark\n\nThis pipeline has been benchmarked on a large collection of datasets.\n\nProcessing is fully automatic:\n\n- no manual voice activity detection (as is sometimes the case in the literature)\n- no manual number of speakers (though it is possible to provide it to the pipeline)\n- no fine-tuning of the internal models nor tuning of the pipeline hyper-parameters to each dataset\n\n... with the least forgiving diarization error rate (DER) setup (named _\"Full\"_ in [this paper](https://doi.org/10.1016/j.csl.2021.101254)):\n\n- no forgiveness collar\n- evaluation of overlapped speech\n\n| Benchmark                                                                                                                                   | [DER%](. \"Diarization error rate\") | [FA%](. \"False alarm rate\") | [Miss%](. \"Missed detection rate\") | [Conf%](. \"Speaker confusion rate\") | Expected output                                                                                                                                    | File-level evaluation                                                                                                                              |\n| ------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------- | --------------------------- | ---------------------------------- | ----------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [AISHELL-4](http://www.openslr.org/111/)                                                                                                    | 12.2                               | 3.8                         | 4.4                                | 4.0                                 | [RTTM](https://huggingface.co/pyannote/speaker-diarization-3.1/blob/main/reproducible_research/AISHELL.SpeakerDiarization.Benchmark.test.rttm)     | [eval](https://huggingface.co/pyannote/speaker-diarization-3.1/blob/main/reproducible_research/AISHELL.SpeakerDiarization.Benchmark.test.eval)     |\n| [AliMeeting (_channel 1_)](https://www.openslr.org/119/)                                                                                    | 24.4                               | 4.4                         | 10.0                               | 10.0                                | [RTTM](https://huggingface.co/pyannote/speaker-diarization-3.1/blob/main/reproducible_research/AliMeeting.SpeakerDiarization.Benchmark.test.rttm)  | [eval](https://huggingface.co/pyannote/speaker-diarization-3.1/blob/main/reproducible_research/AliMeeting.SpeakerDiarization.Benchmark.test.eval)  |\n| [AMI (_headset mix,_](https://groups.inf.ed.ac.uk/ami/corpus/) [_only_words_)](https://github.com/BUTSpeechFIT/AMI-diarization-setup)       | 18.8                               | 3.6                         | 9.5                                | 5.7                                 | [RTTM](https://huggingface.co/pyannote/speaker-diarization-3.1/blob/main/reproducible_research/AMI.SpeakerDiarization.Benchmark.test.rttm)         | [eval](https://huggingface.co/pyannote/speaker-diarization-3.1/blob/main/reproducible_research/AMI.SpeakerDiarization.Benchmark.test.eval)         |\n| [AMI (_array1, channel 1,_](https://groups.inf.ed.ac.uk/ami/corpus/) [_only_words)_](https://github.com/BUTSpeechFIT/AMI-diarization-setup) | 22.4                               | 3.8                         | 11.2                               | 7.5                                 | [RTTM](https://huggingface.co/pyannote/speaker-diarization-3.1/blob/main/reproducible_research/AMI-SDM.SpeakerDiarization.Benchmark.test.rttm)     | [eval](https://huggingface.co/pyannote/speaker-diarization-3.1/blob/main/reproducible_research/AMI-SDM.SpeakerDiarization.Benchmark.test.eval)     |\n| [AVA-AVD](https://arxiv.org/abs/2111.14448)                                                                                                 | 50.0                               | 10.8                        | 15.7                               | 23.4                                | [RTTM](https://huggingface.co/pyannote/speaker-diarization-3.1/blob/main/reproducible_research/AVA-AVD.SpeakerDiarization.Benchmark.test.rttm)     | [eval](https://huggingface.co/pyannote/speaker-diarization-3.1/blob/main/reproducible_research/AVA-AVD.SpeakerDiarization.Benchmark.test.eval)     |\n| [DIHARD 3 (_Full_)](https://arxiv.org/abs/2012.01477)                                                                                       | 21.7                               | 6.2                         | 8.1                                | 7.3                                 | [RTTM](https://huggingface.co/pyannote/speaker-diarization-3.1/blob/main/reproducible_research/DIHARD.SpeakerDiarization.Benchmark.test.rttm)      | [eval](https://huggingface.co/pyannote/speaker-diarization-3.1/blob/main/reproducible_research/DIHARD.SpeakerDiarization.Benchmark.test.eval)      |\n| [MSDWild](https://x-lance.github.io/MSDWILD/)                                                                                               | 25.3                               | 5.8                         | 8.0                                | 11.5                                | [RTTM](https://huggingface.co/pyannote/speaker-diarization-3.1/blob/main/reproducible_research/MSDWILD.SpeakerDiarization.Benchmark.test.rttm)     | [eval](https://huggingface.co/pyannote/speaker-diarization-3.1/blob/main/reproducible_research/MSDWILD.SpeakerDiarization.Benchmark.test.eval)     |\n| [REPERE (_phase 2_)](https://islrn.org/resources/360-758-359-485-0/)                                                                        | 7.8                                | 1.8                         | 2.6                                | 3.5                                 | [RTTM](https://huggingface.co/pyannote/speaker-diarization-3.1/blob/main/reproducible_research/REPERE.SpeakerDiarization.Benchmark.test.rttm)      | [eval](https://huggingface.co/pyannote/speaker-diarization-3.1/blob/main/reproducible_research/REPERE.SpeakerDiarization.Benchmark.test.eval)      |\n| [VoxConverse (_v0.3_)](https://github.com/joonson/voxconverse)                                                                              | 11.3                               | 4.1                         | 3.4                                | 3.8                                 | [RTTM](https://huggingface.co/pyannote/speaker-diarization-3.1/blob/main/reproducible_research/VoxConverse.SpeakerDiarization.Benchmark.test.rttm) | [eval](https://huggingface.co/pyannote/speaker-diarization-3.1/blob/main/reproducible_research/VoxConverse.SpeakerDiarization.Benchmark.test.eval) |\n\n## Citations\n\n```bibtex\n@inproceedings{Plaquet23,\n  author={Alexis Plaquet and Herv\u00e9 Bredin},\n  title={{Powerset multi-class cross entropy loss for neural speaker diarization}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}\n```\n\n```bibtex\n@inproceedings{Bredin23,\n  author={Herv\u00e9 Bredin},\n  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},\n  year=2023,\n  booktitle={Proc. INTERSPEECH 2023},\n}\n```\n"
    },
    {
      "@id": "ark:59852/model-bingsu-adetailer-vm3x80uxna",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "Bingsu/adetailer",
      "description": "- [Anime Face CreateML](https://universe.roboflow.com/my-workspace-mph8o/anime-face-createml)",
      "author": "Bingsu",
      "keywords": [
        "ultralytics",
        "pytorch",
        "dataset:wider_face",
        "dataset:skytnt/anime-segmentation",
        "doi:10.57967/hf/3633",
        "license:apache-2.0",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/wider_face"
        },
        {
          "@id": "https://huggingface.co/datasets/skytnt/anime-segmentation"
        }
      ],
      "usageInformation": "```python\nfrom huggingface_hub import hf_hub_download\nfrom ultralytics import YOLO\n\npath = hf_hub_download(\"Bingsu/adetailer\", \"face_yolov8n.pt\")\nmodel = YOLO(path)\n```\n\n```python\nimport cv2\nfrom PIL import Image\n\nimg = \"https://farm5.staticflickr.com/4139/4887614566_6b57ec4422_z.jpg\"\noutput = model(img)\npred = output[0].plot()\npred = cv2.cvtColor(pred, cv2.COLOR_BGR2RGB)\npred = Image.fromarray(pred)\npred\n```\n\n![image](https://i.imgur.com/9ny1wmD.png)",
      "url": "https://huggingface.co/Bingsu/adetailer",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# YOLOv8 Detection Model\n\n## Datasets\n\n### Face\n\n- [Anime Face CreateML](https://universe.roboflow.com/my-workspace-mph8o/anime-face-createml)\n- [xml2txt](https://universe.roboflow.com/0oooooo0/xml2txt-njqx1)\n- [AN](https://universe.roboflow.com/sed-b8vkf/an-lfg5i)\n- [wider face](http://shuoyang1213.me/WIDERFACE/index.html)\n\n### Hand\n\n- [AnHDet](https://universe.roboflow.com/1-yshhi/anhdet)\n- [hand-detection-fuao9](https://universe.roboflow.com/catwithawand/hand-detection-fuao9)\n\n### Person\n\n- [coco2017](https://cocodataset.org/#home) (only person)\n- [AniSeg](https://github.com/jerryli27/AniSeg)\n- [skytnt/anime-segmentation](https://huggingface.co/datasets/skytnt/anime-segmentation)\n\n### deepfashion2\n\n- [deepfashion2](https://github.com/switchablenorms/DeepFashion2)\n\n| id  | label                 |\n| --- | --------------------- |\n| 0   | short_sleeved_shirt   |\n| 1   | long_sleeved_shirt    |\n| 2   | short_sleeved_outwear |\n| 3   | long_sleeved_outwear  |\n| 4   | vest                  |\n| 5   | sling                 |\n| 6   | shorts                |\n| 7   | trousers              |\n| 8   | skirt                 |\n| 9   | short_sleeved_dress   |\n| 10  | long_sleeved_dress    |\n| 11  | vest_dress            |\n| 12  | sling_dress           |\n\n## Info\n\n| Model                       | Target                | mAP 50                        | mAP 50-95                     |\n| --------------------------- | --------------------- | ----------------------------- | ----------------------------- |\n| face_yolov8n.pt             | 2D / realistic face   | 0.660                         | 0.366                         |\n| face_yolov8n_v2.pt          | 2D / realistic face   | 0.669                         | 0.372                         |\n| face_yolov8s.pt             | 2D / realistic face   | 0.713                         | 0.404                         |\n| face_yolov8m.pt             | 2D / realistic face   | 0.737                         | 0.424                         |\n| face_yolov9c.pt             | 2D / realistic face   | 0.748                         | 0.433                         |\n| hand_yolov8n.pt             | 2D / realistic hand   | 0.767                         | 0.505                         |\n| hand_yolov8s.pt             | 2D / realistic hand   | 0.794                         | 0.527                         |\n| hand_yolov9c.pt             | 2D / realistic hand   | 0.810                         | 0.550                         |\n| person_yolov8n-seg.pt       | 2D / realistic person | 0.782 (bbox)<br/>0.761 (mask) | 0.555 (bbox)<br/>0.460 (mask) |\n| person_yolov8s-seg.pt       | 2D / realistic person | 0.824 (bbox)<br/>0.809 (mask) | 0.605 (bbox)<br/>0.508 (mask) |\n| person_yolov8m-seg.pt       | 2D / realistic person | 0.849 (bbox)<br/>0.831 (mask) | 0.636 (bbox)<br/>0.533 (mask) |\n| deepfashion2_yolov8s-seg.pt | realistic clothes     | 0.849 (bbox)<br/>0.840 (mask) | 0.763 (bbox)<br/>0.675 (mask) |\n\n## Usage\n\n```python\nfrom huggingface_hub import hf_hub_download\nfrom ultralytics import YOLO\n\npath = hf_hub_download(\"Bingsu/adetailer\", \"face_yolov8n.pt\")\nmodel = YOLO(path)\n```\n\n```python\nimport cv2\nfrom PIL import Image\n\nimg = \"https://farm5.staticflickr.com/4139/4887614566_6b57ec4422_z.jpg\"\noutput = model(img)\npred = output[0].plot()\npred = cv2.cvtColor(pred, cv2.COLOR_BGR2RGB)\npred = Image.fromarray(pred)\npred\n```\n\n![image](https://i.imgur.com/9ny1wmD.png)\n\n\n## Unsafe files\n\n![image](https://i.imgur.com/9Btuy8j.png)\n\nSince `getattr` is classified as a dangerous pickle function, any segmentation model that uses it is classified as unsafe.\n\nAll models were created and saved using the official [ultralytics](https://github.com/ultralytics/ultralytics) library, so it's okay to use files downloaded from a trusted source.\n\nSee also: https://huggingface.co/docs/hub/security-pickle\n"
    },
    {
      "@id": "ark:59852/model-distilbert-distilbert-base-uncased-cejglfruyy",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "distilbert/distilbert-base-uncased",
      "description": "This model is a distilled version of the [BERT base model](https://huggingface.co/bert-base-uncased). It was",
      "author": "distilbert",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "rust",
        "safetensors",
        "distilbert",
        "fill-mask",
        "exbert",
        "en",
        "dataset:bookcorpus",
        "dataset:wikipedia",
        "arxiv:1910.01108",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/bookcorpus"
        },
        {
          "@id": "https://huggingface.co/datasets/wikipedia"
        }
      ],
      "hasBias": "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions. It also inherits some of\n[the bias of its teacher model](https://huggingface.co/bert-base-uncased#limitations-and-bias).\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"The White man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the white man worked as a blacksmith. [SEP]',\n  'score': 0.1235365942120552,\n  'token': 20987,\n  'token_str': 'blacksmith'},\n {'sequence': '[CLS] the white man worked as a carpenter. [SEP]',\n  'score': 0.10142576694488525,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the white man worked as a farmer. [SEP]',\n  'score': 0.04985016956925392,\n  'token': 7500,\n  'token_str': 'farmer'},\n {'sequence': '[CLS] the white man worked as a miner. [SEP]',\n  'score': 0.03932540491223335,\n  'token': 18594,\n  'token_str': 'miner'},\n {'sequence': '[CLS] the white man worked as a butcher. [SEP]',\n  'score': 0.03351764753460884,\n  'token': 14998,\n  'token_str': 'butcher'}]\n\n>>> unmasker(\"The Black woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the black woman worked as a waitress. [SEP]',\n  'score': 0.13283951580524445,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the black woman worked as a nurse. [SEP]',\n  'score': 0.12586183845996857,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the black woman worked as a maid. [SEP]',\n  'score': 0.11708822101354599,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the black woman worked as a prostitute. [SEP]',\n  'score': 0.11499975621700287,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the black woman worked as a housekeeper. [SEP]',\n  'score': 0.04722772538661957,\n  'token': 22583,\n  'token_str': 'housekeeper'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.",
      "intendedUseCase": "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=distilbert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.05292855575680733,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.03968575969338417,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a business model. [SEP]\",\n  'score': 0.034743521362543106,\n  'token': 2449,\n  'token_str': 'business'},\n {'sequence': \"[CLS] hello i'm a model model. [SEP]\",\n  'score': 0.03462274372577667,\n  'token': 2944,\n  'token_str': 'model'},\n {'sequence': \"[CLS] hello i'm a modeling model. [SEP]\",\n  'score': 0.018145186826586723,\n  'token': 11643,\n  'token_str': 'modeling'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import DistilBertTokenizer, DistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import DistilBertTokenizer, TFDistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions. It also inherits some of\n[the bias of its teacher model](https://huggingface.co/bert-base-uncased#limitations-and-bias).\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"The White man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the white man worked as a blacksmith. [SEP]',\n  'score': 0.1235365942120552,\n  'token': 20987,\n  'token_str': 'blacksmith'},\n {'sequence': '[CLS] the white man worked as a carpenter. [SEP]',\n  'score': 0.10142576694488525,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the white man worked as a farmer. [SEP]',\n  'score': 0.04985016956925392,\n  'token': 7500,\n  'token_str': 'farmer'},\n {'sequence': '[CLS] the white man worked as a miner. [SEP]',\n  'score': 0.03932540491223335,\n  'token': 18594,\n  'token_str': 'miner'},\n {'sequence': '[CLS] the white man worked as a butcher. [SEP]',\n  'score': 0.03351764753460884,\n  'token': 14998,\n  'token_str': 'butcher'}]\n\n>>> unmasker(\"The Black woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the black woman worked as a waitress. [SEP]',\n  'score': 0.13283951580524445,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the black woman worked as a nurse. [SEP]',\n  'score': 0.12586183845996857,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the black woman worked as a maid. [SEP]',\n  'score': 0.11708822101354599,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the black woman worked as a prostitute. [SEP]',\n  'score': 0.11499975621700287,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the black woman worked as a housekeeper. [SEP]',\n  'score': 0.04722772538661957,\n  'token': 22583,\n  'token_str': 'housekeeper'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.",
      "usageInformation": "You can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.05292855575680733,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.03968575969338417,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a business model. [SEP]\",\n  'score': 0.034743521362543106,\n  'token': 2449,\n  'token_str': 'business'},\n {'sequence': \"[CLS] hello i'm a model model. [SEP]\",\n  'score': 0.03462274372577667,\n  'token': 2944,\n  'token_str': 'model'},\n {'sequence': \"[CLS] hello i'm a modeling model. [SEP]\",\n  'score': 0.018145186826586723,\n  'token': 11643,\n  'token_str': 'modeling'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import DistilBertTokenizer, DistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import DistilBertTokenizer, TFDistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```",
      "contentUrl": "https://huggingface.co/distilbert/distilbert-base-uncased/resolve/main/model.safetensors",
      "url": "https://huggingface.co/distilbert/distilbert-base-uncased",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# DistilBERT base model (uncased)\n\nThis model is a distilled version of the [BERT base model](https://huggingface.co/bert-base-uncased). It was\nintroduced in [this paper](https://arxiv.org/abs/1910.01108). The code for the distillation process can be found\n[here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation). This model is uncased: it does\nnot make a difference between english and English.\n\n## Model description\n\nDistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a\nself-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,\nwith no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic\nprocess to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained\nwith three objectives:\n\n- Distillation loss: the model was trained to return the same probabilities as the BERT base model.\n- Masked language modeling (MLM): this is part of the original training loss of the BERT base model. When taking a\n  sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the\n  model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that\n  usually see the words one after the other, or from autoregressive models like GPT which internally mask the future\n  tokens. It allows the model to learn a bidirectional representation of the sentence.\n- Cosine embedding loss: the model was also trained to generate hidden states as close as possible as the BERT base\n  model.\n\nThis way, the model learns the same inner representation of the English language than its teacher model, while being\nfaster for inference or downstream tasks.\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=distilbert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.05292855575680733,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.03968575969338417,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a business model. [SEP]\",\n  'score': 0.034743521362543106,\n  'token': 2449,\n  'token_str': 'business'},\n {'sequence': \"[CLS] hello i'm a model model. [SEP]\",\n  'score': 0.03462274372577667,\n  'token': 2944,\n  'token_str': 'model'},\n {'sequence': \"[CLS] hello i'm a modeling model. [SEP]\",\n  'score': 0.018145186826586723,\n  'token': 11643,\n  'token_str': 'modeling'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import DistilBertTokenizer, DistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import DistilBertTokenizer, TFDistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions. It also inherits some of\n[the bias of its teacher model](https://huggingface.co/bert-base-uncased#limitations-and-bias).\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n>>> unmasker(\"The White man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the white man worked as a blacksmith. [SEP]',\n  'score': 0.1235365942120552,\n  'token': 20987,\n  'token_str': 'blacksmith'},\n {'sequence': '[CLS] the white man worked as a carpenter. [SEP]',\n  'score': 0.10142576694488525,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the white man worked as a farmer. [SEP]',\n  'score': 0.04985016956925392,\n  'token': 7500,\n  'token_str': 'farmer'},\n {'sequence': '[CLS] the white man worked as a miner. [SEP]',\n  'score': 0.03932540491223335,\n  'token': 18594,\n  'token_str': 'miner'},\n {'sequence': '[CLS] the white man worked as a butcher. [SEP]',\n  'score': 0.03351764753460884,\n  'token': 14998,\n  'token_str': 'butcher'}]\n\n>>> unmasker(\"The Black woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the black woman worked as a waitress. [SEP]',\n  'score': 0.13283951580524445,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the black woman worked as a nurse. [SEP]',\n  'score': 0.12586183845996857,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the black woman worked as a maid. [SEP]',\n  'score': 0.11708822101354599,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the black woman worked as a prostitute. [SEP]',\n  'score': 0.11499975621700287,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the black woman worked as a housekeeper. [SEP]',\n  'score': 0.04722772538661957,\n  'token': 22583,\n  'token_str': 'housekeeper'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nDistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset\nconsisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia)\n(excluding lists, tables and headers).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 8 16 GB V100 for 90 hours. See the\n[training code](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for all hyperparameters\ndetails.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\n|      | 82.2 | 88.5 | 89.2 | 91.3  | 51.3 | 85.8  | 87.5 | 59.9 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.01108}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=distilbert-base-uncased\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n"
    },
    {
      "@id": "ark:59852/model-omni-research-tarsier2-recap-7b-evzs5nf69w",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "omni-research/Tarsier2-Recap-7b",
      "description": "Tarsier2-Recap-7b is build upon [Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct) by distilling the video description capabilities of [Tarsier2-7b](https://huggingface.co/omni-research/Tarsier2-7b-0115). Specifically, we finetuned Qwen2-VL-7B-Instruct on [Tarsier2-Recap-585K](https://huggingface.co/datasets/omni-research/Tarsier2-Recap-585K) for 2 epochs with a learning rate of 2e-5. Tarsier2-Recap-7b shares a similar video captioning ability as Tarsier2-7b, reaching an overall F1 score of 40.7% on [DREAM-1K](https://tarsier-vlm.github.io/), which is only behind Tarsier2-7b (42.0%) and surpasses GPT-4o's 39.2%. See the [Tarsier2 technical report](https://arxiv.org/abs/2501.07888) for more details.",
      "author": "omni-research",
      "keywords": [
        "safetensors",
        "video LLM",
        "arxiv:2501.07888",
        "license:apache-2.0",
        "region:us"
      ],
      "version": "1.0",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "intendedUseCase": "**Primary intended uses:**\r\nThe primary use of Tarsier is research on large multimodal models, especially video description.\r\n\r\n**Primary intended users:**\r\nThe primary intended users of the model are researchers and hobbyists in computer vision, natural language processing, machine learning, and artificial intelligence.",
      "usageInformation": "see https://github.com/bytedance/tarsier?tab=readme-ov-file#usage.\r\n\r\n**Where to send questions or comments about the model:**\r\nhttps://github.com/bytedance/tarsier/issues",
      "contentUrl": "https://huggingface.co/omni-research/Tarsier2-Recap-7b/resolve/main/model-00001-of-00004.safetensors",
      "url": "https://huggingface.co/omni-research/Tarsier2-Recap-7b",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\r\n\r\n# Tarsier Model Card\r\n## Introduction\r\nTarsier2-Recap-7b is build upon [Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct) by distilling the video description capabilities of [Tarsier2-7b](https://huggingface.co/omni-research/Tarsier2-7b-0115). Specifically, we finetuned Qwen2-VL-7B-Instruct on [Tarsier2-Recap-585K](https://huggingface.co/datasets/omni-research/Tarsier2-Recap-585K) for 2 epochs with a learning rate of 2e-5. Tarsier2-Recap-7b shares a similar video captioning ability as Tarsier2-7b, reaching an overall F1 score of 40.7% on [DREAM-1K](https://tarsier-vlm.github.io/), which is only behind Tarsier2-7b (42.0%) and surpasses GPT-4o's 39.2%. See the [Tarsier2 technical report](https://arxiv.org/abs/2501.07888) for more details.\r\n\r\n_**Note**: Please use [Tarsier2-7b](https://huggingface.co/omni-research/Tarsier2-7b-0115) if you need the full-blooded Tarsier2._\r\n\r\n## Model details\r\n- Base Model: [Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct)\r\n- Training Data: [Tarsier2-Recap-585K](https://huggingface.co/datasets/omni-research/Tarsier2-Recap-585K)\r\n\r\n**Model date:**\r\nTarsier2-Recap-7b was trained in December 2024.\r\n\r\n**Paper or resources for more information:**\r\n- github repo: https://github.com/bytedance/tarsier/tree/tarsier2\r\n- paper link: https://arxiv.org/abs/2501.07888\r\n- leaderboard: https://tarsier-vlm.github.io/\r\n\r\n## License\r\nQwen/Qwen2-VL-7B-Instruct license.\r\n\r\n## Intended use\r\n**Primary intended uses:**\r\nThe primary use of Tarsier is research on large multimodal models, especially video description.\r\n\r\n**Primary intended users:**\r\nThe primary intended users of the model are researchers and hobbyists in computer vision, natural language processing, machine learning, and artificial intelligence.\r\n\r\n## Model Performance\r\n### Video Description\r\nWe evaluate Tarsier2-Recap-7b on DREAM-1K, a detailed video description benchmark featuring dynamic and diverse videos, assessing the model\u2019s ability to describe fine-grained actions and events. Here is the evaluation result:\r\n![images](./assets/dream-1k_results.png)\r\n_Note: The results of Tarsier2-Recap-7b is different from the results we reported in Table 11 in the [Tarsier2 technical report](https://arxiv.org/abs/2501.07888), as Tarsier2-Recap-7b is more fully trained (2 epochs vs 1 epoch)._\r\n\r\n### Video Question-Answering\r\nWe evalute Tarsier2-Recap-7b on [TVBench](https://paperswithcode.com/sota/video-question-answering-on-tvbench), a novel multiple-choice question-answering which requires a high level of temporal understanding. As Tarsier2-Recap-7b is only trained with video caption data, it needs some additional prompt to enduce it to conduct multi-choice question-answering tasks, see [TVBench](https://github.com/bytedance/tarsier/blob/tarsier2/data/annotations/TVBench.jsonl) samples as an example. Here is the evaluation result:\r\n\r\n  | Task    | Tarsier2-Recap-7b | Tarsier2-7b |\r\n  | ------- | :--------: | :-------: |\r\n  |    Action Antonym   |   91.2   |   94.1   |\r\n  |    Action Count   |   43.1   |   40.5   |\r\n  |    Action Localization   |   42.5   |   37.5   |\r\n  |    Action Sequence   |   70.5   |   72.3   |\r\n  |    Egocentric Sequence   |   22.0   |   24.5   |\r\n  |    Moving Direction   |   37.1   |   33.2   |\r\n  |    Object Count   |   46.6   |   62.8   |\r\n  |    Object Shuffle   |   36.9  |   31.6   |\r\n  |    Scene Transition   |   85.9   |   88.1   |\r\n  |    Unexpected Action  |   28.0   |   41.5   |\r\n  | OVERALL |   54.0   |   54.7   |\r\n\r\n\r\n## How to Use\r\nsee https://github.com/bytedance/tarsier?tab=readme-ov-file#usage.\r\n\r\n**Where to send questions or comments about the model:**\r\nhttps://github.com/bytedance/tarsier/issues"
    },
    {
      "@id": "ark:59852/model-openai-clip-vit-large-patch14-vlpktz7mnjv",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "openai/clip-vit-large-patch14",
      "description": "Disclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md).",
      "author": "openai",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "safetensors",
        "clip",
        "zero-shot-image-classification",
        "vision",
        "arxiv:2103.00020",
        "arxiv:1908.04913",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "hasBias": "We find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with \u2018Middle Eastern\u2019 having the highest accuracy (98.4%) and \u2018White\u2019 having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.",
      "intendedUseCase": "The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n\nThe primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP\u2019s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.",
      "contentUrl": "https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/model.safetensors",
      "url": "https://huggingface.co/openai/clip-vit-large-patch14",
      "isPartOf": [],
      "README": "\n# Model Card: CLIP\n\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md).\n\n## Model Details\n\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they\u2019re being deployed within.\n\n### Model Date\n\nJanuary 2021\n\n### Model Type\n\nThe base model uses a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n\n- [Blog Post](https://openai.com/blog/clip/)\n- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n\n\n### Use with Transformers\n\n```python\nfrom PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\n\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n#### Primary intended uses\n\nThe primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n### Out-of-Scope Use Cases\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP\u2019s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n\n## Data\n\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n### Data Mission Statement\n\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n\n## Performance and Limitations\n\n### Performance\n\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n## Limitations\n\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n### Bias and Fairness\n\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with \u2018Middle Eastern\u2019 having the highest accuracy (98.4%) and \u2018White\u2019 having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.\n\n\n\n## Feedback\n\n### Where to send questions or comments about the model\n\nPlease use [this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)"
    },
    {
      "@id": "ark:59852/model-openai-community-gpt2-tsm97owyvvw",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "openai-community/gpt2",
      "description": "Test the whole generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large",
      "author": "openai-community",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "tflite",
        "rust",
        "onnx",
        "safetensors",
        "gpt2",
        "text-generation",
        "exbert",
        "en",
        "doi:10.57967/hf/0039",
        "license:mit",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-generation",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "hasBias": "The training data used for this model has not been released as a dataset one can browse. We know it contains a lot of\nunfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their\n[model card](https://github.com/openai/gpt-2/blob/master/model_card.md#out-of-scope-use-cases):\n\n> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don\u2019t support use-cases\n> that require the generated text to be true.\n>\n> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do\n> not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a\n> study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race,\n> and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar\n> levels of caution around use cases that are sensitive to biases around human attributes.\n\nHere's an example of how the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"The White man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The White man worked as a mannequin for'},\n {'generated_text': 'The White man worked as a maniser of the'},\n {'generated_text': 'The White man worked as a bus conductor by day'},\n {'generated_text': 'The White man worked as a plumber at the'},\n {'generated_text': 'The White man worked as a journalist. He had'}]\n\n>>> set_seed(42)\n>>> generator(\"The Black man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The Black man worked as a man at a restaurant'},\n {'generated_text': 'The Black man worked as a car salesman in a'},\n {'generated_text': 'The Black man worked as a police sergeant at the'},\n {'generated_text': 'The Black man worked as a man-eating monster'},\n {'generated_text': 'The Black man worked as a slave, and was'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.",
      "intendedUseCase": "You can use the raw model for text generation or fine-tune it to a downstream task. See the\n[model hub](https://huggingface.co/models?filter=gpt2) to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': \"Hello, I'm a language model, a language for thinking, a language for expressing thoughts.\"},\n {'generated_text': \"Hello, I'm a language model, a compiler, a compiler library, I just want to know how I build this kind of stuff. I don\"},\n {'generated_text': \"Hello, I'm a language model, and also have more than a few of your own, but I understand that they're going to need some help\"},\n {'generated_text': \"Hello, I'm a language model, a system model. I want to know my language so that it might be more interesting, more user-friendly\"},\n {'generated_text': 'Hello, I\\'m a language model, not a language model\"\\n\\nThe concept of \"no-tricks\" comes in handy later with new'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = TFGPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n\nThe training data used for this model has not been released as a dataset one can browse. We know it contains a lot of\nunfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their\n[model card](https://github.com/openai/gpt-2/blob/master/model_card.md#out-of-scope-use-cases):\n\n> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don\u2019t support use-cases\n> that require the generated text to be true.\n>\n> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do\n> not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a\n> study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race,\n> and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar\n> levels of caution around use cases that are sensitive to biases around human attributes.\n\nHere's an example of how the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"The White man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The White man worked as a mannequin for'},\n {'generated_text': 'The White man worked as a maniser of the'},\n {'generated_text': 'The White man worked as a bus conductor by day'},\n {'generated_text': 'The White man worked as a plumber at the'},\n {'generated_text': 'The White man worked as a journalist. He had'}]\n\n>>> set_seed(42)\n>>> generator(\"The Black man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The Black man worked as a man at a restaurant'},\n {'generated_text': 'The Black man worked as a car salesman in a'},\n {'generated_text': 'The Black man worked as a police sergeant at the'},\n {'generated_text': 'The Black man worked as a man-eating monster'},\n {'generated_text': 'The Black man worked as a slave, and was'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.",
      "usageInformation": "You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': \"Hello, I'm a language model, a language for thinking, a language for expressing thoughts.\"},\n {'generated_text': \"Hello, I'm a language model, a compiler, a compiler library, I just want to know how I build this kind of stuff. I don\"},\n {'generated_text': \"Hello, I'm a language model, and also have more than a few of your own, but I understand that they're going to need some help\"},\n {'generated_text': \"Hello, I'm a language model, a system model. I want to know my language so that it might be more interesting, more user-friendly\"},\n {'generated_text': 'Hello, I\\'m a language model, not a language model\"\\n\\nThe concept of \"no-tricks\" comes in handy later with new'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = TFGPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```",
      "contentUrl": "https://huggingface.co/openai-community/gpt2/resolve/main/model.safetensors",
      "url": "https://huggingface.co/openai-community/gpt2",
      "license": "mit",
      "isPartOf": [],
      "README": "\n\n# GPT-2\n\nTest the whole generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large\n\nPretrained model on English language using a causal language modeling (CLM) objective. It was introduced in\n[this paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\nand first released at [this page](https://openai.com/blog/better-language-models/).\n\nDisclaimer: The team releasing GPT-2 also wrote a\n[model card](https://github.com/openai/gpt-2/blob/master/model_card.md) for their model. Content from this model card\nhas been written by the Hugging Face team to complete the information they provided and give specific examples of bias.\n\n## Model description\n\nGPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This\nmeans it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots\nof publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely,\nit was trained to guess the next word in sentences.\n\nMore precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence,\nshifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the\npredictions for the token `i` only uses the inputs from `1` to `i` but not the future tokens.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a\nprompt.\n\nThis is the **smallest** version of GPT-2, with 124M parameters. \n\n**Related Models:** [GPT-Large](https://huggingface.co/gpt2-large), [GPT-Medium](https://huggingface.co/gpt2-medium) and [GPT-XL](https://huggingface.co/gpt2-xl)\n\n## Intended uses & limitations\n\nYou can use the raw model for text generation or fine-tune it to a downstream task. See the\n[model hub](https://huggingface.co/models?filter=gpt2) to look for fine-tuned versions on a task that interests you.\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we\nset a seed for reproducibility:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': \"Hello, I'm a language model, a language for thinking, a language for expressing thoughts.\"},\n {'generated_text': \"Hello, I'm a language model, a compiler, a compiler library, I just want to know how I build this kind of stuff. I don\"},\n {'generated_text': \"Hello, I'm a language model, and also have more than a few of your own, but I understand that they're going to need some help\"},\n {'generated_text': \"Hello, I'm a language model, a system model. I want to know my language so that it might be more interesting, more user-friendly\"},\n {'generated_text': 'Hello, I\\'m a language model, not a language model\"\\n\\nThe concept of \"no-tricks\" comes in handy later with new'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = TFGPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model has not been released as a dataset one can browse. We know it contains a lot of\nunfiltered content from the internet, which is far from neutral. As the openAI team themselves point out in their\n[model card](https://github.com/openai/gpt-2/blob/master/model_card.md#out-of-scope-use-cases):\n\n> Because large-scale language models like GPT-2 do not distinguish fact from fiction, we don\u2019t support use-cases\n> that require the generated text to be true.\n>\n> Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do\n> not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a\n> study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race,\n> and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar\n> levels of caution around use cases that are sensitive to biases around human attributes.\n\nHere's an example of how the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"The White man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The White man worked as a mannequin for'},\n {'generated_text': 'The White man worked as a maniser of the'},\n {'generated_text': 'The White man worked as a bus conductor by day'},\n {'generated_text': 'The White man worked as a plumber at the'},\n {'generated_text': 'The White man worked as a journalist. He had'}]\n\n>>> set_seed(42)\n>>> generator(\"The Black man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The Black man worked as a man at a restaurant'},\n {'generated_text': 'The Black man worked as a car salesman in a'},\n {'generated_text': 'The Black man worked as a police sergeant at the'},\n {'generated_text': 'The Black man worked as a man-eating monster'},\n {'generated_text': 'The Black man worked as a slave, and was'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web\npages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from\nthis dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights\n40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText\n[here](https://github.com/openai/gpt-2/blob/master/domains.txt).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a\nvocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.\n\nThe larger model was trained on 256 cloud TPU v3 cores. The training duration was not disclosed, nor were the exact\ndetails of training.\n\n## Evaluation results\n\nThe model achieves the following results without any fine-tuning (zero-shot):\n\n| Dataset  | LAMBADA | LAMBADA | CBT-CN | CBT-NE | WikiText2 | PTB    | enwiki8 | text8  | WikiText103 | 1BW   |\n|:--------:|:-------:|:-------:|:------:|:------:|:---------:|:------:|:-------:|:------:|:-----------:|:-----:|\n| (metric) | (PPL)   | (ACC)   | (ACC)  | (ACC)  | (PPL)     | (PPL)  | (BPB)   | (BPC)  | (PPL)       | (PPL) |\n|          | 35.13   | 45.99   | 87.65  | 83.4   | 29.41     | 65.85  | 1.16    | 1,17   | 37.50       | 75.20 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{radford2019language,\n  title={Language Models are Unsupervised Multitask Learners},\n  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},\n  year={2019}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=gpt2\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n"
    },
    {
      "@id": "ark:59852/model-colbert-ir-colbertv2-0-wgpq2kzfrly",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "colbert-ir/colbertv2.0",
      "description": "<p align=\"center\">",
      "author": "colbert-ir",
      "keywords": [
        "transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "bert",
        "ColBERT",
        "en",
        "arxiv:2004.12832",
        "arxiv:2007.00814",
        "arxiv:2101.00436",
        "arxiv:2112.01488",
        "arxiv:2205.09707",
        "license:mit",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "usageInformation": "**NEW**: We have an experimental notebook on [Google Colab](https://colab.research.google.com/github/stanford-futuredata/ColBERT/blob/main/docs/intro2new.ipynb) that you can use with free GPUs. Indexing 10,000 on the free Colab T4 GPU takes six minutes.\n\nThis Jupyter notebook **[docs/intro.ipynb notebook](docs/intro.ipynb)** illustrates using the key features of ColBERT with the new Python API.\n\nIt includes how to download the ColBERTv2 model checkpoint trained on MS MARCO Passage Ranking and how to download our new LoTTE benchmark.",
      "contentUrl": "https://huggingface.co/colbert-ir/colbertv2.0/resolve/main/model.safetensors",
      "url": "https://huggingface.co/colbert-ir/colbertv2.0",
      "license": "mit",
      "isPartOf": [],
      "README": "<p align=\"center\">\n  <img align=\"center\" src=\"https://github.com/stanford-futuredata/ColBERT/blob/main/docs/images/colbertofficial.png?raw=true\" width=\"430px\" />\n</p>\n<p align=\"left\">\n\n# ColBERT (v2)\n\n### ColBERT is a _fast_ and _accurate_ retrieval model, enabling scalable BERT-based search over large text collections in tens of milliseconds.\n\n[<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/github/stanford-futuredata/ColBERT/blob/main/docs/intro2new.ipynb)\n\n\n<p align=\"center\">\n  <img align=\"center\" src=\"https://github.com/stanford-futuredata/ColBERT/blob/main/docs/images/ColBERT-Framework-MaxSim-W370px.png?raw=true\" />\n</p>\n<p align=\"center\">\n  <b>Figure 1:</b> ColBERT's late interaction, efficiently scoring the fine-grained similarity between a queries and a passage.\n</p>\n\nAs Figure 1 illustrates, ColBERT relies on fine-grained **contextual late interaction**: it encodes each passage into a **matrix** of token-level embeddings (shown above in blue). Then at search time, it embeds every query into another matrix (shown in green) and efficiently finds passages that contextually match the query using scalable vector-similarity (`MaxSim`) operators.\n\nThese rich interactions allow ColBERT to surpass the quality of _single-vector_ representation models, while scaling efficiently to large corpora. You can read more in our papers:\n\n* [**ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT**](https://arxiv.org/abs/2004.12832) (SIGIR'20).\n* [**Relevance-guided Supervision for OpenQA with ColBERT**](https://arxiv.org/abs/2007.00814) (TACL'21).\n* [**Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval**](https://arxiv.org/abs/2101.00436) (NeurIPS'21).\n* [**ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction**](https://arxiv.org/abs/2112.01488) (NAACL'22).\n* [**PLAID: An Efficient Engine for Late Interaction Retrieval**](https://arxiv.org/abs/2205.09707) (CIKM'22).\n\n----\n\n## \ud83d\udea8 **Announcements** \n\n* (1/29/23) We have merged a new index updater feature and support for additional Hugging Face models! These are in beta so please give us feedback as you try them out.\n* (1/24/23) If you're looking for the **DSP** framework for composing ColBERTv2 and LLMs, it's at: https://github.com/stanfordnlp/dsp\n\n----\n\n## ColBERTv1\n\nThe ColBERTv1 code from the SIGIR'20 paper is in the [`colbertv1` branch](https://github.com/stanford-futuredata/ColBERT/tree/colbertv1). See [here](#branches) for more information on other branches.\n\n\n## Installation\n\nColBERT requires Python 3.7+ and Pytorch 1.9+ and uses the [Hugging Face Transformers](https://github.com/huggingface/transformers) library.\n\nWe strongly recommend creating a conda environment using the commands below. (If you don't have conda, follow the official [conda installation guide](https://docs.anaconda.com/anaconda/install/linux/#installation).)\n\nWe have also included a new environment file specifically for CPU-only environments (`conda_env_cpu.yml`), but note that if you are testing CPU execution on a machine that includes GPUs you might need to specify `CUDA_VISIBLE_DEVICES=\"\"` as part of your command. Note that a GPU is required for training and indexing.\n\n```\nconda env create -f conda_env[_cpu].yml\nconda activate colbert\n```\n\nIf you face any problems, please [open a new issue](https://github.com/stanford-futuredata/ColBERT/issues) and we'll help you promptly!\n\n\n\n## Overview\n\nUsing ColBERT on a dataset typically involves the following steps.\n\n**Step 0: Preprocess your collection.** At its simplest, ColBERT works with tab-separated (TSV) files: a file (e.g., `collection.tsv`) will contain all passages and another (e.g., `queries.tsv`) will contain a set of queries for searching the collection.\n\n**Step 1: Download the [pre-trained ColBERTv2 checkpoint](https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz).** This checkpoint has been trained on the MS MARCO Passage Ranking task. You can also _optionally_ [train your own ColBERT model](#training).\n\n**Step 2: Index your collection.** Once you have a trained ColBERT model, you need to [index your collection](#indexing) to permit fast retrieval. This step encodes all passages into matrices, stores them on disk, and builds data structures for efficient search.\n\n**Step 3: Search the collection with your queries.** Given the model and index, you can [issue queries over the collection](#retrieval) to retrieve the top-k passages for each query.\n\nBelow, we illustrate these steps via an example run on the MS MARCO Passage Ranking task.\n\n\n## API Usage Notebook\n\n**NEW**: We have an experimental notebook on [Google Colab](https://colab.research.google.com/github/stanford-futuredata/ColBERT/blob/main/docs/intro2new.ipynb) that you can use with free GPUs. Indexing 10,000 on the free Colab T4 GPU takes six minutes.\n\nThis Jupyter notebook **[docs/intro.ipynb notebook](docs/intro.ipynb)** illustrates using the key features of ColBERT with the new Python API.\n\nIt includes how to download the ColBERTv2 model checkpoint trained on MS MARCO Passage Ranking and how to download our new LoTTE benchmark.\n\n\n## Data\n\nThis repository works directly with a simple **tab-separated file** format to store queries, passages, and top-k ranked lists.\n\n\n* Queries: each line is `qid \\t query text`.\n* Collection: each line is `pid \\t passage text`.\n* Top-k Ranking: each line is `qid \\t pid \\t rank`.\n\nThis works directly with the data format of the [MS MARCO Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) dataset. You will need the training triples (`triples.train.small.tar.gz`), the official top-1000 ranked lists for the dev set queries (`top1000.dev`), and the dev set relevant passages (`qrels.dev.small.tsv`). For indexing the full collection, you will also need the list of passages (`collection.tar.gz`).\n\n\n## Indexing\n\nFor fast retrieval, indexing precomputes the ColBERT representations of passages.\n\nExample usage:\n\n```python\nfrom colbert.infra import Run, RunConfig, ColBERTConfig\nfrom colbert import Indexer\n\nif __name__=='__main__':\n    with Run().context(RunConfig(nranks=1, experiment=\"msmarco\")):\n\n        config = ColBERTConfig(\n            nbits=2,\n            root=\"/path/to/experiments\",\n        )\n        indexer = Indexer(checkpoint=\"/path/to/checkpoint\", config=config)\n        indexer.index(name=\"msmarco.nbits=2\", collection=\"/path/to/MSMARCO/collection.tsv\")\n```\n\n\n## Retrieval\n\nWe typically recommend that you use ColBERT for **end-to-end** retrieval, where it directly finds its top-k passages from the full collection:\n\n```python\nfrom colbert.data import Queries\nfrom colbert.infra import Run, RunConfig, ColBERTConfig\nfrom colbert import Searcher\n\nif __name__=='__main__':\n    with Run().context(RunConfig(nranks=1, experiment=\"msmarco\")):\n\n        config = ColBERTConfig(\n            root=\"/path/to/experiments\",\n        )\n        searcher = Searcher(index=\"msmarco.nbits=2\", config=config)\n        queries = Queries(\"/path/to/MSMARCO/queries.dev.small.tsv\")\n        ranking = searcher.search_all(queries, k=100)\n        ranking.save(\"msmarco.nbits=2.ranking.tsv\")\n```\n\nYou can optionally specify the `ncells`, `centroid_score_threshold`, and `ndocs` search hyperparameters to trade off between speed and result quality. Defaults for different values of `k` are listed in colbert/searcher.py.\n\nWe can evaluate the MSMARCO rankings using the following command:\n\n```\npython -m utility.evaluate.msmarco_passages --ranking \"/path/to/msmarco.nbits=2.ranking.tsv\" --qrels \"/path/to/MSMARCO/qrels.dev.small.tsv\"\n```\n\n## Training\n\nWe provide a [pre-trained model checkpoint](https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz), but we also detail how to train from scratch here.\nNote that this example demonstrates the ColBERTv1 style of training, but the provided checkpoint was trained with ColBERTv2.\n\nTraining requires a JSONL triples file with a `[qid, pid+, pid-]` list per line. The query IDs and passage IDs correspond to the specified `queries.tsv` and `collection.tsv` files respectively.\n\nExample usage (training on 4 GPUs):\n\n```python\nfrom colbert.infra import Run, RunConfig, ColBERTConfig\nfrom colbert import Trainer\n\nif __name__=='__main__':\n    with Run().context(RunConfig(nranks=4, experiment=\"msmarco\")):\n\n        config = ColBERTConfig(\n            bsize=32,\n            root=\"/path/to/experiments\",\n        )\n        trainer = Trainer(\n            triples=\"/path/to/MSMARCO/triples.train.small.tsv\",\n            queries=\"/path/to/MSMARCO/queries.train.small.tsv\",\n            collection=\"/path/to/MSMARCO/collection.tsv\",\n            config=config,\n        )\n\n        checkpoint_path = trainer.train()\n\n        print(f\"Saved checkpoint to {checkpoint_path}...\")\n```\n\n## Running a lightweight ColBERTv2 server\nWe provide a script to run a lightweight server which serves k (upto 100) results in ranked order for a given search query, in JSON format. This script can be used to power DSP programs. \n\nTo run the server, update the environment variables `INDEX_ROOT` and `INDEX_NAME` in the `.env` file to point to the appropriate ColBERT index. The run the following command:\n```\npython server.py\n```\n\nA sample query:\n```\nhttp://localhost:8893/api/search?query=Who won the 2022 FIFA world cup&k=25\n```\n\n## Branches\n\n### Supported branches\n\n* [`main`](https://github.com/stanford-futuredata/ColBERT/tree/main): Stable branch with ColBERTv2 + PLAID.\n* [`colbertv1`](https://github.com/stanford-futuredata/ColBERT/tree/colbertv1): Legacy branch for ColBERTv1.\n\n### Deprecated branches\n* [`new_api`](https://github.com/stanford-futuredata/ColBERT/tree/new_api): Base ColBERTv2 implementation.\n* [`cpu_inference`](https://github.com/stanford-futuredata/ColBERT/tree/cpu_inference): ColBERTv2 implementation with CPU search support.\n* [`fast_search`](https://github.com/stanford-futuredata/ColBERT/tree/fast_search): ColBERTv2 implementation with PLAID.\n* [`binarization`](https://github.com/stanford-futuredata/ColBERT/tree/binarization): ColBERT with a baseline binarization-based compression strategy (as opposed to ColBERTv2's residual compression, which we found to be more robust).\n\n## Acknowledgments\n\nColBERT logo designed by Chuyi Zhang."
    },
    {
      "@id": "ark:59852/model-facebookai-roberta-base-1otynrz79gx",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "FacebookAI/roberta-base",
      "description": "Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in",
      "author": "FacebookAI",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "rust",
        "safetensors",
        "roberta",
        "fill-mask",
        "exbert",
        "en",
        "dataset:bookcorpus",
        "dataset:wikipedia",
        "arxiv:1907.11692",
        "arxiv:1806.02847",
        "license:mit",
        "autotrain_compatible",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/bookcorpus"
        },
        {
          "@id": "https://huggingface.co/datasets/wikipedia"
        }
      ],
      "hasBias": "The training data used for this model contains a lot of unfiltered content from the internet, which is far from\nneutral. Therefore, the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-base')\n>>> unmasker(\"The man worked as a <mask>.\")\n\n[{'sequence': '<s>The man worked as a mechanic.</s>',\n  'score': 0.08702439814805984,\n  'token': 25682,\n  'token_str': '\u0120mechanic'},\n {'sequence': '<s>The man worked as a waiter.</s>',\n  'score': 0.0819653645157814,\n  'token': 38233,\n  'token_str': '\u0120waiter'},\n {'sequence': '<s>The man worked as a butcher.</s>',\n  'score': 0.073323555290699,\n  'token': 32364,\n  'token_str': '\u0120butcher'},\n {'sequence': '<s>The man worked as a miner.</s>',\n  'score': 0.046322137117385864,\n  'token': 18678,\n  'token_str': '\u0120miner'},\n {'sequence': '<s>The man worked as a guard.</s>',\n  'score': 0.040150221437215805,\n  'token': 2510,\n  'token_str': '\u0120guard'}]\n\n>>> unmasker(\"The Black woman worked as a <mask>.\")\n\n[{'sequence': '<s>The Black woman worked as a waitress.</s>',\n  'score': 0.22177888453006744,\n  'token': 35698,\n  'token_str': '\u0120waitress'},\n {'sequence': '<s>The Black woman worked as a prostitute.</s>',\n  'score': 0.19288744032382965,\n  'token': 36289,\n  'token_str': '\u0120prostitute'},\n {'sequence': '<s>The Black woman worked as a maid.</s>',\n  'score': 0.06498628109693527,\n  'token': 29754,\n  'token_str': '\u0120maid'},\n {'sequence': '<s>The Black woman worked as a secretary.</s>',\n  'score': 0.05375480651855469,\n  'token': 2971,\n  'token_str': '\u0120secretary'},\n {'sequence': '<s>The Black woman worked as a nurse.</s>',\n  'score': 0.05245552211999893,\n  'token': 9008,\n  'token_str': '\u0120nurse'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.",
      "intendedUseCase": "You can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task.\nSee the [model hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task that\ninterests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at a model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-base')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n\n[{'sequence': \"<s>Hello I'm a male model.</s>\",\n  'score': 0.3306540250778198,\n  'token': 2943,\n  'token_str': '\u0120male'},\n {'sequence': \"<s>Hello I'm a female model.</s>\",\n  'score': 0.04655390977859497,\n  'token': 2182,\n  'token_str': '\u0120female'},\n {'sequence': \"<s>Hello I'm a professional model.</s>\",\n  'score': 0.04232972860336304,\n  'token': 2038,\n  'token_str': '\u0120professional'},\n {'sequence': \"<s>Hello I'm a fashion model.</s>\",\n  'score': 0.037216778844594955,\n  'token': 2734,\n  'token_str': '\u0120fashion'},\n {'sequence': \"<s>Hello I'm a Russian model.</s>\",\n  'score': 0.03253649175167084,\n  'token': 1083,\n  'token_str': '\u0120Russian'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import RobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = TFRobertaModel.from_pretrained('roberta-base')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from\nneutral. Therefore, the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-base')\n>>> unmasker(\"The man worked as a <mask>.\")\n\n[{'sequence': '<s>The man worked as a mechanic.</s>',\n  'score': 0.08702439814805984,\n  'token': 25682,\n  'token_str': '\u0120mechanic'},\n {'sequence': '<s>The man worked as a waiter.</s>',\n  'score': 0.0819653645157814,\n  'token': 38233,\n  'token_str': '\u0120waiter'},\n {'sequence': '<s>The man worked as a butcher.</s>',\n  'score': 0.073323555290699,\n  'token': 32364,\n  'token_str': '\u0120butcher'},\n {'sequence': '<s>The man worked as a miner.</s>',\n  'score': 0.046322137117385864,\n  'token': 18678,\n  'token_str': '\u0120miner'},\n {'sequence': '<s>The man worked as a guard.</s>',\n  'score': 0.040150221437215805,\n  'token': 2510,\n  'token_str': '\u0120guard'}]\n\n>>> unmasker(\"The Black woman worked as a <mask>.\")\n\n[{'sequence': '<s>The Black woman worked as a waitress.</s>',\n  'score': 0.22177888453006744,\n  'token': 35698,\n  'token_str': '\u0120waitress'},\n {'sequence': '<s>The Black woman worked as a prostitute.</s>',\n  'score': 0.19288744032382965,\n  'token': 36289,\n  'token_str': '\u0120prostitute'},\n {'sequence': '<s>The Black woman worked as a maid.</s>',\n  'score': 0.06498628109693527,\n  'token': 29754,\n  'token_str': '\u0120maid'},\n {'sequence': '<s>The Black woman worked as a secretary.</s>',\n  'score': 0.05375480651855469,\n  'token': 2971,\n  'token_str': '\u0120secretary'},\n {'sequence': '<s>The Black woman worked as a nurse.</s>',\n  'score': 0.05245552211999893,\n  'token': 9008,\n  'token_str': '\u0120nurse'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.",
      "usageInformation": "You can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-base')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n\n[{'sequence': \"<s>Hello I'm a male model.</s>\",\n  'score': 0.3306540250778198,\n  'token': 2943,\n  'token_str': '\u0120male'},\n {'sequence': \"<s>Hello I'm a female model.</s>\",\n  'score': 0.04655390977859497,\n  'token': 2182,\n  'token_str': '\u0120female'},\n {'sequence': \"<s>Hello I'm a professional model.</s>\",\n  'score': 0.04232972860336304,\n  'token': 2038,\n  'token_str': '\u0120professional'},\n {'sequence': \"<s>Hello I'm a fashion model.</s>\",\n  'score': 0.037216778844594955,\n  'token': 2734,\n  'token_str': '\u0120fashion'},\n {'sequence': \"<s>Hello I'm a Russian model.</s>\",\n  'score': 0.03253649175167084,\n  'token': 1083,\n  'token_str': '\u0120Russian'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import RobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = TFRobertaModel.from_pretrained('roberta-base')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```",
      "contentUrl": "https://huggingface.co/FacebookAI/roberta-base/resolve/main/model.safetensors",
      "url": "https://huggingface.co/FacebookAI/roberta-base",
      "license": "mit",
      "isPartOf": [],
      "README": "\n# RoBERTa base model\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1907.11692) and first released in\n[this repository](https://github.com/pytorch/fairseq/tree/master/examples/roberta). This model is case-sensitive: it\nmakes a difference between english and English.\n\nDisclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nRoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. \n\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model\nrandomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict\nthe masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one\nafter the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to\nlearn a bidirectional representation of the sentence.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task.\nSee the [model hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task that\ninterests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at a model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-base')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n\n[{'sequence': \"<s>Hello I'm a male model.</s>\",\n  'score': 0.3306540250778198,\n  'token': 2943,\n  'token_str': '\u0120male'},\n {'sequence': \"<s>Hello I'm a female model.</s>\",\n  'score': 0.04655390977859497,\n  'token': 2182,\n  'token_str': '\u0120female'},\n {'sequence': \"<s>Hello I'm a professional model.</s>\",\n  'score': 0.04232972860336304,\n  'token': 2038,\n  'token_str': '\u0120professional'},\n {'sequence': \"<s>Hello I'm a fashion model.</s>\",\n  'score': 0.037216778844594955,\n  'token': 2734,\n  'token_str': '\u0120fashion'},\n {'sequence': \"<s>Hello I'm a Russian model.</s>\",\n  'score': 0.03253649175167084,\n  'token': 1083,\n  'token_str': '\u0120Russian'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import RobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = TFRobertaModel.from_pretrained('roberta-base')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from\nneutral. Therefore, the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='roberta-base')\n>>> unmasker(\"The man worked as a <mask>.\")\n\n[{'sequence': '<s>The man worked as a mechanic.</s>',\n  'score': 0.08702439814805984,\n  'token': 25682,\n  'token_str': '\u0120mechanic'},\n {'sequence': '<s>The man worked as a waiter.</s>',\n  'score': 0.0819653645157814,\n  'token': 38233,\n  'token_str': '\u0120waiter'},\n {'sequence': '<s>The man worked as a butcher.</s>',\n  'score': 0.073323555290699,\n  'token': 32364,\n  'token_str': '\u0120butcher'},\n {'sequence': '<s>The man worked as a miner.</s>',\n  'score': 0.046322137117385864,\n  'token': 18678,\n  'token_str': '\u0120miner'},\n {'sequence': '<s>The man worked as a guard.</s>',\n  'score': 0.040150221437215805,\n  'token': 2510,\n  'token_str': '\u0120guard'}]\n\n>>> unmasker(\"The Black woman worked as a <mask>.\")\n\n[{'sequence': '<s>The Black woman worked as a waitress.</s>',\n  'score': 0.22177888453006744,\n  'token': 35698,\n  'token_str': '\u0120waitress'},\n {'sequence': '<s>The Black woman worked as a prostitute.</s>',\n  'score': 0.19288744032382965,\n  'token': 36289,\n  'token_str': '\u0120prostitute'},\n {'sequence': '<s>The Black woman worked as a maid.</s>',\n  'score': 0.06498628109693527,\n  'token': 29754,\n  'token_str': '\u0120maid'},\n {'sequence': '<s>The Black woman worked as a secretary.</s>',\n  'score': 0.05375480651855469,\n  'token': 2971,\n  'token_str': '\u0120secretary'},\n {'sequence': '<s>The Black woman worked as a nurse.</s>',\n  'score': 0.05245552211999893,\n  'token': 9008,\n  'token_str': '\u0120nurse'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe RoBERTa model was pretrained on the reunion of five datasets:\n- [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books;\n- [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers) ;\n- [CC-News](https://commoncrawl.org/2016/10/news-dataset-available/), a dataset containing 63 millions English news\n  articles crawled between September 2016 and February 2019.\n- [OpenWebText](https://github.com/jcpeterson/openwebtext), an opensource recreation of the WebText dataset used to\n  train GPT-2,\n- [Stories](https://arxiv.org/abs/1806.02847) a dataset containing a subset of CommonCrawl data filtered to match the\n  story-like style of Winograd schemas.\n\nTogether these datasets weigh 160GB of text.\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of\nthe model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked\nwith `<s>` and the end of one by `</s>`\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `<mask>`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\nContrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).\n\n### Pretraining\n\nThe model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512. The\noptimizer used is Adam with a learning rate of 6e-4, \\\\(\\beta_{1} = 0.9\\\\), \\\\(\\beta_{2} = 0.98\\\\) and\n\\\\(\\epsilon = 1e-6\\\\), a weight decay of 0.01, learning rate warmup for 24,000 steps and linear decay of the learning\nrate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\n|      | 87.6 | 91.9 | 92.8 | 94.8  | 63.6 | 91.2  | 90.2 | 78.7 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs/1907.11692},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1907.11692},\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=roberta-base\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n"
    },
    {
      "@id": "ark:59852/model-jonatasgrosman-wav2vec2-large-xlsr-53-russian-s6vypbl8gby",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "jonatasgrosman/wav2vec2-large-xlsr-53-russian",
      "description": "Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Russian using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice) and [CSS10](https://github.com/Kyubyong/css10).",
      "author": "jonatasgrosman",
      "keywords": [
        "transformers",
        "pytorch",
        "jax",
        "wav2vec2",
        "automatic-speech-recognition",
        "audio",
        "hf-asr-leaderboard",
        "mozilla-foundation/common_voice_6_0",
        "robust-speech-event",
        "ru",
        "speech",
        "xlsr-fine-tuning-week",
        "dataset:common_voice",
        "dataset:mozilla-foundation/common_voice_6_0",
        "doi:10.57967/hf/3571",
        "license:apache-2.0",
        "model-index",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/common_voice"
        },
        {
          "@id": "https://huggingface.co/datasets/mozilla-foundation/common_voice_6_0"
        }
      ],
      "usageInformation": "The model can be used directly (without a language model) as follows...\n\nUsing the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-russian\")\naudio_paths = [\"/path/to/file.mp3\", \"/path/to/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"ru\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-russian\"\nSAMPLES = 5\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)",
      "contentUrl": "https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-russian/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-russian",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Fine-tuned XLSR-53 large model for speech recognition in Russian\n\nFine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Russian using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice) and [CSS10](https://github.com/Kyubyong/css10).\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\nThis model has been fine-tuned thanks to the GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/) :)\n\nThe script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint\n\n## Usage\n\nThe model can be used directly (without a language model) as follows...\n\nUsing the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-russian\")\naudio_paths = [\"/path/to/file.mp3\", \"/path/to/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"ru\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-russian\"\nSAMPLES = 5\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n    batch[\"speech\"] = speech_array\n    batch[\"sentence\"] = batch[\"sentence\"].upper()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\npredicted_sentences = processor.batch_decode(predicted_ids)\n\nfor i, predicted_sentence in enumerate(predicted_sentences):\n    print(\"-\" * 100)\n    print(\"Reference:\", test_dataset[i][\"sentence\"])\n    print(\"Prediction:\", predicted_sentence)\n```\n\n| Reference  | Prediction |\n| ------------- | ------------- |\n| \u041e\u041d \u0420\u0410\u0411\u041e\u0422\u0410\u0422\u042c, \u0410 \u0415\u0415 \u041d\u0415 \u0423\u0414\u0415\u0420\u0416\u0410\u0422\u042c \u041d\u0418\u041a\u0410\u041a \u2014 \u0411\u0415\u0413\u0410\u0415\u0422 \u0417\u0410 \u041a\u041b\u0401\u0428\u0415\u041c \u041a\u0410\u0416\u0414\u041e\u0413\u041e \u0411\u0423\u041b\u042c\u0412\u0410\u0420\u041d\u0418\u041a\u0410. | \u041e\u041d \u0420\u0410\u0411\u041e\u0422\u0410\u0422\u042c \u0410 \u0415\u0415 \u041d\u0415 \u0423\u0414\u0415\u0420\u0416\u0410\u0422 \u041d\u0418\u041a\u0410\u041a  \u0411\u0415\u0413\u0410\u0415\u0422 \u0417\u0410 \u041a\u041b\u0415\u0428\u041e\u041c \u041a\u0410\u0416\u0414\u041e\u0413\u041e \u0411\u0423\u041b\u042c\u0411\u0410\u0420\u041d\u0418\u041a\u0410 |\n| \u0415\u0421\u041b\u0418 \u041d\u0415 \u0411\u0423\u0414\u0415\u0422 \u0412\u041e\u0417\u0420\u0410\u0416\u0415\u041d\u0418\u0419, \u042f \u0411\u0423\u0414\u0423 \u0421\u0427\u0418\u0422\u0410\u0422\u042c, \u0427\u0422\u041e \u0410\u0421\u0421\u0410\u041c\u0411\u041b\u0415\u042f \u0421\u041e\u0413\u041b\u0410\u0421\u041d\u0410 \u0421 \u042d\u0422\u0418\u041c \u041f\u0420\u0415\u0414\u041b\u041e\u0416\u0415\u041d\u0418\u0415\u041c. | \u0415\u0421\u041b\u0418 \u041d\u0415 \u0411\u0423\u0414\u0415\u0422 \u0412\u041e\u0417\u0420\u0410\u0416\u0415\u041d\u0418\u0419 \u042f \u0411\u0423\u0414\u0423 \u0421\u0427\u0418\u0422\u0410\u0422\u042c \u0427\u0422\u041e \u0410\u0421\u0421\u0410\u041c\u0411\u041b\u0415\u042f \u0421\u041e\u0413\u041b\u0410\u0421\u041d\u0410 \u0421 \u042d\u0422\u0418\u041c \u041f\u0420\u0415\u0414\u041b\u041e\u0416\u0415\u041d\u0418\u0415\u041c |\n| \u041f\u0410\u041b\u0415\u0421\u0422\u0418\u041d\u0426\u0410\u041c \u041d\u0415\u041e\u0411\u0425\u041e\u0414\u0418\u041c\u041e \u0421\u041d\u0410\u0427\u0410\u041b\u0410 \u0423\u0421\u0422\u0410\u041d\u041e\u0412\u0418\u0422\u042c \u041c\u0418\u0420 \u0421 \u0418\u0417\u0420\u0410\u0418\u041b\u0415\u041c, \u0410 \u0417\u0410\u0422\u0415\u041c \u0414\u041e\u0411\u0418\u0412\u0410\u0422\u042c\u0421\u042f \u041f\u0420\u0418\u0417\u041d\u0410\u041d\u0418\u042f \u0413\u041e\u0421\u0423\u0414\u0410\u0420\u0421\u0422\u0412\u0415\u041d\u041d\u041e\u0421\u0422\u0418. | \u041f\u0410\u041b\u0415\u0421\u0422\u0418\u041d\u0426\u0410\u041c \u041d\u0415\u041e\u0411\u0425\u041e\u0414\u0418\u041c\u041e \u0421\u041d\u0410\u0427\u0410\u041b\u0410 \u0423\u0421\u0422\u0410\u041d\u041e\u0412\u0418\u0422\u042c \u0421 \u041d\u0418 \u041c\u0418\u0420 \u0424\u0415\u0417\u0420\u0415\u041b\u0415\u041c \u0410 \u0417\u0410\u0422\u0415\u041c \u0414\u041e\u0411\u0418\u0412\u0410\u0422\u042c\u0421\u042f \u041f\u0420\u0418\u0417\u041d\u0410\u041d\u0418\u042f \u0413\u041e\u0421\u0423\u0414\u0410\u0420\u0421\u0422\u0412\u0415\u041d\u0421\u041a\u0418 |\n| \u0423 \u041c\u0415\u041d\u042f \u0411\u042b\u041b\u041e \u0422\u0410\u041a\u041e\u0415 \u0427\u0423\u0412\u0421\u0422\u0412\u041e, \u0427\u0422\u041e \u0427\u0422\u041e-\u0422\u041e \u0422\u0410\u041a\u041e\u0415 \u041e\u0427\u0415\u041d\u042c \u0412\u0410\u0416\u041d\u041e\u0415 \u042f \u041f\u0420\u0418\u0411\u0410\u0412\u041b\u042f\u042e. | \u0423 \u041c\u0415\u041d\u042f \u0411\u042b\u041b\u041e \u0422\u0410\u041a\u041e\u0415 \u0427\u0423\u0412\u0421\u0422\u0412\u041e \u0427\u0422\u041e \u0427\u0422\u041e-\u0422\u041e \u0422\u0410\u041a\u041e\u0415 \u041e\u0427\u0415\u041d\u042c \u0412\u0410\u0416\u041d\u041e\u0415 \u042f \u041f\u0420\u0415\u0414\u0411\u0410\u0412\u041b\u042f\u0415\u0422 |\n| \u0422\u041e\u041b\u042c\u041a\u041e \u0412\u0420\u042f\u0414 \u041b\u0418 \u041f\u041e\u0419\u041c\u0415\u0422. | \u0422\u041e\u041b\u042c\u041a\u041e \u0412\u0420\u042f\u0414 \u041b\u0418 \u041f\u041e\u0419\u041c\u0415\u0422 |\n| \u0412\u0420\u041e\u041d\u0421\u041a\u0418\u0419, \u0421\u041b\u0423\u0428\u0410\u042f \u041e\u0414\u041d\u0418\u041c \u0423\u0425\u041e\u041c, \u041f\u0415\u0420\u0415\u0412\u041e\u0414\u0418\u041b \u0411\u0418\u041d\u041e\u041a\u041b\u042c \u0421 \u0411\u0415\u041d\u0423\u0410\u0420\u0410 \u041d\u0410 \u0411\u0415\u041b\u042c-\u042d\u0422\u0410\u0416 \u0418 \u041e\u0413\u041b\u042f\u0414\u042b\u0412\u0410\u041b \u041b\u041e\u0416\u0418. | \u0417\u041b\u0410\u0417\u041a\u0418 \u0421\u041b\u0423\u0428\u0410\u042e \u041e\u0422 \u041e\u0414\u041d\u0418\u041c \u0423\u0425\u0410\u041c \u0422\u042b \u0412\u041e\u0422\u0418 \u0412 \u0412\u0418\u041d\u041e\u041a\u041e\u0422 \u0421\u041f\u0418\u041b\u0410 \u041d\u0410 \u041f\u0415\u0420\u0415\u0422\u0410\u0427 \u0418 \u041e\u041a\u041b\u042f\u0414\u042b\u0412\u0410\u041b \u0411\u041e\u0421\u0423 |\n| \u041a \u0421\u041e\u0416\u0410\u041b\u0415\u041d\u0418\u042e, \u0421\u0418\u0422\u0423\u0410\u0426\u0418\u042f \u041f\u0420\u041e\u0414\u041e\u041b\u0416\u0410\u0415\u0422 \u0423\u0425\u0423\u0414\u0428\u0410\u0422\u042c\u0421\u042f. | \u041a \u0421\u041e\u0416\u0410\u041b\u0415\u041d\u0418\u042e \u0421\u0418\u0422\u0423\u0410\u0426\u0418\u0418 \u041f\u0420\u041e\u0414\u041e\u041b\u0416\u0410\u0415\u0422 \u0423\u0425\u0423\u0416\u0410\u0422\u042c\u0421\u042f |\n| \u0412\u0421\u0401 \u0416\u0410\u041b\u041e\u0412\u0410\u041d\u0418\u0415 \u0423\u0425\u041e\u0414\u0418\u041b\u041e \u041d\u0410 \u0414\u041e\u041c\u0410\u0428\u041d\u0418\u0415 \u0420\u0410\u0421\u0425\u041e\u0414\u042b \u0418 \u041d\u0410 \u0423\u041f\u041b\u0410\u0422\u0423 \u041c\u0415\u041b\u041a\u0418\u0425 \u041d\u0415\u041f\u0415\u0420\u0415\u0412\u041e\u0414\u0418\u0412\u0428\u0418\u0425\u0421\u042f \u0414\u041e\u041b\u0413\u041e\u0412. | \u0412\u0421\u0415 \u0416\u0410\u041b\u041e\u0412\u0410\u041d\u0418\u0415 \u0423\u0425\u041e\u0414\u0418\u041b\u041e \u041d\u0410 \u0414\u041e\u041c\u0410\u0428\u041d\u0418\u0415 \u0420\u0410\u0421\u0425\u041e\u0414\u042b \u0418 \u041d\u0410 \u0423\u041f\u041b\u0410\u0422\u0423 \u041c\u0415\u041b\u041a\u0418\u0425 \u041d\u0415 \u041f\u0415\u0420\u0415\u0412\u041e\u0414\u0418\u0412\u0428\u0418\u0425\u0421\u042f \u0414\u041e\u041b\u0413\u041e\u0412 |\n| \u0422\u0415\u041f\u0415\u0420\u042c \u0414\u0415\u041b\u041e, \u041a\u041e\u041d\u0415\u0427\u041d\u041e, \u0417\u0410 \u0422\u0415\u041c, \u0427\u0422\u041e\u0411\u042b \u041f\u0420\u0415\u0412\u0420\u0410\u0422\u0418\u0422\u042c \u0421\u041b\u041e\u0412\u0410 \u0412 \u0414\u0415\u041b\u0410. | \u0422\u0415\u041f\u0415\u0420\u042c \u0414\u0415\u041b\u0410\u042e \u041a\u041e\u041d\u0415\u0427\u041d\u041e \u0417\u0410\u0422\u0415\u041c \u0427\u0422\u041e\u0411\u042b \u041f\u0420\u0415\u0412\u0420\u0410\u0422\u0418\u0422\u042c \u0421\u041b\u041e\u0412\u0410 \u0412 \u0414\u0415\u041b\u0410 |\n| \u0414\u0415\u0412\u042f\u0422\u042c | \u041b\u0415\u0412\u0415\u0422\u042c |\n\n## Evaluation\n\n1. To evaluate on `mozilla-foundation/common_voice_6_0` with split `test`\n\n```bash\npython eval.py --model_id jonatasgrosman/wav2vec2-large-xlsr-53-russian --dataset mozilla-foundation/common_voice_6_0 --config ru --split test\n```\n\n2. To evaluate on `speech-recognition-community-v2/dev_data`\n\n```bash\npython eval.py --model_id jonatasgrosman/wav2vec2-large-xlsr-53-russian --dataset speech-recognition-community-v2/dev_data --config ru --split validation --chunk_length_s 5.0 --stride_length_s 1.0\n```\n\n## Citation\nIf you want to cite this model you can use this:\n\n```bibtex\n@misc{grosman2021xlsr53-large-russian,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {R}ussian},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-russian}},\n  year={2021}\n}\n```"
    },
    {
      "@id": "ark:59852/model-qwen-qwen2-5-vl-3b-instruct-rhbckmkeqp",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "Qwen/Qwen2.5-VL-3B-Instruct",
      "description": "<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">",
      "author": "Qwen",
      "keywords": [
        "transformers",
        "safetensors",
        "qwen2_5_vl",
        "image-to-text",
        "multimodal",
        "image-text-to-text",
        "conversational",
        "en",
        "arxiv:2309.00071",
        "arxiv:2409.12191",
        "arxiv:2308.12966",
        "text-generation-inference",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "usageInformation": "text = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n<details>\n<summary>Multi image inference</summary>\n\n```python",
      "contentUrl": "https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/model-00001-of-00002.safetensors",
      "url": "https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct",
      "isPartOf": [],
      "README": "\n# Qwen2.5-VL-3B-Instruct\n<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Introduction\n\nIn the past five months since Qwen2-VL\u2019s release, numerous developers have built new models on the Qwen2-VL vision-language models, providing us with valuable feedback. During this period, we focused on building more useful vision-language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5-VL.\n\n#### Key Enhancements:\n* **Understand things visually**: Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images.\n\n* **Being agentic**: Qwen2.5-VL directly plays as a visual agent that can reason and dynamically direct tools, which is capable of computer use and phone use.\n\n* **Understanding long videos and capturing events**: Qwen2.5-VL can comprehend videos of over 1 hour, and this time it has a new ability of cpaturing event by pinpointing the relevant video segments.\n\n* **Capable of visual localization in different formats**: Qwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes.\n\n* **Generating structured outputs**: for data like scans of invoices, forms, tables, etc. Qwen2.5-VL supports structured outputs of their contents, benefiting usages in finance, commerce, etc.\n\n\n#### Model Architecture Updates:\n\n* **Dynamic Resolution and Frame Rate Training for Video Understanding**:\n\nWe extend dynamic resolution to the temporal dimension by adopting dynamic FPS sampling, enabling the model to comprehend videos at various sampling rates. Accordingly, we update mRoPE in the time dimension with IDs and absolute time alignment, enabling the model to learn temporal sequence and speed, and ultimately acquire the ability to pinpoint specific moments.\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL/qwen2.5vl_arc.jpeg\" width=\"80%\"/>\n<p>\n\n\n* **Streamlined and Efficient Vision Encoder**\n\nWe enhance both training and inference speeds by strategically implementing window attention into the ViT. The ViT architecture is further optimized with SwiGLU and RMSNorm, aligning it with the structure of the Qwen2.5 LLM.\n\n\nWe have three models with 3, 7 and 72 billion parameters. This repo contains the instruction-tuned 3B Qwen2.5-VL model. For more information, visit our [Blog](https://qwenlm.github.io/blog/qwen2.5-vl/) and [GitHub](https://github.com/QwenLM/Qwen2.5-VL).\n\n\n\n## Evaluation\n\n### Image benchmark\n\n| Benchmark | InternVL2.5-4B |Qwen2-VL-7B |Qwen2.5-VL-3B |\n| :--- | :---:  | :---: | :---: |\n| MMMU<sub>val</sub>  | 52.3 | 54.1 | 53.1| \n| MMMU-Pro<sub>val</sub>  | **32.7** | 30.5 | 31.6|\n| AI2D<sub>test</sub> | 81.4 | **83.0** | 81.5 |\n| DocVQA<sub>test</sub>  | 91.6 | 94.5 | **93.9** | \n| InfoVQA<sub>test</sub>  | 72.1 | 76.5 | **77.1** |\n| TextVQA<sub>val</sub>  | 76.8 | **84.3** | 79.3|\n| MMBench-V1.1<sub>test</sub>  | 79.3 | **80.7** | 77.6 | \n| MMStar | 58.3 | **60.7** | 55.9 | \n| MathVista<sub>testmini</sub>  | 60.5 | 58.2 | **62.3** |\n| MathVision<sub>full</sub>  | 20.9 | 16.3  | **21.2** |\n\n\n### Video benchmark\n| Benchmark | InternVL2.5-4B | Qwen2-VL-7B | Qwen2.5-VL-3B |\n| :--- | :---:  | :---: | :---: |\n| MVBench | 71.6 | 67.0 | 67.0 |\n| VideoMME | 63.6/62.3 | 69.0/63.3 | 67.6/61.5 |\n| MLVU | 48.3 | - | 68.2 |\n| LVBench | - | - | 43.3 |\n| MMBench-Video | 1.73 | 1.44 | 1.63 |\n| EgoSchema | - | - | 64.8 |\n| PerceptionTest | - | - | 66.9 |\n| TempCompass | - | - | 64.4 |\n| LongVideoBench | 55.2 | 55.6 | 54.2 |\n| CharadesSTA/mIoU | - | - | 38.8 |\n\n\n### Agent benchmark\n| Benchmarks              | Qwen2.5-VL-3B |\n|-------------------------|---------------|\n| ScreenSpot              |     55.5    |\n| ScreenSpot Pro          |     23.9    |\n| AITZ_EM                 |  \t76.9    |\n| Android Control High_EM |    \t63.7    |\n| Android Control Low_EM  |  \t22.2    |\n| AndroidWorld_SR         | \t90.8  \t|\n| MobileMiniWob++_SR      | \t67.9    |\n\n## Requirements\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers accelerate\n```\nor you might encounter the following error:\n```\nKeyError: 'qwen2_5_vl'\n```\n\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen2.5-VL with \ud83e\udd16 ModelScope and \ud83e\udd17 Transformers.\n\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers accelerate\n```\nor you might encounter the following error:\n```\nKeyError: 'qwen2_5_vl'\n```\n\n\nWe offer a toolkit to help you handle various types of visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved images and videos. You can install it using the following command:\n\n```bash\n# It's highly recommanded to use `[decord]` feature for faster video loading.\npip install qwen-vl-utils[decord]==0.0.8\n```\n\nIf you are not using Linux, you might not be able to install `decord` from PyPI. In that case, you can use `pip install qwen-vl-utils` which will fall back to using torchvision for video processing. However, you can still [install decord from source](https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source) to get decord used when loading video.\n\n### Using \ud83e\udd17  Transformers to Chat\n\nHere we show a code snippet to show you how to use the chat model with `transformers` and `qwen_vl_utils`:\n\n```python\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-VL-3B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2.5-VL-3B-Instruct\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n\n# default processer\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n\n# The default range for the number of visual tokens per image in the model is 4-16384.\n# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n<details>\n<summary>Multi image inference</summary>\n\n```python\n# Messages containing multiple images and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n            {\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n            {\"type\": \"text\", \"text\": \"Identify the similarities between these images.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n</details>\n\n<details>\n<summary>Video inference</summary>\n\n```python\n# Messages containing a images list as a video and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": [\n                    \"file:///path/to/frame1.jpg\",\n                    \"file:///path/to/frame2.jpg\",\n                    \"file:///path/to/frame3.jpg\",\n                    \"file:///path/to/frame4.jpg\",\n                ],\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n# Messages containing a local video path and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": \"file:///path/to/video1.mp4\",\n                \"max_pixels\": 360 * 420,\n                \"fps\": 1.0,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n# Messages containing a video url and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n#In Qwen 2.5 VL, frame rate information is also input into the model to align with absolute time.\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    fps=fps,\n    padding=True,\n    return_tensors=\"pt\",\n    **video_kwargs,\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\nVideo URL compatibility largely depends on the third-party library version. The details are in the table below. change the backend by `FORCE_QWENVL_VIDEO_READER=torchvision` or `FORCE_QWENVL_VIDEO_READER=decord` if you prefer not to use the default one.\n\n| Backend     | HTTP | HTTPS |\n|-------------|------|-------|\n| torchvision >= 0.19.0 | \u2705  | \u2705   |\n| torchvision < 0.19.0  | \u274c  | \u274c   |\n| decord      | \u2705  | \u274c   |\n</details>\n\n<details>\n<summary>Batch inference</summary>\n\n```python\n# Sample messages for batch inference\nmessages1 = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n            {\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n            {\"type\": \"text\", \"text\": \"What are the common elements in these pictures?\"},\n        ],\n    }\n]\nmessages2 = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n# Combine messages for batch processing\nmessages = [messages1, messages2]\n\n# Preparation for batch inference\ntexts = [\n    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n    for msg in messages\n]\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=texts,\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Batch Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_texts = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_texts)\n```\n</details>\n\n### \ud83e\udd16 ModelScope\nWe strongly advise users especially those in mainland China to use ModelScope. `snapshot_download` can help you solve issues concerning downloading checkpoints.\n\n\n### More Usage Tips\n\nFor input images, we support local files, base64, and URLs. For videos, we currently only support local files.\n\n```python\n# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.\n## Local file path\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n## Image URL\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"http://path/to/your/image.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n## Base64 encoded image\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"data:image;base64,/9j/...\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n```\n#### Image Resolution for performance boost\n\nThe model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs, such as a token count range of 256-1280, to balance speed and memory usage.\n\n```python\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n    \"Qwen/Qwen2.5-VL-3B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels\n)\n```\n\nBesides, We provide two methods for fine-grained control over the image size input to the model:\n\n1. Define min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels.\n   \n2. Specify exact dimensions: Directly set `resized_height` and `resized_width`. These values will be rounded to the nearest multiple of 28.\n\n```python\n# min_pixels and max_pixels\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"file:///path/to/your/image.jpg\",\n                \"resized_height\": 280,\n                \"resized_width\": 420,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n# resized_height and resized_width\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"file:///path/to/your/image.jpg\",\n                \"min_pixels\": 50176,\n                \"max_pixels\": 50176,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n\n```\n{\n\t...,\n    \"type\": \"yarn\",\n    \"mrope_section\": [\n        16,\n        24,\n        24\n    ],\n    \"factor\": 4,\n    \"original_max_position_embeddings\": 32768\n}\n```\n\nHowever, it should be noted that this method has a significant impact on the performance of temporal and spatial localization tasks, and is therefore not recommended for use.\n\nAt the same time, for long video inputs, since MRoPE itself is more economical with ids, the max_position_embeddings can be directly modified to a larger value, such as 64k.\n\n\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5-VL,\n    title = {Qwen2.5-VL},\n    url = {https://qwenlm.github.io/blog/qwen2.5-vl/},\n    author = {Qwen Team},\n    month = {January},\n    year = {2025}\n}\n\n@article{Qwen2VL,\n  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\n  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\n  journal={arXiv preprint arXiv:2409.12191},\n  year={2024}\n}\n\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```\n"
    },
    {
      "@id": "ark:59852/model-jonatasgrosman-wav2vec2-large-xlsr-53-chinese-zh-cn-vlpktzymnj1",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn",
      "description": "Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Chinese using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice), [CSS10](https://github.com/Kyubyong/css10) and [ST-CMDS](http://www.openslr.org/38/).",
      "author": "jonatasgrosman",
      "keywords": [
        "transformers",
        "pytorch",
        "jax",
        "wav2vec2",
        "automatic-speech-recognition",
        "audio",
        "speech",
        "xlsr-fine-tuning-week",
        "zh",
        "dataset:common_voice",
        "doi:10.57967/hf/3570",
        "license:apache-2.0",
        "model-index",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/common_voice"
        }
      ],
      "usageInformation": "The model can be used directly (without a language model) as follows...\n\nUsing the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\")\naudio_paths = [\"/path/to/file.mp3\", \"/path/to/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"zh-CN\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\"\nSAMPLES = 10\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)",
      "contentUrl": "https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Fine-tuned XLSR-53 large model for speech recognition in Chinese\n\nFine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Chinese using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice), [CSS10](https://github.com/Kyubyong/css10) and [ST-CMDS](http://www.openslr.org/38/).\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\nThis model has been fine-tuned thanks to the GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/) :)\n\nThe script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint\n\n## Usage\n\nThe model can be used directly (without a language model) as follows...\n\nUsing the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\")\naudio_paths = [\"/path/to/file.mp3\", \"/path/to/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"zh-CN\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\"\nSAMPLES = 10\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n    batch[\"speech\"] = speech_array\n    batch[\"sentence\"] = batch[\"sentence\"].upper()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\npredicted_sentences = processor.batch_decode(predicted_ids)\n\nfor i, predicted_sentence in enumerate(predicted_sentences):\n    print(\"-\" * 100)\n    print(\"Reference:\", test_dataset[i][\"sentence\"])\n    print(\"Prediction:\", predicted_sentence)\n```\n\n| Reference  | Prediction |\n| ------------- | ------------- |\n| \u5b8b\u671d\u672b\u5e74\u5e74\u95f4\u5b9a\u5c45\u7c89\u5cad\u56f4\u3002 | \u5b8b\u671d\u672b\u5e74\u5e74\u95f4\u5b9a\u5c45\u5206\u5b9a\u4e3a |\n| \u6e10\u6e10\u884c\u52a8\u4e0d\u4fbf | \u5efa\u5883\u884c\u52a8\u4e0d\u7247 |\n| \u4e8c\u5341\u4e00\u5e74\u53bb\u4e16\u3002 | \u4e8c\u5341\u4e00\u5e74\u53bb\u4e16 |\n| \u4ed6\u4eec\u81ea\u79f0\u6070\u54c8\u62c9\u3002 | \u4ed6\u4eec\u81ea\u79f0\u5bb6\u54c8<unk> |\n| \u5c40\u90e8\u5e72\u6da9\u7684\u4f8b\u5b50\u5305\u62ec\u6709\u53e3\u5e72\u3001\u773c\u775b\u5e72\u71e5\u3001\u53ca\u9634\u9053\u5e72\u71e5\u3002 | \u83ca\u7269\u5e72\u5bfa\u7684\u4f8b\u5b50\u5305\u62ec\u6709\u53e3\u809d\u773c\u775b\u5e72\u7167\u4ee5\u53ca\u9634\u5230\u5e72<unk> |\n| \u5609\u9756\u4e09\u5341\u516b\u5e74\uff0c\u767b\u8fdb\u58eb\u7b2c\u4e09\u7532\u7b2c\u4e8c\u540d\u3002 | \u5609\u9756\u4e09\u5341\u516b\u5e74\u767b\u8fdb\u58eb\u7b2c\u4e09\u7532\u7b2c\u4e8c\u540d |\n| \u8fd9\u4e00\u540d\u79f0\u4e00\u76f4\u6cbf\u7528\u81f3\u4eca\u3002 | \u8fd9\u4e00\u540d\u79f0\u4e00\u76f4\u6cbf\u7528\u662f\u5fc3 |\n| \u540c\u65f6\u4e54\u51e1\u5c3c\u8fd8\u5f97\u5230\u5305\u7a0e\u5408\u540c\u548c\u8bb8\u591a\u660e\u77fe\u77ff\u7684\u7ecf\u8425\u6743\u3002 | \u540c\u65f6\u6865\u51e1\u59ae\u8fd8\u5f97\u5230\u5305\u7a0e\u5408\u540c\u548c\u8bb8\u591a\u6c11\u7e41\u77ff\u7684\u7ecf\u8425\u6743 |\n| \u4e3a\u4e86\u60e9\u7f5a\u897f\u624e\u57ce\u548c\u585e\u5c14\u67f1\u7684\u7ed3\u76df\uff0c\u76df\u519b\u5728\u62b5\u8fbe\u540e\u5c06\u5916\u57ce\u70e7\u6bc1\u3002 | \u4e3a\u4e86\u66fe\u7f5a\u897f\u624e\u57ce\u548c\u585e\u5c14\u7d20\u7684\u8282\u76df\u76df\u519b\u5728\u62b5\u8fbe\u540e\u5c06\u5916\u66fe\u70e7\u6bc1 |\n| \u6cb3\u5185\u76db\u4ea7\u9ec4\u8272\u65e0\u9c7c\u9cde\u7684\u9ccd\u5c04\u9c7c\u3002 | \u5408\u7c7b\u751f\u573a\u73af\u8272\u65e0\u9c7c\u6797\u7684\u9a91\u5c04\u9c7c |\n\n## Evaluation\n\nThe model can be evaluated as follows on the Chinese (zh-CN) test data of Common Voice.\n\n```python\nimport torch\nimport re\nimport librosa\nfrom datasets import load_dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"zh-CN\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\"\nDEVICE = \"cuda\"\n\nCHARS_TO_IGNORE = [\",\", \"?\", \"\u00bf\", \".\", \"!\", \"\u00a1\", \";\", \"\uff1b\", \":\", '\"\"', \"%\", '\"', \"\ufffd\", \"\u02bf\", \"\u00b7\", \"\u10fb\", \"~\", \"\u055e\",\n                  \"\u061f\", \"\u060c\", \"\u0964\", \"\u0965\", \"\u00ab\", \"\u00bb\", \"\u201e\", \"\u201c\", \"\u201d\", \"\u300c\", \"\u300d\", \"\u2018\", \"\u2019\", \"\u300a\", \"\u300b\", \"(\", \")\", \"[\", \"]\",\n                  \"{\", \"}\", \"=\", \"`\", \"_\", \"+\", \"<\", \">\", \"\u2026\", \"\u2013\", \"\u00b0\", \"\u00b4\", \"\u02be\", \"\u2039\", \"\u203a\", \"\u00a9\", \"\u00ae\", \"\u2014\", \"\u2192\", \"\u3002\",\n                  \"\u3001\", \"\ufe42\", \"\ufe41\", \"\u2027\", \"\uff5e\", \"\ufe4f\", \"\uff0c\", \"\uff5b\", \"\uff5d\", \"\uff08\", \"\uff09\", \"\uff3b\", \"\uff3d\", \"\u3010\", \"\u3011\", \"\u2025\", \"\u303d\",\n                  \"\u300e\", \"\u300f\", \"\u301d\", \"\u301f\", \"\u27e8\", \"\u27e9\", \"\u301c\", \"\uff1a\", \"\uff01\", \"\uff1f\", \"\u266a\", \"\u061b\", \"/\", \"\\\\\", \"\u00ba\", \"\u2212\", \"^\", \"'\", \"\u02bb\", \"\u02c6\"]\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=\"test\")\n\nwer = load_metric(\"wer.py\") # https://github.com/jonatasgrosman/wav2vec2-sprint/blob/main/wer.py\ncer = load_metric(\"cer.py\") # https://github.com/jonatasgrosman/wav2vec2-sprint/blob/main/cer.py\n\nchars_to_ignore_regex = f\"[{re.escape(''.join(CHARS_TO_IGNORE))}]\"\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\nmodel.to(DEVICE)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n    batch[\"speech\"] = speech_array\n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, \"\", batch[\"sentence\"]).upper()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef evaluate(batch):\n    inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\n    with torch.no_grad():\n        logits = model(inputs.input_values.to(DEVICE), attention_mask=inputs.attention_mask.to(DEVICE)).logits\n\n    pred_ids = torch.argmax(logits, dim=-1)\n    batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n    return batch\n\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\n\npredictions = [x.upper() for x in result[\"pred_strings\"]]\nreferences = [x.upper() for x in result[\"sentence\"]]\n\nprint(f\"WER: {wer.compute(predictions=predictions, references=references, chunk_size=1000) * 100}\")\nprint(f\"CER: {cer.compute(predictions=predictions, references=references, chunk_size=1000) * 100}\")\n```\n\n**Test Result**:\n\nIn the table below I report the Word Error Rate (WER) and the Character Error Rate (CER) of the model. I ran the evaluation script described above on other models as well (on 2021-05-13). Note that the table below may show different results from those already reported, this may have been caused due to some specificity of the other evaluation scripts used.\n\n| Model | WER | CER |\n| ------------- | ------------- | ------------- |\n| jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn | **82.37%** | **19.03%** |\n| ydshieh/wav2vec2-large-xlsr-53-chinese-zh-cn-gpt | 84.01% | 20.95% |\n\n\n## Citation\nIf you want to cite this model you can use this:\n\n```bibtex\n@misc{grosman2021xlsr53-large-chinese,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {C}hinese},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn}},\n  year={2021}\n}\n```"
    },
    {
      "@id": "ark:59852/model-qwen-qwen2-5-7b-instruct-586jghlqcbj",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "Qwen/Qwen2.5-7B-Instruct",
      "description": "<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">",
      "author": "Qwen",
      "keywords": [
        "transformers",
        "safetensors",
        "qwen2",
        "text-generation",
        "chat",
        "conversational",
        "en",
        "arxiv:2309.00071",
        "arxiv:2407.10671",
        "base_model:Qwen/Qwen2.5-7B",
        "base_model:finetune:Qwen/Qwen2.5-7B",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-generation",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "hasBias": "**This repo contains the instruction-tuned 7B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 7.61B\n- Number of Paramaters (Non-Embedding): 6.53B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 28 for Q and 4 for KV\n- Context Length: Full 131,072 tokens and generation 8192 tokens\n  - Please refer to [this section](#processing-long-texts) for detailed instructions on how to deploy Qwen2.5 for handling long texts.",
      "usageInformation": "For deployment, we recommend using vLLM. \nPlease refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \nWe advise adding the `rope_scaling` configuration only when processing long contexts is required.",
      "baseModel": "Qwen/Qwen2.5-7B",
      "contentUrl": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/model-00001-of-00004.safetensors",
      "url": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Qwen2.5-7B-Instruct\n<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 7B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 7.61B\n- Number of Paramaters (Non-Embedding): 6.53B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 28 for Q and 4 for KV\n- Context Length: Full 131,072 tokens and generation 8192 tokens\n  - Please refer to [this section](#processing-long-texts) for detailed instructions on how to deploy Qwen2.5 for handling long texts.\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: 'qwen2'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n```json\n{\n  ...,\n  \"rope_scaling\": {\n    \"factor\": 4.0,\n    \"original_max_position_embeddings\": 32768,\n    \"type\": \"yarn\"\n  }\n}\n```\n\nFor deployment, we recommend using vLLM. \nPlease refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \nWe advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [\ud83d\udcd1 blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```"
    },
    {
      "@id": "ark:59852/model-facebook-contriever-qrgauvkai00",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "facebook/contriever",
      "description": "This model has been trained without supervision following the approach described in [Towards Unsupervised Dense Information Retrieval with Contrastive Learning](https://arxiv.org/abs/2112.09118). The associated GitHub repository is available here https://github.com/facebookresearch/contriever.",
      "author": "facebook",
      "keywords": [
        "transformers",
        "pytorch",
        "bert",
        "arxiv:2112.09118",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [],
      "usageInformation": "Using the model directly available in HuggingFace transformers requires to add a mean pooling operation to obtain a sentence embedding.\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained('facebook/contriever')\nmodel = AutoModel.from_pretrained('facebook/contriever')\n\nsentences = [\n    \"Where was Marie Curie born?\",\n    \"Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\",\n    \"Born in Paris on 15 May 1859, Pierre Curie was the son of Eug\u00e8ne Curie, a doctor of French Catholic origin from Alsace.\"\n]",
      "contentUrl": "https://huggingface.co/facebook/contriever/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/facebook/contriever",
      "isPartOf": [],
      "README": "This model has been trained without supervision following the approach described in [Towards Unsupervised Dense Information Retrieval with Contrastive Learning](https://arxiv.org/abs/2112.09118). The associated GitHub repository is available here https://github.com/facebookresearch/contriever.\n\n## Usage (HuggingFace Transformers)\nUsing the model directly available in HuggingFace transformers requires to add a mean pooling operation to obtain a sentence embedding.\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained('facebook/contriever')\nmodel = AutoModel.from_pretrained('facebook/contriever')\n\nsentences = [\n    \"Where was Marie Curie born?\",\n    \"Maria Sklodowska, later known as Marie Curie, was born on November 7, 1867.\",\n    \"Born in Paris on 15 May 1859, Pierre Curie was the son of Eug\u00e8ne Curie, a doctor of French Catholic origin from Alsace.\"\n]\n\n# Apply tokenizer\ninputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\noutputs = model(**inputs)\n\n# Mean pooling\ndef mean_pooling(token_embeddings, mask):\n    token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n    sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n    return sentence_embeddings\nembeddings = mean_pooling(outputs[0], inputs['attention_mask'])\n```"
    },
    {
      "@id": "ark:59852/model-facebookai-xlm-roberta-base-xnwortvbhfy",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "FacebookAI/xlm-roberta-base",
      "description": "XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).",
      "author": "FacebookAI",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "onnx",
        "safetensors",
        "xlm-roberta",
        "fill-mask",
        "exbert",
        "multilingual",
        "af",
        "am",
        "ar",
        "as",
        "az",
        "be",
        "bg",
        "bn",
        "br",
        "bs",
        "ca",
        "cs",
        "cy",
        "da",
        "de",
        "el",
        "en",
        "eo",
        "es",
        "et",
        "eu",
        "fa",
        "fi",
        "fr",
        "fy",
        "ga",
        "gd",
        "gl",
        "gu",
        "ha",
        "he",
        "hi",
        "hr",
        "hu",
        "hy",
        "id",
        "is",
        "it",
        "ja",
        "jv",
        "ka",
        "kk",
        "km",
        "kn",
        "ko",
        "ku",
        "ky",
        "la",
        "lo",
        "lt",
        "lv",
        "mg",
        "mk",
        "ml",
        "mn",
        "mr",
        "ms",
        "my",
        "ne",
        "nl",
        "no",
        "om",
        "or",
        "pa",
        "pl",
        "ps",
        "pt",
        "ro",
        "ru",
        "sa",
        "sd",
        "si",
        "sk",
        "sl",
        "so",
        "sq",
        "sr",
        "su",
        "sv",
        "sw",
        "ta",
        "te",
        "th",
        "tl",
        "tr",
        "ug",
        "uk",
        "ur",
        "uz",
        "vi",
        "xh",
        "yi",
        "zh",
        "arxiv:1911.02116",
        "license:mit",
        "autotrain_compatible",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "intendedUseCase": "You can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation, you should look at models like GPT2.",
      "usageInformation": "You can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='xlm-roberta-base')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n\n[{'score': 0.10563907772302628,\n  'sequence': \"Hello I'm a fashion model.\",\n  'token': 54543,\n  'token_str': 'fashion'},\n {'score': 0.08015287667512894,\n  'sequence': \"Hello I'm a new model.\",\n  'token': 3525,\n  'token_str': 'new'},\n {'score': 0.033413201570510864,\n  'sequence': \"Hello I'm a model model.\",\n  'token': 3299,\n  'token_str': 'model'},\n {'score': 0.030217764899134636,\n  'sequence': \"Hello I'm a French model.\",\n  'token': 92265,\n  'token_str': 'French'},\n {'score': 0.026436051353812218,\n  'sequence': \"Hello I'm a sexy model.\",\n  'token': 17473,\n  'token_str': 'sexy'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\nmodel = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")",
      "contentUrl": "https://huggingface.co/FacebookAI/xlm-roberta-base/resolve/main/model.safetensors",
      "url": "https://huggingface.co/FacebookAI/xlm-roberta-base",
      "license": "mit",
      "isPartOf": [],
      "README": "\n# XLM-RoBERTa (base-sized model) \n\nXLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr). \n\nDisclaimer: The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nXLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. \n\nRoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.\n\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.\n\nThis way, the model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the XLM-RoBERTa model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation, you should look at models like GPT2.\n\n## Usage\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='xlm-roberta-base')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n\n[{'score': 0.10563907772302628,\n  'sequence': \"Hello I'm a fashion model.\",\n  'token': 54543,\n  'token_str': 'fashion'},\n {'score': 0.08015287667512894,\n  'sequence': \"Hello I'm a new model.\",\n  'token': 3525,\n  'token_str': 'new'},\n {'score': 0.033413201570510864,\n  'sequence': \"Hello I'm a model model.\",\n  'token': 3299,\n  'token_str': 'model'},\n {'score': 0.030217764899134636,\n  'sequence': \"Hello I'm a French model.\",\n  'token': 92265,\n  'token_str': 'French'},\n {'score': 0.026436051353812218,\n  'sequence': \"Hello I'm a sexy model.\",\n  'token': 17473,\n  'token_str': 'sexy'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\nmodel = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")\n\n# prepare input\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\n\n# forward pass\noutput = model(**encoded_input)\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1911-02116,\n  author    = {Alexis Conneau and\n               Kartikay Khandelwal and\n               Naman Goyal and\n               Vishrav Chaudhary and\n               Guillaume Wenzek and\n               Francisco Guzm{\\'{a}}n and\n               Edouard Grave and\n               Myle Ott and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {Unsupervised Cross-lingual Representation Learning at Scale},\n  journal   = {CoRR},\n  volume    = {abs/1911.02116},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1911.02116},\n  eprinttype = {arXiv},\n  eprint    = {1911.02116},\n  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=xlm-roberta-base\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n"
    },
    {
      "@id": "ark:59852/model-autogluon-chronos-bolt-small-6xmiqq6gbud",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "autogluon/chronos-bolt-small",
      "description": "\ud83d\ude80 **Update Feb 14, 2025**: Chronos-Bolt models are now available on Amazon SageMaker JumpStart! Check out the [tutorial notebook](https://github.com/amazon-science/chronos-forecasting/blob/main/notebooks/deploy-chronos-to-amazon-sagemaker.ipynb) to learn how to deploy Chronos endpoints for production use in a few lines of code.",
      "author": "autogluon",
      "keywords": [
        "safetensors",
        "t5",
        "time series",
        "forecasting",
        "pretrained models",
        "foundation models",
        "time series foundation models",
        "time-series",
        "time-series-forecasting",
        "arxiv:1910.10683",
        "arxiv:2403.07815",
        "license:apache-2.0",
        "region:us"
      ],
      "version": "1.0",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "usageInformation": "Install the required dependencies.\n```\npip install autogluon\n```\nForecast with the Chronos-Bolt model.\n```python\nfrom autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n\ndf = TimeSeriesDataFrame(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_hourly/train.csv\")\n\npredictor = TimeSeriesPredictor(prediction_length=48).fit(\n    df,\n    hyperparameters={\n        \"Chronos\": {\"model_path\": \"autogluon/chronos-bolt-small\"},\n    },\n)\n\npredictions = predictor.predict(df)\n```\n\nFor more advanced features such as **fine-tuning** and **forecasting with covariates**, check out [this tutorial](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-chronos.html).",
      "contentUrl": "https://huggingface.co/autogluon/chronos-bolt-small/resolve/main/model.safetensors",
      "url": "https://huggingface.co/autogluon/chronos-bolt-small",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Chronos-Bolt\u26a1 (Small)\n\n\ud83d\ude80 **Update Feb 14, 2025**: Chronos-Bolt models are now available on Amazon SageMaker JumpStart! Check out the [tutorial notebook](https://github.com/amazon-science/chronos-forecasting/blob/main/notebooks/deploy-chronos-to-amazon-sagemaker.ipynb) to learn how to deploy Chronos endpoints for production use in a few lines of code.\n\nChronos-Bolt is a family of pretrained time series forecasting models which can be used for zero-shot forecasting. It is based on the [T5 encoder-decoder architecture](https://arxiv.org/abs/1910.10683) and has been trained on nearly 100 billion time series observations. It chunks the historical time series context into patches of multiple observations, which are then input into the encoder. The decoder then uses these representations to directly generate quantile forecasts across multiple future steps\u2014a method known as direct multi-step forecasting. Chronos-Bolt models are **more accurate**, up to **250 times faster** and **20 times more memory-efficient** than the [original Chronos](https://arxiv.org/abs/2403.07815) models of the same size.\n\n## Performance\n\nThe following plot compares the inference time of Chronos-Bolt against the original Chronos models for forecasting 1024 time series with a context length of 512 observations and a prediction horizon of 64 steps.\n\n<center>\n<img src=\"https://autogluon.s3.amazonaws.com/images/chronos_bolt_speed.svg\" width=\"50%\"/>\n</center>\n\nChronos-Bolt models are not only significantly faster but also more accurate than the original Chronos models. The following plot reports the probabilistic and point forecasting performance of Chronos-Bolt in terms of the [Weighted Quantile Loss (WQL)](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-metrics.html#autogluon.timeseries.metrics.WQL) and the [Mean Absolute Scaled Error (MASE)](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-metrics.html#autogluon.timeseries.metrics.MASE), respectively, aggregated over 27 datasets (see the [Chronos paper](https://arxiv.org/abs/2403.07815) for details on this benchmark). Remarkably, despite having no prior exposure to these datasets during training, the zero-shot Chronos-Bolt models outperform commonly used statistical models and deep learning models that have been trained on these datasets (highlighted by *). Furthermore, they also perform better than other FMs, denoted by a +, which indicates that these models were pretrained on certain datasets in our benchmark and are not entirely zero-shot. Notably, Chronos-Bolt (Base) also surpasses the original Chronos (Large) model in terms of the forecasting accuracy while being over 600 times faster.\n\n<center>\n<img src=\"https://autogluon.s3.amazonaws.com/images/chronos_bolt_accuracy.svg\" width=\"80%\"/>\n</center>\n\nChronos-Bolt models are available in the following sizes.\n\n\n<div align=\"center\">\n\n| Model                                                                  | Parameters | Based on                                                               |\n| ---------------------------------------------------------------------- | ---------- | ---------------------------------------------------------------------- |\n| [**chronos-bolt-tiny**](https://huggingface.co/autogluon/chronos-bolt-tiny)   | 9M         | [t5-efficient-tiny](https://huggingface.co/google/t5-efficient-tiny)   |\n| [**chronos-bolt-mini**](https://huggingface.co/autogluon/chronos-bolt-mini)   | 21M        | [t5-efficient-mini](https://huggingface.co/google/t5-efficient-mini)   |\n| [**chronos-bolt-small**](https://huggingface.co/autogluon/chronos-bolt-small) | 48M        | [t5-efficient-small](https://huggingface.co/google/t5-efficient-small) |\n| [**chronos-bolt-base**](https://huggingface.co/autogluon/chronos-bolt-base)   | 205M       | [t5-efficient-base](https://huggingface.co/google/t5-efficient-base)   |\n\n</div>\n\n\n## Usage\n\n### Zero-shot inference with Chronos-Bolt in AutoGluon\n\nInstall the required dependencies.\n```\npip install autogluon\n```\nForecast with the Chronos-Bolt model.\n```python\nfrom autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n\ndf = TimeSeriesDataFrame(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_hourly/train.csv\")\n\npredictor = TimeSeriesPredictor(prediction_length=48).fit(\n    df,\n    hyperparameters={\n        \"Chronos\": {\"model_path\": \"autogluon/chronos-bolt-small\"},\n    },\n)\n\npredictions = predictor.predict(df)\n```\n\nFor more advanced features such as **fine-tuning** and **forecasting with covariates**, check out [this tutorial](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-chronos.html).\n\n### Deploying a Chronos-Bolt endpoint to SageMaker\nFirst, update the SageMaker SDK to make sure that all the latest models are available.\n```\npip install -U sagemaker\n```\nDeploy an inference endpoint to SageMaker.\n```python\nfrom sagemaker.jumpstart.model import JumpStartModel\n\nmodel = JumpStartModel(\n    model_id=\"autogluon-forecasting-chronos-bolt-small\",\n    instance_type=\"ml.c5.2xlarge\",\n)\npredictor = model.deploy()\n```\nNow you can send time series data to the endpoint in JSON format.\n```python\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\")\n\npayload = {\n    \"inputs\": [\n        {\"target\": df[\"#Passengers\"].tolist()}\n    ],\n    \"parameters\": {\n        \"prediction_length\": 12,\n    }\n}\nforecast = predictor.predict(payload)[\"predictions\"]\n```\nChronos-Bolt models can be deployed to both CPU and GPU instances. These models also support **forecasting with covariates**. For more details about the endpoint API, check out the [example notebook](https://github.com/amazon-science/chronos-forecasting/blob/main/notebooks/deploy-chronos-bolt-to-amazon-sagemaker.ipynb).\n\n## Citation\n\nIf you find Chronos or Chronos-Bolt models useful for your research, please consider citing the associated [paper](https://arxiv.org/abs/2403.07815):\n\n```\n@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}\n```\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n"
    },
    {
      "@id": "ark:59852/model-sentence-transformers-gtr-t5-base-0djuzy4tek",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "sentence-transformers/gtr-t5-base",
      "description": "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space. The model was specifically trained for the task of semantic search.",
      "author": "sentence-transformers",
      "keywords": [
        "sentence-transformers",
        "pytorch",
        "safetensors",
        "t5",
        "feature-extraction",
        "sentence-similarity",
        "en",
        "arxiv:2112.07899",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "usageInformation": "Using this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/gtr-t5-base')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\nThe model requires sentence-transformers version 2.2.0 or newer.",
      "contentUrl": "https://huggingface.co/sentence-transformers/gtr-t5-base/resolve/main/2_Dense/model.safetensors",
      "url": "https://huggingface.co/sentence-transformers/gtr-t5-base",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# sentence-transformers/gtr-t5-base\n\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space. The model was specifically trained for the task of semantic search.\n\nThis model was converted from the Tensorflow model [gtr-base-1](https://tfhub.dev/google/gtr/gtr-base/1) to PyTorch. When using this model, have a look at the publication: [Large Dual Encoders Are Generalizable Retrievers](https://arxiv.org/abs/2112.07899). The tfhub model and this PyTorch model can produce slightly different embeddings, however, when run on the same benchmarks, they produce identical results.\n\nThe model uses only the encoder from a T5-base model. The weights are stored in FP16.  \n\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/gtr-t5-base')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\nThe model requires sentence-transformers version 2.2.0 or newer.\n\n## Citing & Authors\n\nIf you find this model helpful, please cite the respective publication:\n[Large Dual Encoders Are Generalizable Retrievers](https://arxiv.org/abs/2112.07899)\n"
    },
    {
      "@id": "ark:59852/model-jonatasgrosman-wav2vec2-large-xlsr-53-japanese-jflnukuwxq",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "jonatasgrosman/wav2vec2-large-xlsr-53-japanese",
      "description": "Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Japanese using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice), [CSS10](https://github.com/Kyubyong/css10) and [JSUT](https://sites.google.com/site/shinnosuketakamichi/publication/jsut).",
      "author": "jonatasgrosman",
      "keywords": [
        "transformers",
        "pytorch",
        "jax",
        "wav2vec2",
        "automatic-speech-recognition",
        "audio",
        "speech",
        "xlsr-fine-tuning-week",
        "ja",
        "dataset:common_voice",
        "doi:10.57967/hf/3568",
        "license:apache-2.0",
        "model-index",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/common_voice"
        }
      ],
      "usageInformation": "The model can be used directly (without a language model) as follows...\n\nUsing the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-japanese\")\naudio_paths = [\"/path/to/file.mp3\", \"/path/to/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"ja\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-japanese\"\nSAMPLES = 10\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)",
      "contentUrl": "https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Fine-tuned XLSR-53 large model for speech recognition in Japanese\n\nFine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Japanese using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice), [CSS10](https://github.com/Kyubyong/css10) and [JSUT](https://sites.google.com/site/shinnosuketakamichi/publication/jsut).\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\nThis model has been fine-tuned thanks to the GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/) :)\n\nThe script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint\n\n## Usage\n\nThe model can be used directly (without a language model) as follows...\n\nUsing the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-japanese\")\naudio_paths = [\"/path/to/file.mp3\", \"/path/to/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"ja\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-japanese\"\nSAMPLES = 10\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n    batch[\"speech\"] = speech_array\n    batch[\"sentence\"] = batch[\"sentence\"].upper()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\npredicted_sentences = processor.batch_decode(predicted_ids)\n\nfor i, predicted_sentence in enumerate(predicted_sentences):\n    print(\"-\" * 100)\n    print(\"Reference:\", test_dataset[i][\"sentence\"])\n    print(\"Prediction:\", predicted_sentence)\n```\n\n| Reference  | Prediction |\n| ------------- | ------------- |\n| \u7956\u6bcd\u306f\u3001\u304a\u304a\u3080\u306d\u6a5f\u5acc\u3088\u304f\u3001\u30b5\u30a4\u30b3\u30ed\u3092\u3053\u308d\u304c\u3057\u3066\u3044\u308b\u3002 | \u4eba\u6bcd\u306f\u91cd\u306b\u304d\u306d\u8d77\u304f\u3055\u3044\u304c\u3057\u3066\u3044\u308b |\n| \u8ca1\u5e03\u3092\u306a\u304f\u3057\u305f\u306e\u3067\u3001\u4ea4\u756a\u3078\u884c\u304d\u307e\u3059\u3002 | \u8ca1\u5e03\u3092\u306a\u304f\u624b\u7aef\u306e\u3067\u52fe\u756a\u3078\u884c\u304d\u307e\u3059 |\n| \u98f2\u307f\u5c4b\u306e\u304a\u3084\u3058\u3001\u65c5\u9928\u306e\u4e3b\u4eba\u3001\u533b\u8005\u3092\u306f\u3058\u3081\u3001\u4ea4\u969b\u306e\u3042\u308b\u4eba\u306b\u304d\u3044\u3066\u307e\u308f\u3063\u305f\u3089\u3001\u307f\u3093\u306a\u3001\u79c1\u3088\u308a\u53ce\u5165\u304c\u591a\u3044\u306f\u305a\u306a\u306e\u306b\u3001\u7a0e\u91d1\u306f\u5b89\u3044\u3002 | \u30ce\u5bae\u5c4b\u306e\u304a\u89aa\u3058\u65c5\u9928\u306e\u4e3b\u306b\u533b\u8005\u3092\u306f\u3058\u3081\u4ea4\u969b\u306e\u30a2\u30eb\u4eba\u30c8\u306b\u805e\u3044\u3066\u56de\u3063\u305f\u3089\u307f\u3093\u306a\u79c1\u3088\u308a\u53ce\u5165\u304c\u591a\u3044\u306f\u306a\u3046\u306b\u7a0e\u91d1\u306f\u5b89\u3044 |\n| \u65b0\u3057\u3044\u9774\u3092\u306f\u3044\u3066\u51fa\u304b\u3051\u307e\u3059\u3002 | \u3060\u3089\u3057\u3044\u9774\u3092\u306f\u3044\u3066\u51fa\u304b\u3051\u307e\u3059 |\n| \u3053\u306e\u305f\u3081\u30d7\u30e9\u30ba\u30de\u4e2d\u306e\u30a4\u30aa\u30f3\u3084\u96fb\u5b50\u306e\u6301\u3064\u5e73\u5747\u904b\u52d5\u30a8\u30cd\u30eb\u30ae\u30fc\u3092\u6e29\u5ea6\u3067\u8868\u73fe\u3059\u308b\u3053\u3068\u304c\u3042\u308b | \u3053\u306e\u305f\u3081\u30d7\u30e9\u30ba\u30de\u4e2d\u306e\u30a4\u30aa\u30f3\u3084\u96fb\u5b50\u306e\u6301\u3064\u5e73\u5747\u904b\u52d5\u30a8\u30cd\u30eb\u30ae\u30fc\u3092\u6e29\u5ea6\u3067\u8868\u5f01\u3059\u308b\u3053\u3068\u304c\u3042\u308b |\n| \u677e\u4e95\u3055\u3093\u306f\u30b5\u30c3\u30ab\u30fc\u3088\u308a\u91ce\u7403\u306e\u307b\u3046\u304c\u4e0a\u624b\u3067\u3059\u3002 | \u677e\u4e95\u3055\u3093\u306f\u30b5\u30c3\u30ab\u30fc\u3088\u308a\u91ce\u7403\u306e\u307b\u3046\u304c\u4e0a\u624b\u3067\u3059 |\n| \u65b0\u3057\u3044\u304a\u76bf\u3092\u4f7f\u3044\u307e\u3059\u3002 | \u65b0\u3057\u3044\u304a\u76bf\u3092\u4f7f\u3044\u307e\u3059 |\n| \u7d50\u5a5a\u4ee5\u6765\u4e09\u5e74\u534a\u3076\u308a\u306e\u6771\u4eac\u3082\u3001\u65e7\u53cb\u3068\u306e\u304a\u9152\u3082\u3001\u591c\u884c\u5217\u8eca\u3082\u3001\u99c5\u3067\u5bdd\u3066\u3001\u671d\u3092\u5f85\u3064\u306e\u3082\u4e45\u3057\u3076\u308a\u3060\u3002 | \u7d50\u5a5a\u30eb\u4e8c\u6765\u4e09\u5e74\u534a\u964d\u308a\u306e\u6771\u4eac\u3082\u5438\u3068\u306e\u304a\u9152\u3082\u91ce\u8d8a\u8005\u3082\u99c5\u3067\u5bdd\u3066\u671d\u3092\u5f85\u3064\u306e\u4e45\u3057\u3076\u308a\u305f |\n| \u3053\u308c\u307e\u3067\u3001\u5c11\u5e74\u91ce\u7403\u3001\u30de\u30de\u3055\u3093\u30d0\u30ec\u30fc\u306a\u3069\u3001\u5730\u57df\u30b9\u30dd\u30fc\u30c4\u3092\u652f\u3048\u3001\u5e02\u6c11\u306b\u5bc6\u7740\u3057\u3066\u304d\u305f\u306e\u306f\u3001\u7121\u6570\u306e\u30dc\u30e9\u30f3\u30c6\u30a3\u30a2\u3060\u3063\u305f\u3002 | \u3053\u308c\u307e\u3067\u5c11\u5e74\u91ce\u7403<unk>\u4e09\u30d0\u30ec\u30fc\u306a\u3069\u5730\u57df\u30b9\u30dd\u30fc\u30c4\u3092\u652f\u3048\u5e02\u6c11\u306b\u6e80\u7740\u3057\u3066\u304d\u305f\u306e\u306f\u5a18\u6570\u306e\u30dc\u30e9\u30f3\u30c6\u30a3\u30a2\u3060\u3063\u305f |\n| \u9774\u3092\u8131\u3044\u3067\u3001\u30b9\u30ea\u30c3\u30d1\u3092\u306f\u304d\u307e\u3059\u3002 | \u9774\u3092\u8131\u3044\u3067\u30b9\u30a4\u30d1\u30fc\u3092\u306f\u304d\u307e\u3059 |\n\n## Evaluation\n\nThe model can be evaluated as follows on the Japanese test data of Common Voice.\n\n```python\nimport torch\nimport re\nimport librosa\nfrom datasets import load_dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"ja\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-japanese\"\nDEVICE = \"cuda\"\n\nCHARS_TO_IGNORE = [\",\", \"?\", \"\u00bf\", \".\", \"!\", \"\u00a1\", \";\", \"\uff1b\", \":\", '\"\"', \"%\", '\"', \"\ufffd\", \"\u02bf\", \"\u00b7\", \"\u10fb\", \"~\", \"\u055e\",\n                   \"\u061f\", \"\u060c\", \"\u0964\", \"\u0965\", \"\u00ab\", \"\u00bb\", \"\u201e\", \"\u201c\", \"\u201d\", \"\u300c\", \"\u300d\", \"\u2018\", \"\u2019\", \"\u300a\", \"\u300b\", \"(\", \")\", \"[\", \"]\",\n                   \"{\", \"}\", \"=\", \"`\", \"_\", \"+\", \"<\", \">\", \"\u2026\", \"\u2013\", \"\u00b0\", \"\u00b4\", \"\u02be\", \"\u2039\", \"\u203a\", \"\u00a9\", \"\u00ae\", \"\u2014\", \"\u2192\", \"\u3002\",\n                   \"\u3001\", \"\ufe42\", \"\ufe41\", \"\u2027\", \"\uff5e\", \"\ufe4f\", \"\uff0c\", \"\uff5b\", \"\uff5d\", \"\uff08\", \"\uff09\", \"\uff3b\", \"\uff3d\", \"\u3010\", \"\u3011\", \"\u2025\", \"\u303d\",\n                   \"\u300e\", \"\u300f\", \"\u301d\", \"\u301f\", \"\u27e8\", \"\u27e9\", \"\u301c\", \"\uff1a\", \"\uff01\", \"\uff1f\", \"\u266a\", \"\u061b\", \"/\", \"\\\\\", \"\u00ba\", \"\u2212\", \"^\", \"'\", \"\u02bb\", \"\u02c6\"]\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=\"test\")\n\nwer = load_metric(\"wer.py\") # https://github.com/jonatasgrosman/wav2vec2-sprint/blob/main/wer.py\ncer = load_metric(\"cer.py\") # https://github.com/jonatasgrosman/wav2vec2-sprint/blob/main/cer.py\n\nchars_to_ignore_regex = f\"[{re.escape(''.join(CHARS_TO_IGNORE))}]\"\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\nmodel.to(DEVICE)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n    batch[\"speech\"] = speech_array\n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, \"\", batch[\"sentence\"]).upper()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef evaluate(batch):\n    inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\n    with torch.no_grad():\n        logits = model(inputs.input_values.to(DEVICE), attention_mask=inputs.attention_mask.to(DEVICE)).logits\n\n    pred_ids = torch.argmax(logits, dim=-1)\n    batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n    return batch\n\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\n\npredictions = [x.upper() for x in result[\"pred_strings\"]]\nreferences = [x.upper() for x in result[\"sentence\"]]\n\nprint(f\"WER: {wer.compute(predictions=predictions, references=references, chunk_size=1000) * 100}\")\nprint(f\"CER: {cer.compute(predictions=predictions, references=references, chunk_size=1000) * 100}\")\n```\n\n**Test Result**:\n\nIn the table below I report the Word Error Rate (WER) and the Character Error Rate (CER) of the model. I ran the evaluation script described above on other models as well (on 2021-05-10). Note that the table below may show different results from those already reported, this may have been caused due to some specificity of the other evaluation scripts used.\n\n| Model | WER | CER |\n| ------------- | ------------- | ------------- |\n| jonatasgrosman/wav2vec2-large-xlsr-53-japanese | **81.80%** | **20.16%** |\n| vumichien/wav2vec2-large-xlsr-japanese | 1108.86% | 23.40% |\n| qqhann/w2v_hf_jsut_xlsr53 | 1012.18% | 70.77% |\n\n## Citation\nIf you want to cite this model you can use this:\n\n```bibtex\n@misc{grosman2021xlsr53-large-japanese,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {J}apanese},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese}},\n  year={2021}\n}\n```"
    },
    {
      "@id": "ark:59852/model-baai-bge-m3-darh3ir4qvp",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "BAAI/bge-m3",
      "description": "For more details please refer to our github repo: https://github.com/FlagOpen/FlagEmbedding",
      "author": "BAAI",
      "keywords": [
        "sentence-transformers",
        "pytorch",
        "onnx",
        "xlm-roberta",
        "feature-extraction",
        "sentence-similarity",
        "arxiv:2402.03216",
        "arxiv:2004.04906",
        "arxiv:2106.14807",
        "arxiv:2107.05720",
        "arxiv:2004.12832",
        "license:mit",
        "autotrain_compatible",
        "text-embeddings-inference",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [],
      "usageInformation": "Install: \n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\npip install -e .\n```\nor: \n```\npip install -U FlagEmbedding\n```\n\n\n\n### Generate Embedding for text\n\n- Dense Embedding\n```python\nfrom FlagEmbedding import BGEM3FlagModel\n\nmodel = BGEM3FlagModel('BAAI/bge-m3',  \n                       use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nsentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\nsentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n\nembeddings_1 = model.encode(sentences_1, \n                            batch_size=12, \n                            max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n                            )['dense_vecs']\nembeddings_2 = model.encode(sentences_2)['dense_vecs']\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)",
      "contentUrl": "https://huggingface.co/BAAI/bge-m3/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/BAAI/bge-m3",
      "license": "mit",
      "isPartOf": [],
      "README": "\nFor more details please refer to our github repo: https://github.com/FlagOpen/FlagEmbedding\n\n# BGE-M3 ([paper](https://arxiv.org/pdf/2402.03216.pdf), [code](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3))\n\nIn this project, we introduce BGE-M3, which is distinguished for its versatility in Multi-Functionality, Multi-Linguality, and Multi-Granularity. \n- Multi-Functionality: It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval. \n- Multi-Linguality: It can support more than 100 working languages. \n- Multi-Granularity: It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. \n\n\n\n**Some suggestions for retrieval pipeline in RAG**\n\nWe recommend to use the following pipeline: hybrid retrieval + re-ranking. \n- Hybrid retrieval leverages the strengths of various methods, offering higher accuracy and stronger generalization capabilities. \nA classic example: using both embedding retrieval and the BM25 algorithm. \nNow, you can try to use BGE-M3, which supports both embedding and sparse retrieval. \nThis allows you to obtain token weights (similar to the BM25) without any additional cost when generate dense embeddings.\nTo use hybrid retrieval, you can refer to [Vespa](https://github.com/vespa-engine/pyvespa/blob/master/docs/sphinx/source/examples/mother-of-all-embedding-models-cloud.ipynb\n) and [Milvus](https://github.com/milvus-io/pymilvus/blob/master/examples/hello_hybrid_sparse_dense.py).\n\n- As cross-encoder models, re-ranker demonstrates higher accuracy than bi-encoder embedding model. \nUtilizing the re-ranking model (e.g., [bge-reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker), [bge-reranker-v2](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker)) after retrieval can further filter the selected text.\n\n\n## News:\n- 2024/7/1: **We update the MIRACL evaluation results of BGE-M3**. To reproduce the new results, you can refer to: [bge-m3_miracl_2cr](https://huggingface.co/datasets/hanhainebula/bge-m3_miracl_2cr). We have also updated our [paper](https://arxiv.org/pdf/2402.03216) on arXiv.\n  <details>\n  <summary> Details </summary>\n\n  The previous test results were lower because we mistakenly removed the passages that have the same id as the query from the search results. After correcting this mistake, the overall performance of BGE-M3 on MIRACL is higher than the previous results, but the experimental conclusion remains unchanged. The other results are not affected by this mistake. To reproduce the previous lower results, you need to add the `--remove-query` parameter when using `pyserini.search.faiss` or `pyserini.search.lucene` to search the passages.\n\n  </details>\n- 2024/3/20: **Thanks Milvus team!** Now you can use hybrid retrieval of bge-m3 in Milvus: [pymilvus/examples\n/hello_hybrid_sparse_dense.py](https://github.com/milvus-io/pymilvus/blob/master/examples/hello_hybrid_sparse_dense.py).\n- 2024/3/8: **Thanks for the [experimental results](https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05) from @[Yannael](https://huggingface.co/Yannael). In this benchmark, BGE-M3 achieves top performance in both English and other languages, surpassing models such as OpenAI.**\n- 2024/3/2: Release unified fine-tuning [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/unified_finetune) and [data](https://huggingface.co/datasets/Shitao/bge-m3-data) \n- 2024/2/6: We release the [MLDR](https://huggingface.co/datasets/Shitao/MLDR) (a long document retrieval dataset covering 13 languages) and [evaluation pipeline](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB/MLDR). \n- 2024/2/1: **Thanks for the excellent tool from Vespa.** You can easily use multiple modes of BGE-M3 following this [notebook](https://github.com/vespa-engine/pyvespa/blob/master/docs/sphinx/source/examples/mother-of-all-embedding-models-cloud.ipynb)\n\n\n## Specs\n\n- Model  \n\n| Model Name |  Dimension | Sequence Length | Introduction |\n|:----:|:---:|:---:|:---:|\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3) | 1024 | 8192 | multilingual; unified fine-tuning (dense, sparse, and colbert) from bge-m3-unsupervised|\n| [BAAI/bge-m3-unsupervised](https://huggingface.co/BAAI/bge-m3-unsupervised) | 1024 | 8192 | multilingual; contrastive learning from bge-m3-retromae |\n| [BAAI/bge-m3-retromae](https://huggingface.co/BAAI/bge-m3-retromae) | -- | 8192 | multilingual; extend the max_length of [xlm-roberta](https://huggingface.co/FacebookAI/xlm-roberta-large) to 8192 and further pretrained via [retromae](https://github.com/staoxiao/RetroMAE)| \n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) | 1024 | 512 | English model | \n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |  768 | 512 | English model | \n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |  384 | 512 | English model | \n\n- Data\n\n|                          Dataset                           |                   Introduction                    |\n|:----------------------------------------------------------:|:-------------------------------------------------:|\n|    [MLDR](https://huggingface.co/datasets/Shitao/MLDR)     | Docuemtn Retrieval Dataset, covering 13 languages |\n| [bge-m3-data](https://huggingface.co/datasets/Shitao/bge-m3-data) |          Fine-tuning data used by bge-m3          |\n\n\n\n## FAQ\n\n**1. Introduction for different retrieval methods**\n\n- Dense retrieval: map the text into a single embedding, e.g., [DPR](https://arxiv.org/abs/2004.04906), [BGE-v1.5](https://github.com/FlagOpen/FlagEmbedding)\n- Sparse retrieval (lexical matching): a vector of size equal to the vocabulary, with the majority of positions set to zero, calculating a weight only for tokens present in the text. e.g., BM25, [unicoil](https://arxiv.org/pdf/2106.14807.pdf), and [splade](https://arxiv.org/abs/2107.05720)\n- Multi-vector retrieval: use multiple vectors to represent a text, e.g., [ColBERT](https://arxiv.org/abs/2004.12832).\n\n\n**2. How to use BGE-M3 in other projects?**\n\nFor embedding retrieval, you can employ the BGE-M3 model using the same approach as BGE. \nThe only difference is that the BGE-M3 model no longer requires adding instructions to the queries. \n\nFor hybrid retrieval, you can use [Vespa](https://github.com/vespa-engine/pyvespa/blob/master/docs/sphinx/source/examples/mother-of-all-embedding-models-cloud.ipynb\n) and [Milvus](https://github.com/milvus-io/pymilvus/blob/master/examples/hello_hybrid_sparse_dense.py).\n\n\n**3. How to fine-tune bge-M3 model?**\n\nYou can follow the common in this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) \nto fine-tune the dense embedding.\n\nIf you want to fine-tune all embedding function of m3 (dense, sparse and colbert), you can refer to the [unified_fine-tuning example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/unified_finetune)\n\n\n\n\n\n\n## Usage\n\nInstall: \n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\npip install -e .\n```\nor: \n```\npip install -U FlagEmbedding\n```\n\n\n\n### Generate Embedding for text\n\n- Dense Embedding\n```python\nfrom FlagEmbedding import BGEM3FlagModel\n\nmodel = BGEM3FlagModel('BAAI/bge-m3',  \n                       use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nsentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\nsentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n\nembeddings_1 = model.encode(sentences_1, \n                            batch_size=12, \n                            max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n                            )['dense_vecs']\nembeddings_2 = model.encode(sentences_2)['dense_vecs']\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n# [[0.6265, 0.3477], [0.3499, 0.678 ]]\n```\nYou also can use sentence-transformers and huggingface transformers to generate dense embeddings.\nRefer to [baai_general_embedding](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding#usage) for details.\n\n\n- Sparse Embedding (Lexical Weight)\n```python\nfrom FlagEmbedding import BGEM3FlagModel\n\nmodel = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nsentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\nsentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n\noutput_1 = model.encode(sentences_1, return_dense=True, return_sparse=True, return_colbert_vecs=False)\noutput_2 = model.encode(sentences_2, return_dense=True, return_sparse=True, return_colbert_vecs=False)\n\n# you can see the weight for each token:\nprint(model.convert_id_to_token(output_1['lexical_weights']))\n# [{'What': 0.08356, 'is': 0.0814, 'B': 0.1296, 'GE': 0.252, 'M': 0.1702, '3': 0.2695, '?': 0.04092}, \n#  {'De': 0.05005, 'fin': 0.1368, 'ation': 0.04498, 'of': 0.0633, 'BM': 0.2515, '25': 0.3335}]\n\n\n# compute the scores via lexical mathcing\nlexical_scores = model.compute_lexical_matching_score(output_1['lexical_weights'][0], output_2['lexical_weights'][0])\nprint(lexical_scores)\n# 0.19554901123046875\n\nprint(model.compute_lexical_matching_score(output_1['lexical_weights'][0], output_1['lexical_weights'][1]))\n# 0.0\n```\n\n- Multi-Vector (ColBERT)\n```python\nfrom FlagEmbedding import BGEM3FlagModel\n\nmodel = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True) \n\nsentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\nsentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n\noutput_1 = model.encode(sentences_1, return_dense=True, return_sparse=True, return_colbert_vecs=True)\noutput_2 = model.encode(sentences_2, return_dense=True, return_sparse=True, return_colbert_vecs=True)\n\nprint(model.colbert_score(output_1['colbert_vecs'][0], output_2['colbert_vecs'][0]))\nprint(model.colbert_score(output_1['colbert_vecs'][0], output_2['colbert_vecs'][1]))\n# 0.7797\n# 0.4620\n```\n\n\n### Compute score for text pairs\nInput a list of text pairs, you can get the scores computed by different methods.\n```python\nfrom FlagEmbedding import BGEM3FlagModel\n\nmodel = BGEM3FlagModel('BAAI/bge-m3',  use_fp16=True) \n\nsentences_1 = [\"What is BGE M3?\", \"Defination of BM25\"]\nsentences_2 = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\", \n               \"BM25 is a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document\"]\n\nsentence_pairs = [[i,j] for i in sentences_1 for j in sentences_2]\n\nprint(model.compute_score(sentence_pairs, \n                          max_passage_length=128, # a smaller max length leads to a lower latency\n                          weights_for_different_modes=[0.4, 0.2, 0.4])) # weights_for_different_modes(w) is used to do weighted sum: w[0]*dense_score + w[1]*sparse_score + w[2]*colbert_score\n\n# {\n#   'colbert': [0.7796499729156494, 0.4621465802192688, 0.4523794651031494, 0.7898575067520142], \n#   'sparse': [0.195556640625, 0.00879669189453125, 0.0, 0.1802978515625], \n#   'dense': [0.6259765625, 0.347412109375, 0.349853515625, 0.67822265625], \n#   'sparse+dense': [0.482503205537796, 0.23454029858112335, 0.2332356721162796, 0.5122477412223816], \n#   'colbert+sparse+dense': [0.6013619303703308, 0.3255828022956848, 0.32089319825172424, 0.6232916116714478]\n# }\n```\n\n\n\n\n## Evaluation  \n\nWe provide the evaluation script for [MKQA](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB/MKQA) and [MLDR](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB/MLDR)\n\n### Benchmarks from the open-source community\n  ![avatar](./imgs/others.webp)\n The BGE-M3 model emerged as the top performer on this benchmark (OAI is short for OpenAI). \n  For more details, please refer to the [article](https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05) and [Github Repo](https://github.com/Yannael/multilingual-embeddings)\n\n\n### Our results\n- Multilingual (Miracl dataset) \n\n![avatar](./imgs/miracl.jpg)\n\n- Cross-lingual (MKQA dataset)\n\n![avatar](./imgs/mkqa.jpg)\n\n- Long Document Retrieval\n  - MLDR:   \n  ![avatar](./imgs/long.jpg)\n  Please note that [MLDR](https://huggingface.co/datasets/Shitao/MLDR) is a document retrieval dataset we constructed via LLM, \n  covering 13 languages, including test set, validation set, and training set. \n  We utilized the training set from MLDR to enhance the model's long document retrieval capabilities. \n  Therefore, comparing baselines with `Dense w.o.long`(fine-tuning without long document dataset) is more equitable. \n  Additionally, this long document retrieval dataset will be open-sourced to address the current lack of open-source multilingual long text retrieval datasets.\n  We believe that this data will be helpful for the open-source community in training document retrieval models.\n\n  - NarritiveQA:  \n  ![avatar](./imgs/nqa.jpg)\n\n- Comparison with BM25  \n\nWe utilized Pyserini to implement BM25, and the test results can be reproduced by this [script](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB/MLDR#bm25-baseline).\nWe tested BM25 using two different tokenizers: \none using Lucene Analyzer and the other using the same tokenizer as M3 (i.e., the tokenizer of xlm-roberta). \nThe results indicate that BM25 remains a competitive baseline, \nespecially in long document retrieval.\n\n![avatar](./imgs/bm25.jpg)\n\n\n\n## Training\n- Self-knowledge Distillation: combining multiple outputs from different \nretrieval modes as reward signal to enhance the performance of single mode(especially for sparse retrieval and multi-vec(colbert) retrival)\n- Efficient Batching: Improve the efficiency when fine-tuning on long text. \nThe small-batch strategy is simple but effective, which also can used to fine-tune large embedding model.\n- MCLS: A simple method to improve the performance on long text without fine-tuning. \nIf you have no enough resource to fine-tuning model with long text, the method is useful.\n\nRefer to our [report](https://arxiv.org/pdf/2402.03216.pdf) for more details. \n\n\n\n\n\n\n## Acknowledgement\n\nThanks to the authors of open-sourced datasets, including Miracl, MKQA, NarritiveQA, etc. \nThanks to the open-sourced libraries like [Tevatron](https://github.com/texttron/tevatron), [Pyserini](https://github.com/castorini/pyserini).\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge-m3,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n"
    },
    {
      "@id": "ark:59852/model-jonatasgrosman-wav2vec2-large-xlsr-53-arabic-fuk3yq59zve",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "jonatasgrosman/wav2vec2-large-xlsr-53-arabic",
      "description": "Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Arabic using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice) and [Arabic Speech Corpus](https://huggingface.co/datasets/arabic_speech_corpus).",
      "author": "jonatasgrosman",
      "keywords": [
        "transformers",
        "pytorch",
        "jax",
        "wav2vec2",
        "automatic-speech-recognition",
        "audio",
        "speech",
        "xlsr-fine-tuning-week",
        "ar",
        "dataset:common_voice",
        "dataset:arabic_speech_corpus",
        "doi:10.57967/hf/3573",
        "license:apache-2.0",
        "model-index",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/common_voice"
        },
        {
          "@id": "https://huggingface.co/datasets/arabic_speech_corpus"
        }
      ],
      "usageInformation": "The model can be used directly (without a language model) as follows...\n\nUsing the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-arabic\")\naudio_paths = [\"/path/to/file.mp3\", \"/path/to/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"ar\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-arabic\"\nSAMPLES = 10\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)",
      "contentUrl": "https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-arabic/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-arabic",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Fine-tuned XLSR-53 large model for speech recognition in Arabic\n\nFine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Arabic using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice) and [Arabic Speech Corpus](https://huggingface.co/datasets/arabic_speech_corpus).\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\nThis model has been fine-tuned thanks to the GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/) :)\n\nThe script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint\n\n## Usage\n\nThe model can be used directly (without a language model) as follows...\n\nUsing the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-arabic\")\naudio_paths = [\"/path/to/file.mp3\", \"/path/to/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"ar\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-arabic\"\nSAMPLES = 10\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n    batch[\"speech\"] = speech_array\n    batch[\"sentence\"] = batch[\"sentence\"].upper()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\npredicted_sentences = processor.batch_decode(predicted_ids)\n\nfor i, predicted_sentence in enumerate(predicted_sentences):\n    print(\"-\" * 100)\n    print(\"Reference:\", test_dataset[i][\"sentence\"])\n    print(\"Prediction:\", predicted_sentence)\n```\n\n| Reference  | Prediction |\n| ------------- | ------------- |\n| \u0623\u0644\u062f\u064a\u0643 \u0642\u0644\u0645 \u061f | \u0623\u0644\u062f\u064a\u0643 \u0642\u0644\u0645 |\n| \u0644\u064a\u0633\u062a \u0647\u0646\u0627\u0643 \u0645\u0633\u0627\u0641\u0629 \u0639\u0644\u0649 \u0647\u0630\u0647 \u0627\u0644\u0623\u0631\u0636 \u0623\u0628\u0639\u062f \u0645\u0646 \u064a\u0648\u0645 \u0623\u0645\u0633. | \u0644\u064a\u0633\u062a \u0646\u0627\u0644\u0643 \u0645\u0633\u0627\u0641\u0629 \u0639\u0644\u0649 \u0647\u0630\u0647 \u0627\u0644\u0623\u0631\u0636 \u0623\u0628\u0639\u062f \u0645\u0646 \u064a\u0648\u0645 \u0627\u0644\u0623\u0645\u0633  \u0645 |\n| \u0625\u0646\u0643 \u062a\u0643\u0628\u0631 \u0627\u0644\u0645\u0634\u0643\u0644\u0629. | \u0625\u0646\u0643 \u062a\u0643\u0628\u0631 \u0627\u0644\u0645\u0634\u0643\u0644\u0629 |\n| \u064a\u0631\u063a\u0628 \u0623\u0646 \u064a\u0644\u062a\u0642\u064a \u0628\u0643. | \u064a\u0631\u063a\u0628 \u0623\u0646 \u064a\u0644\u062a\u0642\u064a \u0628\u0643 |\n| \u0625\u0646\u0647\u0645 \u0644\u0627 \u064a\u0639\u0631\u0641\u0648\u0646 \u0644\u0645\u0627\u0630\u0627 \u062d\u062a\u0649. | \u0625\u0646\u0647\u0645 \u0644\u0627 \u064a\u0639\u0631\u0641\u0648\u0646 \u0644\u0645\u0627\u0630\u0627 \u062d\u062a\u0649 |\n| \u0633\u064a\u0633\u0639\u062f\u0646\u064a \u0645\u0633\u0627\u0639\u062f\u062a\u0643 \u0623\u064a \u0648\u0642\u062a \u062a\u062d\u0628. | \u0633\u064a\u0633\u0626\u062f\u0646\u064a\u0645\u0633\u0627\u0639\u062f\u062a\u0643 \u0623\u064a \u0648\u0642\u062f \u062a\u062d\u0628 |\n| \u0623\u064e\u062d\u064e\u0628\u0651\u064f \u0646\u0638\u0631\u064a\u0651\u0629 \u0639\u0644\u0645\u064a\u0629 \u0625\u0644\u064a\u0651 \u0647\u064a \u0623\u0646 \u062d\u0644\u0642\u0627\u062a \u0632\u062d\u0644 \u0645\u0643\u0648\u0646\u0629 \u0628\u0627\u0644\u0643\u0627\u0645\u0644 \u0645\u0646 \u0627\u0644\u0623\u0645\u062a\u0639\u0629 \u0627\u0644\u0645\u0641\u0642\u0648\u062f\u0629. | \u0623\u062d\u0628 \u0646\u0638\u0631\u064a\u0629 \u0639\u0644\u0645\u064a\u0629 \u0625\u0644\u064a  \u0647\u064a \u0623\u0646 \u062d\u0644 \u0642\u062a\u0632\u062d \u0627\u0644\u0645\u0643\u0648\u064a\u0646\u0627 \u0628\u0627\u0644\u0643\u0627\u0645\u0644 \u0645\u0646 \u0627\u0644\u0623\u0645\u062a \u0639\u0646 \u0627\u0644\u0645\u0641\u0642\u0648\u062f\u0629 |\n| \u0633\u0623\u0634\u062a\u0631\u064a \u0644\u0647 \u0642\u0644\u0645\u0627\u064b. | \u0633\u0623\u0634\u062a\u0631\u064a \u0644\u0647 \u0642\u0644\u0645\u0627 |\n| \u0623\u064a\u0646 \u0627\u0644\u0645\u0634\u0643\u0644\u0629 \u061f | \u0623\u064a\u0646 \u0627\u0644\u0645\u0634\u0643\u0644 |\n| \u0648\u064e\u0644\u0650\u0644\u0651\u064e\u0647\u0650 \u064a\u064e\u0633\u0652\u062c\u064f\u062f\u064f \u0645\u064e\u0627 \u0641\u0650\u064a \u0627\u0644\u0633\u0651\u064e\u0645\u064e\u0627\u0648\u064e\u0627\u062a\u0650 \u0648\u064e\u0645\u064e\u0627 \u0641\u0650\u064a \u0627\u0644\u0652\u0623\u064e\u0631\u0652\u0636\u0650 \u0645\u0650\u0646\u0652 \u062f\u064e\u0627\u0628\u0651\u064e\u0629\u064d \u0648\u064e\u0627\u0644\u0652\u0645\u064e\u0644\u064e\u0627\u0626\u0650\u0643\u064e\u0629\u064f \u0648\u064e\u0647\u064f\u0645\u0652 \u0644\u064e\u0627 \u064a\u064e\u0633\u0652\u062a\u064e\u0643\u0652\u0628\u0650\u0631\u064f\u0648\u0646\u064e | \u0648\u0644\u0644\u0647 \u064a\u0633\u062c\u062f \u0645\u0627 \u0641\u064a \u0627\u0644\u0633\u0645\u0627\u0648\u0627\u062a \u0648\u0645\u0627 \u0641\u064a \u0627\u0644\u0623\u0631\u0636 \u0645\u0646 \u062f\u0627\u0628\u0629 \u0648\u0627\u0644\u0645\u0644\u0627\u0626\u0643\u0629 \u0648\u0647\u0645 \u0644\u0627 \u064a\u0633\u062a\u0643\u0628\u0631\u0648\u0646 |\n\n## Evaluation\n\nThe model can be evaluated as follows on the Arabic test data of Common Voice.\n\n```python\nimport torch\nimport re\nimport librosa\nfrom datasets import load_dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"ar\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-arabic\"\nDEVICE = \"cuda\"\n\nCHARS_TO_IGNORE = [\",\", \"?\", \"\u00bf\", \".\", \"!\", \"\u00a1\", \";\", \"\uff1b\", \":\", '\"\"', \"%\", '\"', \"\ufffd\", \"\u02bf\", \"\u00b7\", \"\u10fb\", \"~\", \"\u055e\",\n                  \"\u061f\", \"\u060c\", \"\u0964\", \"\u0965\", \"\u00ab\", \"\u00bb\", \"\u201e\", \"\u201c\", \"\u201d\", \"\u300c\", \"\u300d\", \"\u2018\", \"\u2019\", \"\u300a\", \"\u300b\", \"(\", \")\", \"[\", \"]\",\n                  \"{\", \"}\", \"=\", \"`\", \"_\", \"+\", \"<\", \">\", \"\u2026\", \"\u2013\", \"\u00b0\", \"\u00b4\", \"\u02be\", \"\u2039\", \"\u203a\", \"\u00a9\", \"\u00ae\", \"\u2014\", \"\u2192\", \"\u3002\",\n                  \"\u3001\", \"\ufe42\", \"\ufe41\", \"\u2027\", \"\uff5e\", \"\ufe4f\", \"\uff0c\", \"\uff5b\", \"\uff5d\", \"\uff08\", \"\uff09\", \"\uff3b\", \"\uff3d\", \"\u3010\", \"\u3011\", \"\u2025\", \"\u303d\",\n                  \"\u300e\", \"\u300f\", \"\u301d\", \"\u301f\", \"\u27e8\", \"\u27e9\", \"\u301c\", \"\uff1a\", \"\uff01\", \"\uff1f\", \"\u266a\", \"\u061b\", \"/\", \"\\\\\", \"\u00ba\", \"\u2212\", \"^\", \"'\", \"\u02bb\", \"\u02c6\"]\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=\"test\")\n\nwer = load_metric(\"wer.py\") # https://github.com/jonatasgrosman/wav2vec2-sprint/blob/main/wer.py\ncer = load_metric(\"cer.py\") # https://github.com/jonatasgrosman/wav2vec2-sprint/blob/main/cer.py\n\nchars_to_ignore_regex = f\"[{re.escape(''.join(CHARS_TO_IGNORE))}]\"\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\nmodel.to(DEVICE)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n    batch[\"speech\"] = speech_array\n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, \"\", batch[\"sentence\"]).upper()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef evaluate(batch):\n    inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\n    with torch.no_grad():\n        logits = model(inputs.input_values.to(DEVICE), attention_mask=inputs.attention_mask.to(DEVICE)).logits\n\n    pred_ids = torch.argmax(logits, dim=-1)\n    batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n    return batch\n\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\n\npredictions = [x.upper() for x in result[\"pred_strings\"]]\nreferences = [x.upper() for x in result[\"sentence\"]]\n\nprint(f\"WER: {wer.compute(predictions=predictions, references=references, chunk_size=1000) * 100}\")\nprint(f\"CER: {cer.compute(predictions=predictions, references=references, chunk_size=1000) * 100}\")\n```\n\n**Test Result**:\n\nIn the table below I report the Word Error Rate (WER) and the Character Error Rate (CER) of the model. I ran the evaluation script described above on other models as well (on 2021-05-14). Note that the table below may show different results from those already reported, this may have been caused due to some specificity of the other evaluation scripts used.\n\n| Model | WER | CER |\n| ------------- | ------------- | ------------- |\n| jonatasgrosman/wav2vec2-large-xlsr-53-arabic | **39.59%** | **18.18%** |\n| bakrianoo/sinai-voice-ar-stt | 45.30% | 21.84% |\n| othrif/wav2vec2-large-xlsr-arabic | 45.93% | 20.51% |\n| kmfoda/wav2vec2-large-xlsr-arabic | 54.14% | 26.07% |\n| mohammed/wav2vec2-large-xlsr-arabic | 56.11% | 26.79% |\n| anas/wav2vec2-large-xlsr-arabic | 62.02% | 27.09% |\n| elgeish/wav2vec2-large-xlsr-53-arabic | 100.00% | 100.56% |\n\n## Citation\nIf you want to cite this model you can use this:\n\n```bibtex\n@misc{grosman2021xlsr53-large-arabic,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {A}rabic},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-arabic}},\n  year={2021}\n}\n```"
    },
    {
      "@id": "ark:59852/model-qwen-qwen3-0-6b-jriwtxoazf",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "Qwen/Qwen3-0.6B",
      "description": "<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">",
      "author": "Qwen",
      "keywords": [
        "transformers",
        "safetensors",
        "qwen3",
        "text-generation",
        "conversational",
        "arxiv:2505.09388",
        "base_model:Qwen/Qwen3-0.6B-Base",
        "base_model:finetune:Qwen/Qwen3-0.6B-Base",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-generation",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "intendedUseCase": "For local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.",
      "usageInformation": "We provide a soft switch mechanism that allows users to dynamically control the model's behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-0.6B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)",
      "baseModel": "Qwen/Qwen3-0.6B-Base",
      "contentUrl": "https://huggingface.co/Qwen/Qwen3-0.6B/resolve/main/model.safetensors",
      "url": "https://huggingface.co/Qwen/Qwen3-0.6B",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Qwen3-0.6B\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Qwen3 Highlights\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n- **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n## Model Overview\n\n**Qwen3-0.6B** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 0.6B\n- Number of Paramaters (Non-Embedding): 0.44B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 16 for Q and 8 for KV\n- Context Length: 32,768 \n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n> [!TIP]\n> If you encounter significant endless repetitions, please refer to the [Best Practices](#best-practices) section for optimal sampling parameters, and set the ``presence_penalty`` to 1.5.\n\n## Quickstart\n\nThe code of Qwen3 has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: 'qwen3'\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-0.6B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-0.6B --reasoning-parser qwen3\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-0.6B --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Switching Between Thinking and Non-Thinking Mode\n\n> [!TIP]\n> The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. \n> Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### `enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `<think>...</think>` block, followed by the final response.\n\n> [!NOTE]\n> For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n\n### `enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `<think>...</think>` block.\n\n> [!NOTE]\n> For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n### Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model's behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-0.6B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")\n```\n\n> [!NOTE]\n> For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `<think>...</think>`. However, the content inside this block may be empty if thinking is disabled.\n> When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `<think>...</think>` block.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    'model': 'Qwen3-0.6B',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # 'model_type': 'qwen_dashscope',\n    # 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    'model_server': 'http://localhost:8000/v1',  # api_base\n    'api_key': 'EMPTY',\n\n    # Other parameters:\n    # 'generate_cfg': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         'thought_in_content': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {'mcpServers': {  # You can specify the MCP configuration file\n            'time': {\n                'command': 'uvx',\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n            },\n            \"fetch\": {\n                \"command\": \"uvx\",\n                \"args\": [\"mcp-server-fetch\"]\n            }\n        }\n    },\n  'code_interpreter',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n   - For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g., `\"answer\": \"C\"`.\"\n\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```"
    },
    {
      "@id": "ark:59852/model-cross-encoder-ms-marco-minilm-l6-v2-zyb6ofgc3g2",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "cross-encoder/ms-marco-MiniLM-L6-v2",
      "description": "This model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.",
      "author": "cross-encoder",
      "keywords": [
        "sentence-transformers",
        "pytorch",
        "jax",
        "onnx",
        "safetensors",
        "openvino",
        "bert",
        "text-classification",
        "transformers",
        "text-ranking",
        "en",
        "dataset:sentence-transformers/msmarco",
        "base_model:cross-encoder/ms-marco-MiniLM-L12-v2",
        "base_model:quantized:cross-encoder/ms-marco-MiniLM-L12-v2",
        "license:apache-2.0",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-classification",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/sentence-transformers/msmarco"
        }
      ],
      "intendedUseCase": "The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/cross_encoder/training/ms_marco)",
      "usageInformation": "The usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)",
      "baseModel": "cross-encoder/ms-marco-MiniLM-L12-v2",
      "contentUrl": "https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/model.safetensors",
      "url": "https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "# Cross-Encoder for MS Marco\n\nThis model was trained on the [MS Marco Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) task.\n\nThe model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See [SBERT.net Retrieve & Re-rank](https://www.sbert.net/examples/applications/retrieve_rerank/README.html) for more details. The training code is available here: [SBERT.net Training MS Marco](https://github.com/UKPLab/sentence-transformers/tree/master/examples/cross_encoder/training/ms_marco)\n\n\n## Usage with SentenceTransformers\n\nThe usage is easy when you have [SentenceTransformers](https://www.sbert.net/) installed. Then you can use the pre-trained models like this:\n```python\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L6-v2')\nscores = model.predict([\n    (\"How many people live in Berlin?\", \"Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.\"),\n    (\"How many people live in Berlin?\", \"Berlin is well known for its museums.\"),\n])\nprint(scores)\n# [ 8.607138 -4.320078]\n```\n\n\n## Usage with Transformers\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained('cross-encoder/ms-marco-MiniLM-L6-v2')\ntokenizer = AutoTokenizer.from_pretrained('cross-encoder/ms-marco-MiniLM-L6-v2')\n\nfeatures = tokenizer(['How many people live in Berlin?', 'How many people live in Berlin?'], ['Berlin has a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.', 'New York City is famous for the Metropolitan Museum of Art.'],  padding=True, truncation=True, return_tensors=\"pt\")\n\nmodel.eval()\nwith torch.no_grad():\n    scores = model(**features).logits\n    print(scores)\n```\n\n\n## Performance\nIn the following table, we provide various pre-trained Cross-Encoders together with their performance on the [TREC Deep Learning 2019](https://microsoft.github.io/TREC-2019-Deep-Learning/) and the [MS Marco Passage Reranking](https://github.com/microsoft/MSMARCO-Passage-Ranking/) dataset. \n\n\n| Model-Name        | NDCG@10 (TREC DL 19) | MRR@10 (MS Marco Dev)  | Docs / Sec |\n| ------------- |:-------------| -----| --- | \n| **Version 2 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2-v2 | 69.84 | 32.56 | 9000\n| cross-encoder/ms-marco-MiniLM-L2-v2 | 71.01 | 34.85 | 4100\n| cross-encoder/ms-marco-MiniLM-L4-v2 | 73.04 | 37.70 | 2500\n| cross-encoder/ms-marco-MiniLM-L6-v2 | 74.30 | 39.01 | 1800\n| cross-encoder/ms-marco-MiniLM-L12-v2 | 74.31 | 39.02 | 960\n| **Version 1 models** | | | \n| cross-encoder/ms-marco-TinyBERT-L2  | 67.43 | 30.15  | 9000\n| cross-encoder/ms-marco-TinyBERT-L4  | 68.09 | 34.50  | 2900\n| cross-encoder/ms-marco-TinyBERT-L6 |  69.57 | 36.13  | 680\n| cross-encoder/ms-marco-electra-base | 71.99 | 36.41 | 340\n| **Other models** | | | \n| nboost/pt-tinybert-msmarco | 63.63 | 28.80 | 2900 \n| nboost/pt-bert-base-uncased-msmarco | 70.94 | 34.75 | 340 \n| nboost/pt-bert-large-msmarco | 73.36 | 36.48 | 100 \n| Capreolus/electra-base-msmarco | 71.23 | 36.89 | 340 \n| amberoad/bert-multilingual-passage-reranking-msmarco | 68.40 | 35.54 | 330 \n| sebastian-hofstaetter/distilbert-cat-margin_mse-T2-msmarco | 72.82 | 37.88 | 720\n \n Note: Runtime was computed on a V100 GPU."
    },
    {
      "@id": "ark:59852/model-usyd-community-vitpose-plus-base-jriwtxdazqj",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "usyd-community/vitpose-plus-base",
      "description": "<img src=\"https://cdn-uploads.huggingface.co/production/uploads/6579e0eaa9e58aec614e9d97/ZuIwMdomy2_6aJ_JTE1Yd.png\" alt=\"x\" width=\"400\"/>",
      "author": "usyd-community",
      "keywords": [
        "transformers",
        "safetensors",
        "vitpose",
        "keypoint-detection",
        "en",
        "arxiv:2204.12484",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "hasBias": "In this paper, we propose a simple yet effective vision transformer baseline for pose estimation,\ni.e., ViTPose. Despite no elaborate designs in structure, ViTPose obtains SOTA performance\non the MS COCO dataset. However, the potential of ViTPose is not fully explored with more\nadvanced technologies, such as complex decoders or FPN structures, which may further improve the\nperformance. Besides, although the ViTPose demonstrates exciting properties such as simplicity,\nscalability, flexibility, and transferability, more research efforts could be made, e.g., exploring the\nprompt-based tuning to demonstrate the flexibility of ViTPose further. In addition, we believe\nViTPose can also be applied to other pose estimation datasets, e.g., animal pose estimation [47, 9, 45]\nand face keypoint detection [21, 6]. We leave them as the future work.",
      "intendedUseCase": "In this paper, we propose a simple yet effective vision transformer baseline for pose estimation,\ni.e., ViTPose. Despite no elaborate designs in structure, ViTPose obtains SOTA performance\non the MS COCO dataset. However, the potential of ViTPose is not fully explored with more\nadvanced technologies, such as complex decoders or FPN structures, which may further improve the\nperformance. Besides, although the ViTPose demonstrates exciting properties such as simplicity,\nscalability, flexibility, and transferability, more research efforts could be made, e.g., exploring the\nprompt-based tuning to demonstrate the flexibility of ViTPose further. In addition, we believe\nViTPose can also be applied to other pose estimation datasets, e.g., animal pose estimation [47, 9, 45]\nand face keypoint detection [21, 6]. We leave them as the future work.",
      "contentUrl": "https://huggingface.co/usyd-community/vitpose-plus-base/resolve/main/model.safetensors",
      "url": "https://huggingface.co/usyd-community/vitpose-plus-base",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Model Card for VitPose\n\n<img src=\"https://cdn-uploads.huggingface.co/production/uploads/6579e0eaa9e58aec614e9d97/ZuIwMdomy2_6aJ_JTE1Yd.png\" alt=\"x\" width=\"400\"/>\n\nViTPose: Simple Vision Transformer Baselines for Human Pose Estimation and ViTPose+: Vision Transformer Foundation Model for Generic Body Pose Estimation. It obtains 81.1 AP on MS COCO Keypoint test-dev set.\n\n## Model Details\n\nAlthough no specific domain knowledge is considered in the design, plain vision transformers have shown excellent performance in visual recognition tasks. However, little effort has been made to reveal the potential of such simple structures for\npose estimation tasks. In this paper, we show the surprisingly good capabilities of plain vision transformers for pose estimation from various aspects, namely simplicity in model structure, scalability in model size, flexibility in training paradigm,\nand transferability of knowledge between models, through a simple baseline model called ViTPose. Specifically, ViTPose employs plain and non-hierarchical vision\ntransformers as backbones to extract features for a given person instance and a\nlightweight decoder for pose estimation. It can be scaled up from 100M to 1B\nparameters by taking the advantages of the scalable model capacity and high\nparallelism of transformers, setting a new Pareto front between throughput and performance. Besides, ViTPose is very flexible regarding the attention type, input resolution, pre-training and finetuning strategy, as well as dealing with multiple pose\ntasks. We also empirically demonstrate that the knowledge of large ViTPose models\ncan be easily transferred to small ones via a simple knowledge token. Experimental\nresults show that our basic ViTPose model outperforms representative methods\non the challenging MS COCO Keypoint Detection benchmark, while the largest\nmodel sets a new state-of-the-art, i.e., 80.9 AP on the MS COCO test-dev set. The\ncode and models are available at https://github.com/ViTAE-Transformer/ViTPose\n\n### Model Description\n\nThis is the model card of a \ud83e\udd17 transformers model that has been pushed on the Hub. This model card has been automatically generated.\n\n- **Developed by:** Yufei Xu, Jing Zhang, Qiming Zhang, Dacheng Tao\n- **Funded by:** ARC FL-170100117 and IH-180100002.\n- **License:** Apache-2.0\n- **Ported to \ud83e\udd17 Transformers by:** Sangbum Choi and Niels Rogge\n\n### Model Sources\n\n- **Original repository:** https://github.com/ViTAE-Transformer/ViTPose\n- **Paper:** https://arxiv.org/pdf/2204.12484\n- **Demo:** https://huggingface.co/spaces?sort=trending&search=vitpose\n\n## Uses\n\nThe ViTPose model, developed by the ViTAE-Transformer team, is primarily designed for pose estimation tasks. Here are some direct uses of the model:\n\nHuman Pose Estimation: The model can be used to estimate the poses of humans in images or videos. This involves identifying the locations of key body joints such as the head, shoulders, elbows, wrists, hips, knees, and ankles.\n\nAction Recognition: By analyzing the poses over time, the model can help in recognizing various human actions and activities.\n\nSurveillance: In security and surveillance applications, ViTPose can be used to monitor and analyze human behavior in public spaces or private premises.\n\nHealth and Fitness: The model can be utilized in fitness apps to track and analyze exercise poses, providing feedback on form and technique.\n\nGaming and Animation: ViTPose can be integrated into gaming and animation systems to create more realistic character movements and interactions.\n\n\n## Bias, Risks, and Limitations\n\nIn this paper, we propose a simple yet effective vision transformer baseline for pose estimation,\ni.e., ViTPose. Despite no elaborate designs in structure, ViTPose obtains SOTA performance\non the MS COCO dataset. However, the potential of ViTPose is not fully explored with more\nadvanced technologies, such as complex decoders or FPN structures, which may further improve the\nperformance. Besides, although the ViTPose demonstrates exciting properties such as simplicity,\nscalability, flexibility, and transferability, more research efforts could be made, e.g., exploring the\nprompt-based tuning to demonstrate the flexibility of ViTPose further. In addition, we believe\nViTPose can also be applied to other pose estimation datasets, e.g., animal pose estimation [47, 9, 45]\nand face keypoint detection [21, 6]. We leave them as the future work.\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n```python\nimport torch\nimport requests\nimport numpy as np\n\nfrom PIL import Image\n\nfrom transformers import (\n    AutoProcessor,\n    RTDetrForObjectDetection,\n    VitPoseForPoseEstimation,\n)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nurl = \"http://images.cocodataset.org/val2017/000000000139.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# ------------------------------------------------------------------------\n# Stage 1. Detect humans on the image\n# ------------------------------------------------------------------------\n\n# You can choose detector by your choice\nperson_image_processor = AutoProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\nperson_model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\", device_map=device)\n\ninputs = person_image_processor(images=image, return_tensors=\"pt\").to(device)\n\nwith torch.no_grad():\n    outputs = person_model(**inputs)\n\nresults = person_image_processor.post_process_object_detection(\n    outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.3\n)\nresult = results[0]  # take first image results\n\n# Human label refers 0 index in COCO dataset\nperson_boxes = result[\"boxes\"][result[\"labels\"] == 0]\nperson_boxes = person_boxes.cpu().numpy()\n\n# Convert boxes from VOC (x1, y1, x2, y2) to COCO (x1, y1, w, h) format\nperson_boxes[:, 2] = person_boxes[:, 2] - person_boxes[:, 0]\nperson_boxes[:, 3] = person_boxes[:, 3] - person_boxes[:, 1]\n\n# ------------------------------------------------------------------------\n# Stage 2. Detect keypoints for each person found\n# ------------------------------------------------------------------------\n\nimage_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-plus-base\")\nmodel = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-plus-base\", device_map=device)\n\ninputs = image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\n\n# This is MOE architecture, we should specify dataset indexes for each image in range 0..5\ninputs[\"dataset_index\"] = torch.tensor([0], device=device)\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\npose_results = image_processor.post_process_pose_estimation(outputs, boxes=[person_boxes], threshold=0.3)\nimage_pose_result = pose_results[0]  # results for first image\n\nfor i, person_pose in enumerate(image_pose_result):\n    print(f\"Person #{i}\")\n    for keypoint, label, score in zip(\n        person_pose[\"keypoints\"], person_pose[\"labels\"], person_pose[\"scores\"]\n    ):\n        keypoint_name = model.config.id2label[label.item()]\n        x, y = keypoint\n        print(f\" - {keypoint_name}: x={x.item():.2f}, y={y.item():.2f}, score={score.item():.2f}\")\n\n```\n\nOutput:\n```\nPerson #0\n - Nose: x=428.81, y=171.53, score=0.92\n - L_Eye: x=429.32, y=168.30, score=0.92\n - R_Eye: x=428.84, y=168.47, score=0.82\n - L_Ear: x=434.60, y=166.54, score=0.90\n - R_Ear: x=440.14, y=165.80, score=0.80\n - L_Shoulder: x=440.74, y=176.95, score=0.96\n - R_Shoulder: x=444.06, y=177.52, score=0.68\n - L_Elbow: x=436.30, y=197.08, score=0.91\n - R_Elbow: x=432.29, y=201.22, score=0.79\n - L_Wrist: x=429.91, y=217.90, score=0.84\n - R_Wrist: x=421.08, y=212.72, score=0.90\n - L_Hip: x=446.15, y=223.88, score=0.74\n - R_Hip: x=449.32, y=223.45, score=0.65\n - L_Knee: x=443.73, y=255.72, score=0.76\n - R_Knee: x=450.72, y=255.21, score=0.73\n - L_Ankle: x=452.14, y=287.30, score=0.66\n - R_Ankle: x=456.02, y=285.99, score=0.72\nPerson #1\n - Nose: x=398.22, y=181.60, score=0.88\n - L_Eye: x=398.67, y=179.84, score=0.87\n - R_Eye: x=396.07, y=179.44, score=0.87\n - R_Ear: x=388.94, y=180.38, score=0.87\n - L_Shoulder: x=397.11, y=194.19, score=0.71\n - R_Shoulder: x=384.75, y=190.74, score=0.55\n```\n\n## Training Details\n\n### Training Data\n\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\nDataset details. We use MS COCO [28], AI Challenger [41], MPII [3], and CrowdPose [22] datasets\nfor training and evaluation. OCHuman [54] dataset is only involved in the evaluation stage to measure\nthe models\u2019 performance in dealing with occluded people. The MS COCO dataset contains 118K\nimages and 150K human instances with at most 17 keypoint annotations each instance for training.\nThe dataset is under the CC-BY-4.0 license. MPII dataset is under the BSD license and contains\n15K images and 22K human instances for training. There are at most 16 human keypoints for each\ninstance annotated in this dataset. AI Challenger is much bigger and contains over 200K training\nimages and 350 human instances, with at most 14 keypoints for each instance annotated. OCHuman\ncontains human instances with heavy occlusion and is just used for val and test set, which includes\n4K images and 8K instances.\n\n\n#### Training Hyperparameters\n\n- **Training regime:** ![image/png](https://cdn-uploads.huggingface.co/production/uploads/6579e0eaa9e58aec614e9d97/Gj6gGcIGO3J5HD2MAB_4C.png)\n\n#### Speeds, Sizes, Times\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6579e0eaa9e58aec614e9d97/rsCmn48SAvhi8xwJhX8h5.png)\n\n## Evaluation\n\nOCHuman val and test set. To evaluate the performance of human pose estimation models on the\nhuman instances with heavy occlusion, we test the ViTPose variants and representative models on\nthe OCHuman val and test set with ground truth bounding boxes. We do not adopt extra human\ndetectors since not all human instances are annotated in the OCHuman datasets, where the human\ndetector will cause a lot of \u201cfalse positive\u201d bounding boxes and can not reflect the true ability of\npose estimation models. Specifically, the decoder head of ViTPose corresponding to the MS COCO\ndataset is used, as the keypoint definitions are the same in MS COCO and OCHuman datasets.\n\nMPII val set. We evaluate the performance of ViTPose and representative models on the MPII val\nset with the ground truth bounding boxes. Following the default settings of MPII, we use PCKh\nas metric for performance evaluation.\n\n### Results\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6579e0eaa9e58aec614e9d97/FcHVFdUmCuT2m0wzB8QSS.png)\n\n\n### Model Architecture and Objective\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6579e0eaa9e58aec614e9d97/kf3e1ifJkVtOMbISvmMsM.png)\n\n#### Hardware\n\nThe models are trained on 8 A100 GPUs based on the mmpose codebase\n\n\n## Citation\n\n**BibTeX:**\n\n```bibtex\n@article{xu2022vitposesimplevisiontransformer,\n  title={ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation},\n  author={Yufei Xu and Jing Zhang and Qiming Zhang and Dacheng Tao},\n  year={2022},\n  eprint={2204.12484},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV},\n  url={https://arxiv.org/abs/2204.12484}\n}\n```"
    },
    {
      "@id": "ark:59852/model-openai-gpt-oss-20b-1otyncx7no",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "openai/gpt-oss-20b",
      "description": "<p align=\"center\">",
      "author": "openai",
      "keywords": [
        "transformers",
        "safetensors",
        "gpt_oss",
        "text-generation",
        "vllm",
        "conversational",
        "arxiv:2508.10925",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "8-bit",
        "mxfp4",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-generation",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "intendedUseCase": "Welcome to the gpt-oss series, [OpenAI\u2019s open-weight models](https://openai.com/open-models) designed for powerful reasoning, agentic tasks, and versatile developer use cases.",
      "usageInformation": "## Transformers\n\nYou can use `gpt-oss-120b` and `gpt-oss-20b` with Transformers. If you use the Transformers chat template, it will automatically apply the [harmony response format](https://github.com/openai/harmony). If you use `model.generate` directly, you need to apply the harmony format manually using the chat template or use our [openai-harmony](https://github.com/openai/harmony) package.\n\nTo get started, install the necessary dependencies to setup your environment:\n\n```\npip install -U transformers kernels torch \n```\n\nOnce, setup you can proceed to run the model by running the snippet below:\n\n```py\nfrom transformers import pipeline\nimport torch\n\nmodel_id = \"openai/gpt-oss-20b\"\n\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n]\n\noutputs = pipe(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nAlternatively, you can run the model via [`Transformers Serve`](https://huggingface.co/docs/transformers/main/serving) to spin up a OpenAI-compatible webserver:\n\n```\ntransformers serve\ntransformers chat localhost:8000 --model-name-or-path openai/gpt-oss-20b\n```\n\n[Learn more about how to use gpt-oss with Transformers.](https://cookbook.openai.com/articles/gpt-oss/run-transformers)\n\n## vLLM\n\nvLLM recommends using [uv](https://docs.astral.sh/uv/) for Python dependency management. You can use vLLM to spin up an OpenAI-compatible webserver. The following command will automatically download the model and start the server.\n\n```bash\nuv pip install --pre vllm==0.10.1+gptoss \\\n    --extra-index-url https://wheels.vllm.ai/gpt-oss/ \\\n    --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\\n    --index-strategy unsafe-best-match\n\nvllm serve openai/gpt-oss-20b\n```\n\n[Learn more about how to use gpt-oss with vLLM.](https://cookbook.openai.com/articles/gpt-oss/run-vllm)\n\n## PyTorch / Triton\n\nTo learn about how to use this model with PyTorch and Triton, check out our [reference implementations in the gpt-oss repository](https://github.com/openai/gpt-oss?tab=readme-ov-file#reference-pytorch-implementation).\n\n## Ollama\n\nIf you are trying to run gpt-oss on consumer hardware, you can use Ollama by running the following commands after [installing Ollama](https://ollama.com/download).\n\n```bash",
      "contentUrl": "https://huggingface.co/openai/gpt-oss-20b/resolve/main/model-00000-of-00002.safetensors",
      "url": "https://huggingface.co/openai/gpt-oss-20b",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n<p align=\"center\">\n  <img alt=\"gpt-oss-20b\" src=\"https://raw.githubusercontent.com/openai/gpt-oss/main/docs/gpt-oss-20b.svg\">\n</p>\n\n<p align=\"center\">\n  <a href=\"https://gpt-oss.com\"><strong>Try gpt-oss</strong></a> \u00b7\n  <a href=\"https://cookbook.openai.com/topic/gpt-oss\"><strong>Guides</strong></a> \u00b7\n  <a href=\"https://arxiv.org/abs/2508.10925\"><strong>Model card</strong></a> \u00b7\n  <a href=\"https://openai.com/index/introducing-gpt-oss/\"><strong>OpenAI blog</strong></a>\n</p>\n\n<br>\n\nWelcome to the gpt-oss series, [OpenAI\u2019s open-weight models](https://openai.com/open-models) designed for powerful reasoning, agentic tasks, and versatile developer use cases.\n\nWe\u2019re releasing two flavors of these open models:\n- `gpt-oss-120b` \u2014 for production, general purpose, high reasoning use cases that fit into a single 80GB GPU (like NVIDIA H100 or AMD MI300X) (117B parameters with 5.1B active parameters)\n- `gpt-oss-20b` \u2014 for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)\n\nBoth models were trained on our [harmony response format](https://github.com/openai/harmony) and should only be used with the harmony format as it will not work correctly otherwise.\n\n\n> [!NOTE]\n> This model card is dedicated to the smaller `gpt-oss-20b` model. Check out [`gpt-oss-120b`](https://huggingface.co/openai/gpt-oss-120b) for the larger model.\n\n# Highlights\n\n* **Permissive Apache 2.0 license:** Build freely without copyleft restrictions or patent risk\u2014ideal for experimentation, customization, and commercial deployment.  \n* **Configurable reasoning effort:** Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.  \n* **Full chain-of-thought:** Gain complete access to the model\u2019s reasoning process, facilitating easier debugging and increased trust in outputs. It\u2019s not intended to be shown to end users.  \n* **Fine-tunable:** Fully customize models to your specific use case through parameter fine-tuning.\n* **Agentic capabilities:** Use the models\u2019 native capabilities for function calling, [web browsing](https://github.com/openai/gpt-oss/tree/main?tab=readme-ov-file#browser), [Python code execution](https://github.com/openai/gpt-oss/tree/main?tab=readme-ov-file#python), and Structured Outputs.\n* **MXFP4 quantization:** The models were post-trained with MXFP4 quantization of the MoE weights, making `gpt-oss-120b` run on a single 80GB GPU (like NVIDIA H100 or AMD MI300X) and the `gpt-oss-20b` model run within 16GB of memory. All evals were performed with the same MXFP4 quantization.\n\n---\n\n# Inference examples\n\n## Transformers\n\nYou can use `gpt-oss-120b` and `gpt-oss-20b` with Transformers. If you use the Transformers chat template, it will automatically apply the [harmony response format](https://github.com/openai/harmony). If you use `model.generate` directly, you need to apply the harmony format manually using the chat template or use our [openai-harmony](https://github.com/openai/harmony) package.\n\nTo get started, install the necessary dependencies to setup your environment:\n\n```\npip install -U transformers kernels torch \n```\n\nOnce, setup you can proceed to run the model by running the snippet below:\n\n```py\nfrom transformers import pipeline\nimport torch\n\nmodel_id = \"openai/gpt-oss-20b\"\n\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n]\n\noutputs = pipe(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nAlternatively, you can run the model via [`Transformers Serve`](https://huggingface.co/docs/transformers/main/serving) to spin up a OpenAI-compatible webserver:\n\n```\ntransformers serve\ntransformers chat localhost:8000 --model-name-or-path openai/gpt-oss-20b\n```\n\n[Learn more about how to use gpt-oss with Transformers.](https://cookbook.openai.com/articles/gpt-oss/run-transformers)\n\n## vLLM\n\nvLLM recommends using [uv](https://docs.astral.sh/uv/) for Python dependency management. You can use vLLM to spin up an OpenAI-compatible webserver. The following command will automatically download the model and start the server.\n\n```bash\nuv pip install --pre vllm==0.10.1+gptoss \\\n    --extra-index-url https://wheels.vllm.ai/gpt-oss/ \\\n    --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\\n    --index-strategy unsafe-best-match\n\nvllm serve openai/gpt-oss-20b\n```\n\n[Learn more about how to use gpt-oss with vLLM.](https://cookbook.openai.com/articles/gpt-oss/run-vllm)\n\n## PyTorch / Triton\n\nTo learn about how to use this model with PyTorch and Triton, check out our [reference implementations in the gpt-oss repository](https://github.com/openai/gpt-oss?tab=readme-ov-file#reference-pytorch-implementation).\n\n## Ollama\n\nIf you are trying to run gpt-oss on consumer hardware, you can use Ollama by running the following commands after [installing Ollama](https://ollama.com/download).\n\n```bash\n# gpt-oss-20b\nollama pull gpt-oss:20b\nollama run gpt-oss:20b\n```\n\n[Learn more about how to use gpt-oss with Ollama.](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama)\n\n#### LM Studio\n\nIf you are using [LM Studio](https://lmstudio.ai/) you can use the following commands to download.\n\n```bash\n# gpt-oss-20b\nlms get openai/gpt-oss-20b\n```\n\nCheck out our [awesome list](https://github.com/openai/gpt-oss/blob/main/awesome-gpt-oss.md) for a broader collection of gpt-oss resources and inference partners.\n\n---\n\n# Download the model\n\nYou can download the model weights from the [Hugging Face Hub](https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4) directly from Hugging Face CLI:\n\n```shell\n# gpt-oss-20b\nhuggingface-cli download openai/gpt-oss-20b --include \"original/*\" --local-dir gpt-oss-20b/\npip install gpt-oss\npython -m gpt_oss.chat model/\n```\n\n# Reasoning levels\n\nYou can adjust the reasoning level that suits your task across three levels:\n\n* **Low:** Fast responses for general dialogue.  \n* **Medium:** Balanced speed and detail.  \n* **High:** Deep and detailed analysis.\n\nThe reasoning level can be set in the system prompts, e.g., \"Reasoning: high\".\n\n# Tool use\n\nThe gpt-oss models are excellent for:\n* Web browsing (using built-in browsing tools)\n* Function calling with defined schemas\n* Agentic operations like browser tasks\n\n# Fine-tuning\n\nBoth gpt-oss models can be fine-tuned for a variety of specialized use cases.\n\nThis smaller model `gpt-oss-20b` can be fine-tuned on consumer hardware, whereas the larger [`gpt-oss-120b`](https://huggingface.co/openai/gpt-oss-120b) can be fine-tuned on a single H100 node.\n\n# Citation\n\n```bibtex\n@misc{openai2025gptoss120bgptoss20bmodel,\n      title={gpt-oss-120b & gpt-oss-20b Model Card}, \n      author={OpenAI},\n      year={2025},\n      eprint={2508.10925},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2508.10925}, \n}\n```"
    },
    {
      "@id": "ark:59852/model-kijai-wanvideocomfy-1otynci7svy",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "Kijai/WanVideo_comfy",
      "description": "Combined and quantized models for WanVideo, originating from here:",
      "author": "Kijai",
      "keywords": [
        "diffusion-single-file",
        "comfyui",
        "base_model:Wan-AI/Wan2.1-VACE-1.3B",
        "base_model:finetune:Wan-AI/Wan2.1-VACE-1.3B",
        "region:us"
      ],
      "version": "1.0",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "usageInformation": "---\nCausVid LoRAs are experimental extractions from the CausVid finetunes, the aim with them is to benefit from the distillation in CausVid, rather than any actual causal inference.\n---\nv1 = direct extraction, has adverse effects on motion and introduces flashing artifact at full strength.",
      "baseModel": "Wan-AI/Wan2.1-VACE-14B'...an-AI/Wan2.1-VACE-1.3B",
      "contentUrl": "https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Bindweave/Wan2_1-I2V-14B-Bindweave_fp16.safetensors",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy",
      "isPartOf": [],
      "README": "Combined and quantized models for WanVideo, originating from here:\n\nhttps://huggingface.co/Wan-AI/\n\nCan be used with: https://github.com/kijai/ComfyUI-WanVideoWrapper and ComfyUI native WanVideo nodes.\n\nI've also started to do fp8_scaled versions over here: https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled\n\nOther model sources:\n\nTinyVAE from https://github.com/madebyollin/taehv\n\nSkyReels: https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9\n\nWanVideoFun: https://huggingface.co/collections/alibaba-pai/wan21-fun-v11-680f514c89fe7b4df9d44f17\n\n---\n\nLightx2v:\n\nCausVid 14B: https://huggingface.co/lightx2v/Wan2.1-T2V-14B-CausVid\n\nCFG and Step distill 14B: https://huggingface.co/lightx2v/Wan2.1-T2V-14B-StepDistill-CfgDistill\n\n---\n\nCausVid 1.3B: https://huggingface.co/tianweiy/CausVid\n\nAccVideo: https://huggingface.co/aejion/AccVideo-WanX-T2V-14B\n\nPhantom: https://huggingface.co/bytedance-research/Phantom\n\nATI: https://huggingface.co/bytedance-research/ATI\n\nMiniMaxRemover: https://huggingface.co/zibojia/minimax-remover\n\nMAGREF: https://huggingface.co/MAGREF-Video/MAGREF\n\nFantasyTalking: https://github.com/Fantasy-AMAP/fantasy-talking\n\nMultiTalk: https://github.com/MeiGen-AI/MultiTalk\n\nAnisora: https://huggingface.co/IndexTeam/Index-anisora/tree/main/14B\n\nPusa: https://huggingface.co/RaphaelLiu/PusaV1/tree/main\n\nFastVideo: https://huggingface.co/FastVideo\n\nEchoShot: https://github.com/D2I-ai/EchoShot\n\nWan22 5B Turbo: https://huggingface.co/quanhaol/Wan2.2-TI2V-5B-Turbo\n\nOvi: https://github.com/character-ai/Ovi\n\nFlashVSR: https://huggingface.co/JunhaoZhuang/FlashVSR\n\nrCM: https://huggingface.co/worstcoder/rcm-Wan/tree/main\n\n---\nCausVid LoRAs are experimental extractions from the CausVid finetunes, the aim with them is to benefit from the distillation in CausVid, rather than any actual causal inference.\n---\nv1 = direct extraction, has adverse effects on motion and introduces flashing artifact at full strength.\n\nv1.5 = same as above, but without the first block which fixes the flashing at full strength.\n\nv2 = further pruned version with only attention layers and no first block, fixes flashing and retains motion better, needs more steps and can also benefit from cfg."
    },
    {
      "@id": "ark:59852/model-distilbert-distilbert-base-uncased-finetuned-sst-2-english-jflnukgwzr",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "distilbert/distilbert-base-uncased-finetuned-sst-2-english",
      "description": "- [Model Details](#model-details)",
      "author": "distilbert",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "rust",
        "onnx",
        "safetensors",
        "distilbert",
        "text-classification",
        "en",
        "dataset:sst2",
        "dataset:glue",
        "arxiv:1910.01108",
        "doi:10.57967/hf/0181",
        "license:apache-2.0",
        "model-index",
        "autotrain_compatible",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-classification",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/sst2"
        },
        {
          "@id": "https://huggingface.co/datasets/glue"
        }
      ],
      "hasBias": "Based on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations.\n\nFor instance, for sentences like `This film was filmed in COUNTRY`, this binary classification model will give radically different probabilities for the positive label depending on the country (0.89 if the country is France, but 0.08 if the country is Afghanistan) when nothing in the input indicates such a strong semantic shift. In this [colab](https://colab.research.google.com/gist/ageron/fb2f64fb145b4bc7c49efc97e5f114d3/biasmap.ipynb), [Aur\u00e9lien G\u00e9ron](https://twitter.com/aureliengeron) made an interesting map plotting these probabilities for each country.\n\n<img src=\"https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/map.jpeg\" alt=\"Map of positive probabilities per country.\" width=\"500\"/>\n\nWe strongly advise users to thoroughly probe these aspects on their use-cases in order to evaluate the risks of this model. We recommend looking at the following bias evaluation datasets as a place to start: [WinoBias](https://huggingface.co/datasets/wino_bias), [WinoGender](https://huggingface.co/datasets/super_glue), [Stereoset](https://huggingface.co/datasets/stereoset).",
      "intendedUseCase": "Based on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations.\n\nFor instance, for sentences like `This film was filmed in COUNTRY`, this binary classification model will give radically different probabilities for the positive label depending on the country (0.89 if the country is France, but 0.08 if the country is Afghanistan) when nothing in the input indicates such a strong semantic shift. In this [colab](https://colab.research.google.com/gist/ageron/fb2f64fb145b4bc7c49efc97e5f114d3/biasmap.ipynb), [Aur\u00e9lien G\u00e9ron](https://twitter.com/aureliengeron) made an interesting map plotting these probabilities for each country.\n\n<img src=\"https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/map.jpeg\" alt=\"Map of positive probabilities per country.\" width=\"500\"/>\n\nWe strongly advise users to thoroughly probe these aspects on their use-cases in order to evaluate the risks of this model. We recommend looking at the following bias evaluation datasets as a place to start: [WinoBias](https://huggingface.co/datasets/wino_bias), [WinoGender](https://huggingface.co/datasets/super_glue), [Stereoset](https://huggingface.co/datasets/stereoset).",
      "usageInformation": "This model can be used for  topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.",
      "contentUrl": "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/model.safetensors",
      "url": "https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# DistilBERT base uncased finetuned SST-2\n\n## Table of Contents\n- [Model Details](#model-details)\n- [How to Get Started With the Model](#how-to-get-started-with-the-model)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n\n## Model Details\n**Model Description:** This model is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased), fine-tuned on SST-2.\nThis model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).\n- **Developed by:** Hugging Face\n- **Model Type:** Text Classification\n- **Language(s):** English\n- **License:** Apache-2.0\n- **Parent Model:** For more details about DistilBERT, we encourage users to check out [this model card](https://huggingface.co/distilbert-base-uncased).\n- **Resources for more information:**\n    - [Model Documentation](https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification)\n    - [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n\n## How to Get Started With the Model\n\nExample of single-label classification:\n\u200b\u200b\n```python\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\nmodel = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]\n\n```\n\n## Uses\n\n#### Direct Use\n\nThis model can be used for  topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\n\n#### Misuse and Out-of-scope Use\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n\n## Risks, Limitations and Biases\n\nBased on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations.\n\nFor instance, for sentences like `This film was filmed in COUNTRY`, this binary classification model will give radically different probabilities for the positive label depending on the country (0.89 if the country is France, but 0.08 if the country is Afghanistan) when nothing in the input indicates such a strong semantic shift. In this [colab](https://colab.research.google.com/gist/ageron/fb2f64fb145b4bc7c49efc97e5f114d3/biasmap.ipynb), [Aur\u00e9lien G\u00e9ron](https://twitter.com/aureliengeron) made an interesting map plotting these probabilities for each country.\n\n<img src=\"https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/map.jpeg\" alt=\"Map of positive probabilities per country.\" width=\"500\"/>\n\nWe strongly advise users to thoroughly probe these aspects on their use-cases in order to evaluate the risks of this model. We recommend looking at the following bias evaluation datasets as a place to start: [WinoBias](https://huggingface.co/datasets/wino_bias), [WinoGender](https://huggingface.co/datasets/super_glue), [Stereoset](https://huggingface.co/datasets/stereoset).\n\n\n\n# Training\n\n\n#### Training Data\n\n\nThe authors use the following Stanford Sentiment Treebank([sst2](https://huggingface.co/datasets/sst2)) corpora for the model.\n\n#### Training Procedure\n\n###### Fine-tuning hyper-parameters\n\n\n- learning_rate = 1e-5\n- batch_size = 32\n- warmup = 600\n- max_seq_length = 128\n- num_train_epochs = 3.0\n\n\n"
    },
    {
      "@id": "ark:59852/model-sentence-transformers-paraphrase-multilingual-mpnet-base-v2-n2hexw5z1s",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
      "description": "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
      "author": "sentence-transformers",
      "keywords": [
        "sentence-transformers",
        "pytorch",
        "tf",
        "onnx",
        "safetensors",
        "openvino",
        "xlm-roberta",
        "feature-extraction",
        "sentence-similarity",
        "transformers",
        "text-embeddings-inference",
        "multilingual",
        "ar",
        "bg",
        "ca",
        "cs",
        "da",
        "de",
        "el",
        "en",
        "es",
        "et",
        "fa",
        "fi",
        "fr",
        "gl",
        "gu",
        "he",
        "hi",
        "hr",
        "hu",
        "hy",
        "id",
        "it",
        "ja",
        "ka",
        "ko",
        "ku",
        "lt",
        "lv",
        "mk",
        "mn",
        "mr",
        "ms",
        "my",
        "nb",
        "nl",
        "pl",
        "pt",
        "ro",
        "ru",
        "sk",
        "sl",
        "sq",
        "sr",
        "sv",
        "th",
        "tr",
        "uk",
        "ur",
        "vi",
        "arxiv:1908.10084",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "usageInformation": "Using this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch",
      "contentUrl": "https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2/resolve/main/model.safetensors",
      "url": "https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, mean pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n\n## Usage (Text Embeddings Inference (TEI))\n\n[Text Embeddings Inference (TEI)](https://github.com/huggingface/text-embeddings-inference) is a blazing fast inference solution for text embedding models.\n\n- CPU:\n```bash\ndocker run -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cpu-latest --model-id sentence-transformers/paraphrase-multilingual-mpnet-base-v2 --pooling mean --dtype float16\n```\n\n- NVIDIA GPU:\n```bash\ndocker run --gpus all -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cuda-latest --model-id sentence-transformers/paraphrase-multilingual-mpnet-base-v2 --pooling mean --dtype float16\n```\n\nSend a request to `/v1/embeddings` to generate embeddings via the [OpenAI Embeddings API](https://platform.openai.com/docs/api-reference/embeddings/create):\n```bash\ncurl http://localhost:8080/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n    \"input\": \"This is an example sentence\"\n  }'\n```\n\nOr check the [Text Embeddings Inference API specification](https://huggingface.github.io/text-embeddings-inference/) instead.\n\n\n\n## Full Model Architecture\n```\nSentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\n```\n\n## Citing & Authors\n\nThis model was trained by [sentence-transformers](https://www.sbert.net/). \n        \nIf you find this model helpful, feel free to cite our publication [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084):\n```bibtex \n@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n```"
    },
    {
      "@id": "ark:59852/model-jonatasgrosman-wav2vec2-large-xlsr-53-portuguese-4tcbwvuopvu",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "jonatasgrosman/wav2vec2-large-xlsr-53-portuguese",
      "description": "Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Portuguese using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice).",
      "author": "jonatasgrosman",
      "keywords": [
        "transformers",
        "pytorch",
        "jax",
        "wav2vec2",
        "automatic-speech-recognition",
        "audio",
        "hf-asr-leaderboard",
        "mozilla-foundation/common_voice_6_0",
        "pt",
        "robust-speech-event",
        "speech",
        "xlsr-fine-tuning-week",
        "dataset:common_voice",
        "dataset:mozilla-foundation/common_voice_6_0",
        "doi:10.57967/hf/3572",
        "license:apache-2.0",
        "model-index",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/common_voice"
        },
        {
          "@id": "https://huggingface.co/datasets/mozilla-foundation/common_voice_6_0"
        }
      ],
      "usageInformation": "The model can be used directly (without a language model) as follows...\n\nUsing the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-portuguese\")\naudio_paths = [\"/path/to/file.mp3\", \"/path/to/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"pt\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-portuguese\"\nSAMPLES = 10\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)",
      "contentUrl": "https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-portuguese/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-portuguese",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Fine-tuned XLSR-53 large model for speech recognition in Portuguese\n\nFine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Portuguese using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice).\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\nThis model has been fine-tuned thanks to the GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/) :)\n\nThe script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint\n\n## Usage\n\nThe model can be used directly (without a language model) as follows...\n\nUsing the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-portuguese\")\naudio_paths = [\"/path/to/file.mp3\", \"/path/to/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"pt\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-portuguese\"\nSAMPLES = 10\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n    batch[\"speech\"] = speech_array\n    batch[\"sentence\"] = batch[\"sentence\"].upper()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\npredicted_sentences = processor.batch_decode(predicted_ids)\n\nfor i, predicted_sentence in enumerate(predicted_sentences):\n    print(\"-\" * 100)\n    print(\"Reference:\", test_dataset[i][\"sentence\"])\n    print(\"Prediction:\", predicted_sentence)\n```\n\n| Reference  | Prediction |\n| ------------- | ------------- |\n| NEM O RADAR NEM OS OUTROS INSTRUMENTOS DETECTARAM O BOMBARDEIRO STEALTH. | NEMHUM VADAN OS  OLTWES INSTRUMENTOS DE TT\u00c9\u00c3N UM BOMBERDEIRO OSTER |\n| PEDIR DINHEIRO EMPRESTADO \u00c0S PESSOAS DA ALDEIA | E DIR ENGINHEIRO EMPRESTAR AS PESSOAS DA ALDEIA |\n| OITO | OITO |\n| TRANC\u00c1-LOS | TRANCAUVOS |\n| REALIZAR UMA INVESTIGA\u00c7\u00c3O PARA RESOLVER O PROBLEMA | REALIZAR UMA INVESTIGA\u00c7\u00c3O PARA RESOLVER O PROBLEMA |\n| O YOUTUBE AINDA \u00c9 A MELHOR PLATAFORMA DE V\u00cdDEOS. | YOUTUBE AINDA \u00c9 A MELHOR PLATAFOMA DE V\u00cdDEOS |\n| MENINA E MENINO BEIJANDO NAS SOMBRAS | MENINA E MENINO BEIJANDO NAS SOMBRAS |\n| EU SOU O SENHOR | EU SOU O SENHOR |\n| DUAS MULHERES QUE SENTAM-SE PARA BAIXO LENDO JORNAIS. | DUAS MIERES QUE SENTAM-SE PARA BAICLANE JODN\u00d3I |\n| EU ORIGINALMENTE ESPERAVA | EU ORIGINALMENTE ESPERAVA |\n\n## Evaluation\n\n1. To evaluate on `mozilla-foundation/common_voice_6_0` with split `test`\n\n```bash\npython eval.py --model_id jonatasgrosman/wav2vec2-large-xlsr-53-portuguese --dataset mozilla-foundation/common_voice_6_0 --config pt --split test\n```\n\n2. To evaluate on `speech-recognition-community-v2/dev_data`\n\n```bash\npython eval.py --model_id jonatasgrosman/wav2vec2-large-xlsr-53-portuguese --dataset speech-recognition-community-v2/dev_data --config pt --split validation --chunk_length_s 5.0 --stride_length_s 1.0\n```\n\n## Citation\nIf you want to cite this model you can use this:\n\n```bibtex\n@misc{grosman2021xlsr53-large-portuguese,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {P}ortuguese},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-portuguese}},\n  year={2021}\n}\n```"
    },
    {
      "@id": "ark:59852/model-comfy-org-wan2-2comfyuirepackaged-acvf1ohtlae",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "Comfy-Org/Wan_2.2_ComfyUI_Repackaged",
      "description": "Examples: https://comfyanonymous.github.io/ComfyUI_examples/wan22/",
      "author": "Comfy-Org",
      "keywords": [
        "diffusion-single-file",
        "comfyui",
        "region:us"
      ],
      "version": "1.0",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "contentUrl": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/audio_encoders/wav2vec2_large_english_fp16.safetensors",
      "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged",
      "isPartOf": [],
      "README": "\nExamples: https://comfyanonymous.github.io/ComfyUI_examples/wan22/"
    },
    {
      "@id": "ark:59852/model-qwen-qwen2-5-3b-instruct-jflnukbwlhy",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "Qwen/Qwen2.5-3B-Instruct",
      "description": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:",
      "author": "Qwen",
      "keywords": [
        "transformers",
        "safetensors",
        "qwen2",
        "text-generation",
        "chat",
        "conversational",
        "en",
        "arxiv:2407.10671",
        "base_model:Qwen/Qwen2.5-3B",
        "base_model:finetune:Qwen/Qwen2.5-3B",
        "license:other",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-generation",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "hasBias": "**This repo contains the instruction-tuned 3B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\n- Number of Parameters: 3.09B\n- Number of Paramaters (Non-Embedding): 2.77B\n- Number of Layers: 36\n- Number of Attention Heads (GQA): 16 for Q and 2 for KV\n- Context Length: Full 32,768 tokens and generation 8192 tokens",
      "baseModel": "Qwen/Qwen2.5-3B",
      "contentUrl": "https://huggingface.co/Qwen/Qwen2.5-3B-Instruct/resolve/main/model-00001-of-00002.safetensors",
      "url": "https://huggingface.co/Qwen/Qwen2.5-3B-Instruct",
      "license": "other",
      "isPartOf": [],
      "README": "\n# Qwen2.5-3B-Instruct\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 3B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\n- Number of Parameters: 3.09B\n- Number of Paramaters (Non-Embedding): 2.77B\n- Number of Layers: 36\n- Number of Attention Heads (GQA): 16 for Q and 2 for KV\n- Context Length: Full 32,768 tokens and generation 8192 tokens\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: 'qwen2'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-3B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [\ud83d\udcd1 blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```"
    },
    {
      "@id": "ark:59852/model-google-bert-bert-base-multilingual-cased-yj9xpuhec6v",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "google-bert/bert-base-multilingual-cased",
      "description": "Pretrained model on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective.",
      "author": "google-bert",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "safetensors",
        "bert",
        "fill-mask",
        "multilingual",
        "af",
        "sq",
        "ar",
        "an",
        "hy",
        "ast",
        "az",
        "ba",
        "eu",
        "bar",
        "be",
        "bn",
        "inc",
        "bs",
        "br",
        "bg",
        "my",
        "ca",
        "ceb",
        "ce",
        "zh",
        "cv",
        "hr",
        "cs",
        "da",
        "nl",
        "en",
        "et",
        "fi",
        "fr",
        "gl",
        "ka",
        "de",
        "el",
        "gu",
        "ht",
        "he",
        "hi",
        "hu",
        "is",
        "io",
        "id",
        "ga",
        "it",
        "ja",
        "jv",
        "kn",
        "kk",
        "ky",
        "ko",
        "la",
        "lv",
        "lt",
        "roa",
        "nds",
        "lm",
        "mk",
        "mg",
        "ms",
        "ml",
        "mr",
        "mn",
        "min",
        "ne",
        "new",
        "nb",
        "nn",
        "oc",
        "fa",
        "pms",
        "pl",
        "pt",
        "pa",
        "ro",
        "ru",
        "sco",
        "sr",
        "scn",
        "sk",
        "sl",
        "aze",
        "es",
        "su",
        "sw",
        "sv",
        "tl",
        "tg",
        "th",
        "ta",
        "tt",
        "te",
        "tr",
        "uk",
        "ud",
        "uz",
        "vi",
        "vo",
        "war",
        "cy",
        "fry",
        "pnb",
        "yo",
        "dataset:wikipedia",
        "arxiv:1810.04805",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/wikipedia"
        }
      ],
      "intendedUseCase": "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-multilingual-cased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] Hello I'm a model model. [SEP]\",\n  'score': 0.10182085633277893,\n  'token': 13192,\n  'token_str': 'model'},\n {'sequence': \"[CLS] Hello I'm a world model. [SEP]\",\n  'score': 0.052126359194517136,\n  'token': 11356,\n  'token_str': 'world'},\n {'sequence': \"[CLS] Hello I'm a data model. [SEP]\",\n  'score': 0.048930276185274124,\n  'token': 11165,\n  'token_str': 'data'},\n {'sequence': \"[CLS] Hello I'm a flight model. [SEP]\",\n  'score': 0.02036019042134285,\n  'token': 23578,\n  'token_str': 'flight'},\n {'sequence': \"[CLS] Hello I'm a business model. [SEP]\",\n  'score': 0.020079681649804115,\n  'token': 14155,\n  'token_str': 'business'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nmodel = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nmodel = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```",
      "usageInformation": "You can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-multilingual-cased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] Hello I'm a model model. [SEP]\",\n  'score': 0.10182085633277893,\n  'token': 13192,\n  'token_str': 'model'},\n {'sequence': \"[CLS] Hello I'm a world model. [SEP]\",\n  'score': 0.052126359194517136,\n  'token': 11356,\n  'token_str': 'world'},\n {'sequence': \"[CLS] Hello I'm a data model. [SEP]\",\n  'score': 0.048930276185274124,\n  'token': 11165,\n  'token_str': 'data'},\n {'sequence': \"[CLS] Hello I'm a flight model. [SEP]\",\n  'score': 0.02036019042134285,\n  'token': 23578,\n  'token_str': 'flight'},\n {'sequence': \"[CLS] Hello I'm a business model. [SEP]\",\n  'score': 0.020079681649804115,\n  'token': 14155,\n  'token_str': 'business'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nmodel = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nmodel = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```",
      "contentUrl": "https://huggingface.co/google-bert/bert-base-multilingual-cased/resolve/main/model.safetensors",
      "url": "https://huggingface.co/google-bert/bert-base-multilingual-cased",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# BERT multilingual base model (cased)\n\nPretrained model on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective.\nIt was introduced in [this paper](https://arxiv.org/abs/1810.04805) and first released in\n[this repository](https://github.com/google-research/bert). This model is case sensitive: it makes a difference\nbetween english and English.\n\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nBERT is a transformers model pretrained on a large corpus of multilingual data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the languages in the training set that can then be used to\nextract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a\nstandard classifier using the features produced by the BERT model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-multilingual-cased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] Hello I'm a model model. [SEP]\",\n  'score': 0.10182085633277893,\n  'token': 13192,\n  'token_str': 'model'},\n {'sequence': \"[CLS] Hello I'm a world model. [SEP]\",\n  'score': 0.052126359194517136,\n  'token': 11356,\n  'token_str': 'world'},\n {'sequence': \"[CLS] Hello I'm a data model. [SEP]\",\n  'score': 0.048930276185274124,\n  'token': 11165,\n  'token_str': 'data'},\n {'sequence': \"[CLS] Hello I'm a flight model. [SEP]\",\n  'score': 0.02036019042134285,\n  'token': 23578,\n  'token_str': 'flight'},\n {'sequence': \"[CLS] Hello I'm a business model. [SEP]\",\n  'score': 0.020079681649804115,\n  'token': 14155,\n  'token_str': 'business'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nmodel = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nmodel = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n## Training data\n\nThe BERT model was pretrained on the 104 languages with the largest Wikipedias. You can find the complete list\n[here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a shared vocabulary size of 110,000. The languages with a\nlarger Wikipedia are under-sampled and the ones with lower resources are oversampled. For languages like Chinese,\nJapanese Kanji and Korean Hanja that don't have space, a CJK Unicode block is added around every character. \n\nThe inputs of the model are then of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n"
    },
    {
      "@id": "ark:59852/model-qwen-qwen3-4b-instruct-2507-8gxrkmrrr4",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "Qwen/Qwen3-4B-Instruct-2507",
      "description": "<a href=\"https://chat.qwen.ai\" target=\"_blank\" style=\"margin: 2px;\">",
      "author": "Qwen",
      "keywords": [
        "transformers",
        "safetensors",
        "qwen3",
        "text-generation",
        "conversational",
        "arxiv:2505.09388",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-generation",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "intendedUseCase": "For local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.",
      "usageInformation": "- **Significant improvements** in general capabilities, including **instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage**.\n- **Substantial gains** in long-tail knowledge coverage across **multiple languages**.\n- **Markedly better alignment** with user preferences in **subjective and open-ended tasks**, enabling more helpful responses and higher-quality text generation.\n- **Enhanced capabilities** in **256K long-context understanding**.",
      "contentUrl": "https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/resolve/main/model-00001-of-00003.safetensors",
      "url": "https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Qwen3-4B-Instruct-2507\n<a href=\"https://chat.qwen.ai\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Highlights\n\nWe introduce the updated version of the **Qwen3-4B non-thinking mode**, named **Qwen3-4B-Instruct-2507**, featuring the following key enhancements:\n\n- **Significant improvements** in general capabilities, including **instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage**.\n- **Substantial gains** in long-tail knowledge coverage across **multiple languages**.\n- **Markedly better alignment** with user preferences in **subjective and open-ended tasks**, enabling more helpful responses and higher-quality text generation.\n- **Enhanced capabilities** in **256K long-context understanding**.\n\n![image/jpeg](https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-2507/Qwen3-4B-Instruct.001.jpeg)\n\n## Model Overview\n\n**Qwen3-4B-Instruct-2507** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 4.0B\n- Number of Paramaters (Non-Embedding): 3.6B\n- Number of Layers: 36\n- Number of Attention Heads (GQA): 32 for Q and 8 for KV\n- Context Length: **262,144 natively**. \n\n**NOTE: This model supports only non-thinking mode and does not generate ``<think></think>`` blocks in its output. Meanwhile, specifying `enable_thinking=False` is no longer required.**\n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n\n## Performance\n\n|  | GPT-4.1-nano-2025-04-14 | Qwen3-30B-A3B Non-Thinking | Qwen3-4B Non-Thinking | Qwen3-4B-Instruct-2507 |\n|--- | --- | --- | --- | --- |\n| **Knowledge** | | | |\n| MMLU-Pro | 62.8 | 69.1 | 58.0 | **69.6** |\n| MMLU-Redux | 80.2 | 84.1 | 77.3 | **84.2** |\n| GPQA | 50.3 | 54.8 | 41.7 | **62.0** |\n| SuperGPQA | 32.2 | 42.2 | 32.0 | **42.8** |\n| **Reasoning** | | | |\n| AIME25 | 22.7 | 21.6 | 19.1 | **47.4** |\n| HMMT25 | 9.7 | 12.0 | 12.1 | **31.0** |\n| ZebraLogic | 14.8 | 33.2 | 35.2 | **80.2** |\n| LiveBench 20241125 | 41.5 | 59.4 | 48.4 | **63.0** |\n| **Coding** | | | |\n| LiveCodeBench v6 (25.02-25.05) | 31.5 | 29.0 | 26.4 | **35.1** |\n| MultiPL-E | 76.3 | 74.6 | 66.6 | **76.8** |\n| Aider-Polyglot |  9.8 | **24.4** | 13.8 | 12.9 |\n| **Alignment** | | | |\n| IFEval | 74.5 | **83.7** | 81.2 | 83.4 |\n| Arena-Hard v2* | 15.9 | 24.8 | 9.5 | **43.4** |\n| Creative Writing v3 | 72.7 | 68.1 | 53.6 | **83.5** |\n| WritingBench | 66.9 | 72.2 | 68.5 | **83.4** |\n| **Agent** | | | |\n| BFCL-v3 | 53.0 | 58.6 | 57.6 | **61.9** |\n| TAU1-Retail | 23.5 | 38.3 | 24.3 | **48.7** |\n| TAU1-Airline | 14.0 | 18.0 | 16.0 | **32.0** |\n| TAU2-Retail | - | 31.6 | 28.1 | **40.4** |\n| TAU2-Airline | - | 18.0 | 12.0 | **24.0** |\n| TAU2-Telecom | - | **18.4** | 17.5 | 13.2 |\n| **Multilingualism** | | | |\n| MultiIF | 60.7 | **70.8** | 61.3 | 69.0 |\n| MMLU-ProX | 56.2 | **65.1** | 49.6 | 61.6 |\n| INCLUDE | 58.6 | **67.8** | 53.8 | 60.1 |\n| PolyMATH | 15.6 | 23.3 | 16.6 | **31.1** |\n\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\n\n\n## Quickstart\n\nThe code of Qwen3 has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: 'qwen3'\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=16384\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\n\nprint(\"content:\", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-4B-Instruct-2507 --context-length 262144\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-4B-Instruct-2507 --max-model-len 262144\n    ```\n\n**Note: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as `32,768`.**\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    'model': 'Qwen3-4B-Instruct-2507',\n\n    # Use a custom endpoint compatible with OpenAI API:\n    'model_server': 'http://localhost:8000/v1',  # api_base\n    'api_key': 'EMPTY',\n}\n\n# Define Tools\ntools = [\n    {'mcpServers': {  # You can specify the MCP configuration file\n            'time': {\n                'command': 'uvx',\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n            },\n            \"fetch\": {\n                \"command\": \"uvx\",\n                \"args\": [\"mcp-server-fetch\"]\n            }\n        }\n    },\n  'code_interpreter',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - We suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g., `\"answer\": \"C\"`.\"\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```"
    },
    {
      "@id": "ark:59852/model-deepseek-ai-deepseek-ocr-iz2sih8d7md",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "deepseek-ai/DeepSeek-OCR",
      "description": "<div align=\"center\">",
      "author": "deepseek-ai",
      "keywords": [
        "transformers",
        "safetensors",
        "deepseek_vl_v2",
        "feature-extraction",
        "deepseek",
        "vision-language",
        "ocr",
        "custom_code",
        "image-text-to-text",
        "multilingual",
        "arxiv:2510.18234",
        "license:mit",
        "region:us"
      ],
      "version": "1.0",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "usageInformation": "Inference using Huggingface transformers on NVIDIA GPUs. Requirements tested on python 3.12.9 + CUDA11.8\uff1a\n\n```\ntorch==2.6.0\ntransformers==4.46.3\ntokenizers==0.20.3\neinops\naddict \neasydict\npip install flash-attn==2.7.3 --no-build-isolation\n```\n\n```python\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\nmodel_name = 'deepseek-ai/DeepSeek-OCR'\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True)\nmodel = model.eval().cuda().to(torch.bfloat16)",
      "contentUrl": "https://huggingface.co/deepseek-ai/DeepSeek-OCR/resolve/main/model-00001-of-000001.safetensors",
      "url": "https://huggingface.co/deepseek-ai/DeepSeek-OCR",
      "license": "mit",
      "isPartOf": [],
      "README": "<div align=\"center\">\n  <img src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true\" width=\"60%\" alt=\"DeepSeek AI\" />\n</div>\n<hr>\n<div align=\"center\">\n  <a href=\"https://www.deepseek.com/\" target=\"_blank\">\n    <img alt=\"Homepage\" src=\"https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true\" />\n  </a>\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\" target=\"_blank\">\n    <img alt=\"Hugging Face\" src=\"https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white\" />\n  </a>\n\n</div>\n\n<div align=\"center\">\n\n  <a href=\"https://discord.gg/Tc7c45Zzu5\" target=\"_blank\">\n    <img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da\" />\n  </a>\n  <a href=\"https://twitter.com/deepseek_ai\" target=\"_blank\">\n    <img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white\" />\n  </a>\n\n</div>\n\n\n\n<p align=\"center\">\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR\"><b>\ud83c\udf1f Github</b></a> |\n  <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-OCR\"><b>\ud83d\udce5 Model Download</b></a> |\n  <a href=\"https://github.com/deepseek-ai/DeepSeek-OCR/blob/main/DeepSeek_OCR_paper.pdf\"><b>\ud83d\udcc4 Paper Link</b></a> |\n  <a href=\"https://arxiv.org/abs/2510.18234\"><b>\ud83d\udcc4 Arxiv Paper Link</b></a> |\n</p>\n<h2>\n<p align=\"center\">\n  <a href=\"https://huggingface.co/papers/2510.18234\">DeepSeek-OCR: Contexts Optical Compression</a>\n</p>\n</h2>\n<p align=\"center\">\n<img src=\"assets/fig1.png\" style=\"width: 1000px\" align=center>\n</p>\n<p align=\"center\">\n<a href=\"https://huggingface.co/papers/2510.18234\">Explore the boundaries of visual-text compression.</a>       \n</p>\n\n## Usage\nInference using Huggingface transformers on NVIDIA GPUs. Requirements tested on python 3.12.9 + CUDA11.8\uff1a\n\n```\ntorch==2.6.0\ntransformers==4.46.3\ntokenizers==0.20.3\neinops\naddict \neasydict\npip install flash-attn==2.7.3 --no-build-isolation\n```\n\n```python\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\nmodel_name = 'deepseek-ai/DeepSeek-OCR'\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModel.from_pretrained(model_name, _attn_implementation='flash_attention_2', trust_remote_code=True, use_safetensors=True)\nmodel = model.eval().cuda().to(torch.bfloat16)\n\n# prompt = \"<image>\\nFree OCR. \"\nprompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\nimage_file = 'your_image.jpg'\noutput_path = 'your/output/dir'\n\n# infer(self, tokenizer, prompt='', image_file='', output_path = ' ', base_size = 1024, image_size = 640, crop_mode = True, test_compress = False, save_results = False):\n\n# Tiny: base_size = 512, image_size = 512, crop_mode = False\n# Small: base_size = 640, image_size = 640, crop_mode = False\n# Base: base_size = 1024, image_size = 1024, crop_mode = False\n# Large: base_size = 1280, image_size = 1280, crop_mode = False\n\n# Gundam: base_size = 1024, image_size = 640, crop_mode = True\n\nres = model.infer(tokenizer, prompt=prompt, image_file=image_file, output_path = output_path, base_size = 1024, image_size = 640, crop_mode=True, save_results = True, test_compress = True)\n```\n\n## vLLM\nRefer to [\ud83c\udf1fGitHub](https://github.com/deepseek-ai/DeepSeek-OCR/) for guidance on model inference acceleration and PDF processing, etc.<!--  -->\n\n[2025/10/23] \ud83d\ude80\ud83d\ude80\ud83d\ude80 DeepSeek-OCR is now officially supported in upstream [vLLM](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-OCR.html#installing-vllm).\n```shell\nuv venv\nsource .venv/bin/activate\n# Until v0.11.1 release, you need to install vLLM from nightly build\nuv pip install -U vllm --pre --extra-index-url https://wheels.vllm.ai/nightly\n```\n\n```python\nfrom vllm import LLM, SamplingParams\nfrom vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor\nfrom PIL import Image\n\n# Create model instance\nllm = LLM(\n    model=\"deepseek-ai/DeepSeek-OCR\",\n    enable_prefix_caching=False,\n    mm_processor_cache_gb=0,\n    logits_processors=[NGramPerReqLogitsProcessor]\n)\n\n# Prepare batched input with your image file\nimage_1 = Image.open(\"path/to/your/image_1.png\").convert(\"RGB\")\nimage_2 = Image.open(\"path/to/your/image_2.png\").convert(\"RGB\")\nprompt = \"<image>\\nFree OCR.\"\n\nmodel_input = [\n    {\n        \"prompt\": prompt,\n        \"multi_modal_data\": {\"image\": image_1}\n    },\n    {\n        \"prompt\": prompt,\n        \"multi_modal_data\": {\"image\": image_2}\n    }\n]\n\nsampling_param = SamplingParams(\n            temperature=0.0,\n            max_tokens=8192,\n            # ngram logit processor args\n            extra_args=dict(\n                ngram_size=30,\n                window_size=90,\n                whitelist_token_ids={128821, 128822},  # whitelist: <td>, </td>\n            ),\n            skip_special_tokens=False,\n        )\n# Generate output\nmodel_outputs = llm.generate(model_input, sampling_param)\n\n# Print output\nfor output in model_outputs:\n    print(output.outputs[0].text)\n```\n\n\n## Visualizations\n<table>\n<tr>\n<td><img src=\"assets/show1.jpg\" style=\"width: 500px\"></td>\n<td><img src=\"assets/show2.jpg\" style=\"width: 500px\"></td>\n</tr>\n<tr>\n<td><img src=\"assets/show3.jpg\" style=\"width: 500px\"></td>\n<td><img src=\"assets/show4.jpg\" style=\"width: 500px\"></td>\n</tr>\n</table>\n\n\n## Acknowledgement\n\nWe would like to thank [Vary](https://github.com/Ucas-HaoranWei/Vary/), [GOT-OCR2.0](https://github.com/Ucas-HaoranWei/GOT-OCR2.0/), [MinerU](https://github.com/opendatalab/MinerU), [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR), [OneChart](https://github.com/LingyvKong/OneChart), [Slow Perception](https://github.com/Ucas-HaoranWei/Slow-Perception) for their valuable models and ideas.\n\nWe also appreciate the benchmarks: [Fox](https://github.com/ucaslcl/Fox), [OminiDocBench](https://github.com/opendatalab/OmniDocBench).\n\n\n## Citation\n```bibtex\n@article{wei2025deepseek,\n  title={DeepSeek-OCR: Contexts Optical Compression},\n  author={Wei, Haoran and Sun, Yaofeng and Li, Yukun},\n  journal={arXiv preprint arXiv:2510.18234},\n  year={2025}\n}"
    },
    {
      "@id": "ark:59852/model-facebook-bart-large-cnn-wgpq25wfr0d",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "facebook/bart-large-cnn",
      "description": "BART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail). It was introduced in the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al. and first released in [this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart).",
      "author": "facebook",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "rust",
        "safetensors",
        "bart",
        "text2text-generation",
        "summarization",
        "en",
        "dataset:cnn_dailymail",
        "arxiv:1910.13461",
        "license:mit",
        "model-index",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/cnn_dailymail"
        }
      ],
      "intendedUseCase": "You can use this model for text summarization. \n\n### How to use\n\nHere is how to use this model with the [pipeline API](https://huggingface.co/transformers/main_classes/pipelines.html):\n\n```python\nfrom transformers import pipeline\n\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\nARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\nA year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\nOnly 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\nIn 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\nBarrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n2010 marriage license application, according to court documents.\nProsecutors said the marriages were part of an immigration scam.\nOn Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\nAfter leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\nAnnette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\nAll occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\nProsecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\nAny divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\nThe case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\nInvestigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\nHer eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\nIf convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n\"\"\"\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\n>>> [{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1910-13461,\n  author    = {Mike Lewis and\n               Yinhan Liu and\n               Naman Goyal and\n               Marjan Ghazvininejad and\n               Abdelrahman Mohamed and\n               Omer Levy and\n               Veselin Stoyanov and\n               Luke Zettlemoyer},\n  title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language\n               Generation, Translation, and Comprehension},\n  journal   = {CoRR},\n  volume    = {abs/1910.13461},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1910.13461},\n  eprinttype = {arXiv},\n  eprint    = {1910.13461},\n  timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```",
      "usageInformation": "Here is how to use this model with the [pipeline API](https://huggingface.co/transformers/main_classes/pipelines.html):\n\n```python\nfrom transformers import pipeline\n\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\nARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\nA year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\nOnly 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\nIn 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\nBarrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n2010 marriage license application, according to court documents.\nProsecutors said the marriages were part of an immigration scam.\nOn Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\nAfter leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\nAnnette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\nAll occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\nProsecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\nAny divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\nThe case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\nInvestigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\nHer eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\nIf convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n\"\"\"\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\n>>> [{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]\n```",
      "contentUrl": "https://huggingface.co/facebook/bart-large-cnn/resolve/main/model.safetensors",
      "url": "https://huggingface.co/facebook/bart-large-cnn",
      "license": "mit",
      "isPartOf": [],
      "README": "# BART (large-sized model), fine-tuned on CNN Daily Mail \n\nBART model pre-trained on English language, and fine-tuned on [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail). It was introduced in the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Lewis et al. and first released in [this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart). \n\nDisclaimer: The team releasing BART did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nBART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n\nBART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs.\n\n## Intended uses & limitations\n\nYou can use this model for text summarization. \n\n### How to use\n\nHere is how to use this model with the [pipeline API](https://huggingface.co/transformers/main_classes/pipelines.html):\n\n```python\nfrom transformers import pipeline\n\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\nARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\nA year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\nOnly 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\nIn 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\nBarrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n2010 marriage license application, according to court documents.\nProsecutors said the marriages were part of an immigration scam.\nOn Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\nAfter leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\nAnnette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\nAll occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\nProsecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\nAny divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\nThe case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\nInvestigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\nHer eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\nIf convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n\"\"\"\nprint(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\n>>> [{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1910-13461,\n  author    = {Mike Lewis and\n               Yinhan Liu and\n               Naman Goyal and\n               Marjan Ghazvininejad and\n               Abdelrahman Mohamed and\n               Omer Levy and\n               Veselin Stoyanov and\n               Luke Zettlemoyer},\n  title     = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language\n               Generation, Translation, and Comprehension},\n  journal   = {CoRR},\n  volume    = {abs/1910.13461},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1910.13461},\n  eprinttype = {arXiv},\n  eprint    = {1910.13461},\n  timestamp = {Thu, 31 Oct 2019 14:02:26 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-13461.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```"
    },
    {
      "@id": "ark:59852/model-coqui-xtts-v2-uhwhifb25n",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "coqui/XTTS-v2",
      "description": "\u24cdTTS is a Voice generation model that lets you clone voices into different languages by using just a quick 6-second audio clip. There is no need for an excessive amount of training data that spans countless hours.",
      "author": "coqui",
      "keywords": [
        "coqui",
        "text-to-speech",
        "license:other",
        "region:us"
      ],
      "version": "1.0",
      "modelFormat": "pytorch-pth",
      "trainingDataset": [],
      "usageInformation": "### Code\nThe [code-base](https://github.com/coqui-ai/TTS) supports inference and [fine-tuning](https://tts.readthedocs.io/en/latest/models/xtts.html#training).",
      "contentUrl": "https://huggingface.co/coqui/XTTS-v2/resolve/main/dvae.pth",
      "url": "https://huggingface.co/coqui/XTTS-v2",
      "license": "other",
      "isPartOf": [],
      "README": "\n# \u24cdTTS\n\u24cdTTS is a Voice generation model that lets you clone voices into different languages by using just a quick 6-second audio clip. There is no need for an excessive amount of training data that spans countless hours.\n\nThis is the same or similar model to what powers [Coqui Studio](https://coqui.ai/) and [Coqui API](https://docs.coqui.ai/docs).\n\n### Features\n- Supports 17 languages. \n- Voice cloning with just a 6-second audio clip.\n- Emotion and style transfer by cloning. \n- Cross-language voice cloning.\n- Multi-lingual speech generation.\n- 24khz sampling rate.\n\n### Updates over XTTS-v1\n- 2 new languages; Hungarian and Korean\n- Architectural improvements for speaker conditioning.\n- Enables the use of multiple speaker references and interpolation between speakers.\n- Stability improvements.\n- Better prosody and audio quality across the board.\n\n### Languages\nXTTS-v2 supports 17 languages: **English (en), Spanish (es), French (fr), German (de), Italian (it), Portuguese (pt),\nPolish (pl), Turkish (tr), Russian (ru), Dutch (nl), Czech (cs), Arabic (ar), Chinese (zh-cn), Japanese (ja), Hungarian (hu), Korean (ko)\nHindi (hi)**.\n\nStay tuned as we continue to add support for more languages. If you have any language requests, feel free to reach out!\n\n### Code\nThe [code-base](https://github.com/coqui-ai/TTS) supports inference and [fine-tuning](https://tts.readthedocs.io/en/latest/models/xtts.html#training).\n\n### Demo Spaces\n- [XTTS Space](https://huggingface.co/spaces/coqui/xtts)  :  You can see how model performs on supported languages, and try with your own reference or microphone input\n- [XTTS Voice Chat with Mistral or Zephyr](https://huggingface.co/spaces/coqui/voice-chat-with-mistral) : You can experience streaming voice chat with Mistral 7B Instruct or Zephyr 7B Beta\n\n|                                 |                                         |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udc38\ud83d\udcac **CoquiTTS**               | [coqui/TTS on Github](https://github.com/coqui-ai/TTS)|\n| \ud83d\udcbc **Documentation**            | [ReadTheDocs](https://tts.readthedocs.io/en/latest/)\n| \ud83d\udc69\u200d\ud83d\udcbb **Questions**                | [GitHub Discussions](https://github.com/coqui-ai/TTS/discussions) |\n| \ud83d\uddef **Community**         | [Discord](https://discord.gg/5eXr5seRrv)  |\n\n\n### License\nThis model is licensed under [Coqui Public Model License](https://coqui.ai/cpml). There's a lot that goes into a license for generative models, and you can read more of [the origin story of CPML here](https://coqui.ai/blog/tts/cpml).\n\n### Contact\nCome and join in our \ud83d\udc38Community. We're active on [Discord](https://discord.gg/fBC58unbKE) and [Twitter](https://twitter.com/coqui_ai).\nYou can also mail us at info@coqui.ai.\n\nUsing \ud83d\udc38TTS API:\n\n```python\nfrom TTS.api import TTS\ntts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)\n\n# generate speech by cloning a voice using default settings\ntts.tts_to_file(text=\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n                file_path=\"output.wav\",\n                speaker_wav=\"/path/to/target/speaker.wav\",\n                language=\"en\")\n\n```\n\nUsing \ud83d\udc38TTS Command line:\n\n```console\n tts --model_name tts_models/multilingual/multi-dataset/xtts_v2 \\\n     --text \"Bug\u00fcn okula gitmek istemiyorum.\" \\\n     --speaker_wav /path/to/target/speaker.wav \\\n     --language_idx tr \\\n     --use_cuda true\n```\n\nUsing the model directly:\n\n```python\nfrom TTS.tts.configs.xtts_config import XttsConfig\nfrom TTS.tts.models.xtts import Xtts\n\nconfig = XttsConfig()\nconfig.load_json(\"/path/to/xtts/config.json\")\nmodel = Xtts.init_from_config(config)\nmodel.load_checkpoint(config, checkpoint_dir=\"/path/to/xtts/\", eval=True)\nmodel.cuda()\n\noutputs = model.synthesize(\n    \"It took me quite a long time to develop a voice and now that I have it I am not going to be silent.\",\n    config,\n    speaker_wav=\"/data/TTS-public/_refclips/3.wav\",\n    gpt_cond_len=3,\n    language=\"en\",\n)\n```\n"
    },
    {
      "@id": "ark:59852/model-meta-llama-llama-3-1-8b-instruct-b9umck0jl7u",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "meta-llama/Llama-3.1-8B-Instruct",
      "description": "The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.",
      "author": "meta-llama",
      "keywords": [
        "transformers",
        "safetensors",
        "llama",
        "text-generation",
        "facebook",
        "meta",
        "pytorch",
        "llama-3",
        "conversational",
        "en",
        "de",
        "fr",
        "it",
        "pt",
        "hi",
        "es",
        "th",
        "arxiv:2204.05149",
        "base_model:meta-llama/Llama-3.1-8B",
        "base_model:finetune:meta-llama/Llama-3.1-8B",
        "license:llama3.1",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-generation",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "hasBias": "As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n\n\n* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. \n* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n* Provide protections for the community to help prevent the misuse of our models.\n\n\n### Responsible deployment \n\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta\u2019s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more. \n\n\n#### Llama 3.1 instruct \n\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper. \n\n**Fine-tuning data**\n\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We\u2019ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. \n\n**Refusals and Tone**\n\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines. \n\n\n#### Llama 3.1 systems\n\n**Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.** Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. \n\nAs part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. \n\n\n#### New capabilities \n\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\n\n**Tool-use**: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. \n\n**Multilinguality**: Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. \n\n\n### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application. \n\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n\n**Red teaming**\n\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. \n\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n\n### Critical and other risks \n\nWe specifically focused our efforts on mitigating the following critical risk areas:\n\n**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\n\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. \n\n\n**2. Child Safety**\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model\u2019s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n**3. Cyber attack enablement**\n\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\n\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n\nOur study of Llama-3.1-405B\u2019s social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\n\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta\u2019s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.",
      "intendedUseCase": "**Intended Use Cases** Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases. \n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n\n**<span style=\"text-decoration:underline;\">Note</span>: Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.",
      "usageInformation": "This repository contains two versions of Meta-Llama-3.1-8B-Instruct, for use with transformers and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with `transformers >= 4.43.0` onward, you can run conversational inference using the Transformers `pipeline` abstraction or by leveraging the Auto classes with the `generate()` function.\n\nMake sure to update your transformers installation via `pip install --upgrade transformers`.\n\n```python\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nNote: You can also find detailed recipes on how to use the model locally, with `torch.compile()`, assisted generations, quantised and more at [`huggingface-llama-recipes`](https://github.com/huggingface/huggingface-llama-recipes)\n\n### Tool use with transformers\n\nLLaMA-3.1 supports multiple tool use formats. You can see a full guide to prompt formatting [here](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/).\n\nTool use is also supported through [chat templates](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling) in Transformers. \nHere is a quick example showing a single simple tool:\n\n```python",
      "baseModel": "meta-llama/Meta-Llama-3.1-8B",
      "contentUrl": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/model-00001-of-00004.safetensors",
      "url": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
      "license": "llama3.1",
      "isPartOf": [],
      "README": "\n## Model Information\n\nThe Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out). The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model developer**: Meta\n\n**Model Architecture:** Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. \n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Data</strong>\n   </td>\n   <td><strong>Params</strong>\n   </td>\n   <td><strong>Input modalities</strong>\n   </td>\n   <td><strong>Output modalities</strong>\n   </td>\n   <td><strong>Context length</strong>\n   </td>\n   <td><strong>GQA</strong>\n   </td>\n   <td><strong>Token count</strong>\n   </td>\n   <td><strong>Knowledge cutoff</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"3\" >Llama 3.1 (text only)\n   </td>\n   <td rowspan=\"3\" >A new mix of publicly available online data.\n   </td>\n   <td>8B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n   <td rowspan=\"3\" >15T+\n   </td>\n   <td rowspan=\"3\" >December 2023\n   </td>\n  </tr>\n  <tr>\n   <td>70B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n  </tr>\n  <tr>\n   <td>405B\n   </td>\n   <td>Multilingual Text\n   </td>\n   <td>Multilingual Text and code\n   </td>\n   <td>128k\n   </td>\n   <td>Yes\n   </td>\n  </tr>\n</table>\n\n\n**Supported languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.\n\n**Llama 3.1 family of models**. Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date:** July 23, 2024.\n\n**Status:** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License:** A custom commercial license, the Llama 3.1 Community License, is available at: [https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)\n\nWhere to send questions or comments about the model Instructions on how to provide feedback or comments on the model can be found in the model [README](https://github.com/meta-llama/llama3). For more technical information about generation parameters and recipes for how to use Llama 3.1 in applications, please go [here](https://github.com/meta-llama/llama-recipes). \n\n\n## Intended Use\n\n**Intended Use Cases** Llama 3.1 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. The Llama 3.1 model collection also supports the ability to leverage the outputs of its models to improve other models including synthetic data generation and distillation. The Llama 3.1 Community License allows for these use cases. \n\n**Out-of-scope** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.1 Community License. Use in languages beyond those explicitly referenced as supported in this model card**.\n\n**<span style=\"text-decoration:underline;\">Note</span>: Llama 3.1 has been trained on a broader collection of languages than the 8 supported languages. Developers may fine-tune Llama 3.1 models for languages beyond the 8 supported languages provided they comply with the Llama 3.1 Community License and the Acceptable Use Policy and in such cases are responsible for ensuring that any uses of Llama 3.1 in additional languages is done in a safe and responsible manner.\n\n## How to use\n\nThis repository contains two versions of Meta-Llama-3.1-8B-Instruct, for use with transformers and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with `transformers >= 4.43.0` onward, you can run conversational inference using the Transformers `pipeline` abstraction or by leveraging the Auto classes with the `generate()` function.\n\nMake sure to update your transformers installation via `pip install --upgrade transformers`.\n\n```python\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nNote: You can also find detailed recipes on how to use the model locally, with `torch.compile()`, assisted generations, quantised and more at [`huggingface-llama-recipes`](https://github.com/huggingface/huggingface-llama-recipes)\n\n### Tool use with transformers\n\nLLaMA-3.1 supports multiple tool use formats. You can see a full guide to prompt formatting [here](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/).\n\nTool use is also supported through [chat templates](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling) in Transformers. \nHere is a quick example showing a single simple tool:\n\n```python\n# First, define a tool\ndef get_current_temperature(location: str) -> float:\n    \"\"\"\n    Get the current temperature at a location.\n    \n    Args:\n        location: The location to get the temperature for, in the format \"City, Country\"\n    Returns:\n        The current temperature at the specified location in the specified units, as a float.\n    \"\"\"\n    return 22.  # A real function should probably actually get the temperature!\n\n# Next, create a chat and apply the chat template\nmessages = [\n  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n]\n\ninputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\n```\n\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\n\n```python\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n```\n\nand then call the tool and append the result, with the `tool` role, like so:\n\n```python\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n```\n\nAfter that, you can `generate()` again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling - for more information,\nsee the [LLaMA prompt format docs](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/) and the Transformers [tool use documentation](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling).\n\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama)\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include \"original/*\" --local-dir Meta-Llama-3.1-8B-Instruct\n```\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, annotation, and evaluation were also performed on production infrastructure.\n\n**Training utilized a cumulative of** 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency. \n\n\n**Training Greenhouse Gas Emissions** Estimated total location-based greenhouse gas emissions were **11,390** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy, therefore the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n\n<table>\n  <tr>\n   <td>\n   </td>\n   <td><strong>Training Time (GPU hours)</strong>\n   </td>\n   <td><strong>Training Power Consumption (W)</strong>\n   </td>\n   <td><strong>Training Location-Based Greenhouse Gas Emissions</strong>\n<p>\n<strong>(tons CO2eq)</strong>\n   </td>\n   <td><strong>Training Market-Based Greenhouse Gas Emissions</strong>\n<p>\n<strong>(tons CO2eq)</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 8B\n   </td>\n   <td>1.46M\n   </td>\n   <td>700\n   </td>\n   <td>420\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 70B\n   </td>\n   <td>7.0M\n   </td>\n   <td>700\n   </td>\n   <td>2,040\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Llama 3.1 405B\n   </td>\n   <td>30.84M\n   </td>\n   <td>700\n   </td>\n   <td>8,930\n   </td>\n   <td>0\n   </td>\n  </tr>\n  <tr>\n   <td>Total\n   </td>\n   <td>39.3M\n   <td>\n<ul>\n\n</ul>\n   </td>\n   <td>11,390\n   </td>\n   <td>0\n   </td>\n  </tr>\n</table>\n\n\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149).  Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions  will not be incurred by others.\n\n\n## Training Data\n\n**Overview:** Llama 3.1 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples. \n\n**Data Freshness:** The pretraining data has a cutoff of December 2023.\n\n\n## Benchmark scores\n\nIn this section, we report the results for Llama 3.1 models on standard automatic benchmarks. For all the evaluations, we use our internal evaluations library. \n\n### Base pretrained models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong># Shots</strong>\n   </td>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Llama 3 8B</strong>\n   </td>\n   <td><strong>Llama 3.1 8B</strong>\n   </td>\n   <td><strong>Llama 3 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 405B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"7\" >General\n   </td>\n   <td>MMLU\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc_char\n   </td>\n   <td>66.7\n   </td>\n   <td>66.7\n   </td>\n   <td>79.5\n   </td>\n   <td>79.3\n   </td>\n   <td>85.2\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU-Pro (CoT)\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc_char\n   </td>\n   <td>36.2\n   </td>\n   <td>37.1\n   </td>\n   <td>55.0\n   </td>\n   <td>53.8\n   </td>\n   <td>61.6\n   </td>\n  </tr>\n  <tr>\n   <td>AGIEval English\n   </td>\n   <td>3-5\n   </td>\n   <td>average/acc_char\n   </td>\n   <td>47.1\n   </td>\n   <td>47.8\n   </td>\n   <td>63.0\n   </td>\n   <td>64.6\n   </td>\n   <td>71.6\n   </td>\n  </tr>\n  <tr>\n   <td>CommonSenseQA\n   </td>\n   <td>7\n   </td>\n   <td>acc_char\n   </td>\n   <td>72.6\n   </td>\n   <td>75.0\n   </td>\n   <td>83.8\n   </td>\n   <td>84.1\n   </td>\n   <td>85.8\n   </td>\n  </tr>\n  <tr>\n   <td>Winogrande\n   </td>\n   <td>5\n   </td>\n   <td>acc_char\n   </td>\n   <td>-\n   </td>\n   <td>60.5\n   </td>\n   <td>-\n   </td>\n   <td>83.3\n   </td>\n   <td>86.7\n   </td>\n  </tr>\n  <tr>\n   <td>BIG-Bench Hard (CoT)\n   </td>\n   <td>3\n   </td>\n   <td>average/em\n   </td>\n   <td>61.1\n   </td>\n   <td>64.2\n   </td>\n   <td>81.3\n   </td>\n   <td>81.6\n   </td>\n   <td>85.9\n   </td>\n  </tr>\n  <tr>\n   <td>ARC-Challenge\n   </td>\n   <td>25\n   </td>\n   <td>acc_char\n   </td>\n   <td>79.4\n   </td>\n   <td>79.7\n   </td>\n   <td>93.1\n   </td>\n   <td>92.9\n   </td>\n   <td>96.1\n   </td>\n  </tr>\n  <tr>\n   <td>Knowledge reasoning\n   </td>\n   <td>TriviaQA-Wiki\n   </td>\n   <td>5\n   </td>\n   <td>em\n   </td>\n   <td>78.5\n   </td>\n   <td>77.6\n   </td>\n   <td>89.7\n   </td>\n   <td>89.8\n   </td>\n   <td>91.8\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Reading comprehension\n   </td>\n   <td>SQuAD\n   </td>\n   <td>1\n   </td>\n   <td>em\n   </td>\n   <td>76.4\n   </td>\n   <td>77.0\n   </td>\n   <td>85.6\n   </td>\n   <td>81.8\n   </td>\n   <td>89.3\n   </td>\n  </tr>\n  <tr>\n   <td>QuAC (F1)\n   </td>\n   <td>1\n   </td>\n   <td>f1\n   </td>\n   <td>44.4\n   </td>\n   <td>44.9\n   </td>\n   <td>51.1\n   </td>\n   <td>51.1\n   </td>\n   <td>53.6\n   </td>\n  </tr>\n  <tr>\n   <td>BoolQ\n   </td>\n   <td>0\n   </td>\n   <td>acc_char\n   </td>\n   <td>75.7\n   </td>\n   <td>75.0\n   </td>\n   <td>79.0\n   </td>\n   <td>79.4\n   </td>\n   <td>80.0\n   </td>\n  </tr>\n  <tr>\n   <td>DROP (F1)\n   </td>\n   <td>3\n   </td>\n   <td>f1\n   </td>\n   <td>58.4\n   </td>\n   <td>59.5\n   </td>\n   <td>79.7\n   </td>\n   <td>79.6\n   </td>\n   <td>84.8\n   </td>\n  </tr>\n</table>\n\n\n\n### Instruction tuned models\n\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong># Shots</strong>\n   </td>\n   <td><strong>Metric</strong>\n   </td>\n   <td><strong>Llama 3 8B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 8B Instruct</strong>\n   </td>\n   <td><strong>Llama 3 70B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 70B Instruct</strong>\n   </td>\n   <td><strong>Llama 3.1 405B Instruct</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >General\n   </td>\n   <td>MMLU\n   </td>\n   <td>5\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>68.5\n   </td>\n   <td>69.4\n   </td>\n   <td>82.0\n   </td>\n   <td>83.6\n   </td>\n   <td>87.3\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>65.3\n   </td>\n   <td>73.0\n   </td>\n   <td>80.9\n   </td>\n   <td>86.0\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td>MMLU-Pro (CoT)\n   </td>\n   <td>5\n   </td>\n   <td>micro_avg/acc_char\n   </td>\n   <td>45.5\n   </td>\n   <td>48.3\n   </td>\n   <td>63.4\n   </td>\n   <td>66.4\n   </td>\n   <td>73.3\n   </td>\n  </tr>\n  <tr>\n   <td>IFEval\n   </td>\n   <td>\n   </td>\n   <td>\n   </td>\n   <td>76.8\n   </td>\n   <td>80.4\n   </td>\n   <td>82.9\n   </td>\n   <td>87.5\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Reasoning\n   </td>\n   <td>ARC-C\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>82.4\n   </td>\n   <td>83.4\n   </td>\n   <td>94.4\n   </td>\n   <td>94.8\n   </td>\n   <td>96.9\n   </td>\n  </tr>\n  <tr>\n   <td>GPQA\n   </td>\n   <td>0\n   </td>\n   <td>em\n   </td>\n   <td>34.6\n   </td>\n   <td>30.4\n   </td>\n   <td>39.5\n   </td>\n   <td>46.7\n   </td>\n   <td>50.7\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Code\n   </td>\n   <td>HumanEval\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>60.4\n   </td>\n   <td>72.6\n   </td>\n   <td>81.7\n   </td>\n   <td>80.5\n   </td>\n   <td>89.0\n   </td>\n  </tr>\n  <tr>\n   <td>MBPP ++ base version\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>70.6\n   </td>\n   <td>72.8\n   </td>\n   <td>82.5\n   </td>\n   <td>86.0\n   </td>\n   <td>88.6\n   </td>\n  </tr>\n  <tr>\n   <td>Multipl-E HumanEval\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>-\n   </td>\n   <td>50.8\n   </td>\n   <td>-\n   </td>\n   <td>65.5\n   </td>\n   <td>75.2\n   </td>\n  </tr>\n  <tr>\n   <td>Multipl-E MBPP\n   </td>\n   <td>0\n   </td>\n   <td>pass@1\n   </td>\n   <td>-\n   </td>\n   <td>52.4\n   </td>\n   <td>-\n   </td>\n   <td>62.0\n   </td>\n   <td>65.7\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"2\" >Math\n   </td>\n   <td>GSM-8K (CoT)\n   </td>\n   <td>8\n   </td>\n   <td>em_maj1@1\n   </td>\n   <td>80.6\n   </td>\n   <td>84.5\n   </td>\n   <td>93.0\n   </td>\n   <td>95.1\n   </td>\n   <td>96.8\n   </td>\n  </tr>\n  <tr>\n   <td>MATH (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>final_em\n   </td>\n   <td>29.1\n   </td>\n   <td>51.9\n   </td>\n   <td>51.0\n   </td>\n   <td>68.0\n   </td>\n   <td>73.8\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"4\" >Tool Use\n   </td>\n   <td>API-Bank\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>48.3\n   </td>\n   <td>82.6\n   </td>\n   <td>85.1\n   </td>\n   <td>90.0\n   </td>\n   <td>92.0\n   </td>\n  </tr>\n  <tr>\n   <td>BFCL\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>60.3\n   </td>\n   <td>76.1\n   </td>\n   <td>83.0\n   </td>\n   <td>84.8\n   </td>\n   <td>88.5\n   </td>\n  </tr>\n  <tr>\n   <td>Gorilla Benchmark API Bench\n   </td>\n   <td>0\n   </td>\n   <td>acc\n   </td>\n   <td>1.7\n   </td>\n   <td>8.2\n   </td>\n   <td>14.7\n   </td>\n   <td>29.7\n   </td>\n   <td>35.3\n   </td>\n  </tr>\n  <tr>\n   <td>Nexus (0-shot)\n   </td>\n   <td>0\n   </td>\n   <td>macro_avg/acc\n   </td>\n   <td>18.1\n   </td>\n   <td>38.5\n   </td>\n   <td>47.8\n   </td>\n   <td>56.7\n   </td>\n   <td>58.7\n   </td>\n  </tr>\n  <tr>\n   <td>Multilingual\n   </td>\n   <td>Multilingual MGSM (CoT)\n   </td>\n   <td>0\n   </td>\n   <td>em\n   </td>\n   <td>-\n   </td>\n   <td>68.9\n   </td>\n   <td>-\n   </td>\n   <td>86.9\n   </td>\n   <td>91.6\n   </td>\n  </tr>\n</table>\n\n#### Multilingual benchmarks\n\n<table>\n  <tr>\n   <td><strong>Category</strong>\n   </td>\n   <td><strong>Benchmark</strong>\n   </td>\n   <td><strong>Language</strong>\n   </td>\n   <td><strong>Llama 3.1 8B</strong>\n   </td>\n   <td><strong>Llama 3.1 70B</strong>\n   </td>\n   <td><strong>Llama 3.1 405B</strong>\n   </td>\n  </tr>\n  <tr>\n   <td rowspan=\"9\" ><strong>General</strong>\n   </td>\n   <td rowspan=\"9\" ><strong>MMLU (5-shot, macro_avg/acc)</strong>\n   </td>\n   <td>Portuguese\n   </td>\n   <td>62.12\n   </td>\n   <td>80.13\n   </td>\n   <td>84.95\n   </td>\n  </tr>\n  <tr>\n   <td>Spanish\n   </td>\n   <td>62.45\n   </td>\n   <td>80.05\n   </td>\n   <td>85.08\n   </td>\n  </tr>\n  <tr>\n   <td>Italian\n   </td>\n   <td>61.63\n   </td>\n   <td>80.4\n   </td>\n   <td>85.04\n   </td>\n  </tr>\n  <tr>\n   <td>German\n   </td>\n   <td>60.59\n   </td>\n   <td>79.27\n   </td>\n   <td>84.36\n   </td>\n  </tr>\n  <tr>\n   <td>French\n   </td>\n   <td>62.34\n   </td>\n   <td>79.82\n   </td>\n   <td>84.66\n   </td>\n  </tr>\n  <tr>\n   <td>Hindi\n   </td>\n   <td>50.88\n   </td>\n   <td>74.52\n   </td>\n   <td>80.31\n   </td>\n  </tr>\n  <tr>\n   <td>Thai\n   </td>\n   <td>50.32\n   </td>\n   <td>72.95\n   </td>\n   <td>78.21\n   </td>\n  </tr>\n</table>\n\n\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n\n\n* Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama. \n* Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm.\n* Provide protections for the community to help prevent the misuse of our models.\n\n\n### Responsible deployment \n\nLlama is a foundational technology designed to be used in a variety of use cases, examples on how Meta\u2019s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models enabling the world to benefit from the technology power, by aligning our model safety for the generic use cases addressing a standard set of harms. Developers are then in the driver seat to tailor safety for their use case, defining their own policy and deploying the models with the necessary safeguards in their Llama systems. Llama 3.1 was developed following the best practices outlined in our Responsible Use Guide, you can refer to the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/) to learn more. \n\n\n#### Llama 3.1 instruct \n\nOur main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. For more details on the safety mitigations implemented please read the Llama 3 paper. \n\n**Fine-tuning data**\n\nWe employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We\u2019ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control. \n\n**Refusals and Tone**\n\nBuilding on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow  tone guidelines. \n\n\n#### Llama 3.1 systems\n\n**Large language models, including Llama 3.1, are not designed to be deployed in isolation but instead should be deployed as part of an overall AI system with additional safety guardrails as required.** Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. \n\nAs part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard 3, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box. \n\n\n#### New capabilities \n\nNote that this release introduces new capabilities, including a longer context window, multilingual inputs and outputs and possible integrations by developers with third party tools. Building with these new capabilities requires specific considerations in addition to the best practices that generally apply across all Generative AI use cases.\n\n**Tool-use**: Just like in standard software development, developers are responsible for the integration of the LLM with the tools and services of their choice. They should define a clear policy for their use case and assess the integrity of the third party services they use to be aware of the safety and security limitations when using this capability. Refer to the Responsible Use Guide for best practices on the safe deployment of the third party safeguards. \n\n**Multilinguality**: Llama 3.1 supports 7 languages in addition to English: French, German, Hindi, Italian, Portuguese, Spanish, and Thai. Llama may be able to output text in other languages than those that meet performance thresholds for safety and helpfulness. We strongly discourage developers from using this model to converse in non-supported languages without implementing finetuning and system controls in alignment with their policies and the best practices shared in the Responsible Use Guide. \n\n\n### Evaluations\n\nWe evaluated Llama models for common use cases as well as specific capabilities. Common use cases evaluations measure safety risks of systems for most commonly built applications including chat bot, coding assistant, tool calls. We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Llama Guard 3 to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case. Prompt Guard and Code Shield are also available if relevant to the application. \n\nCapability evaluations measure vulnerabilities of Llama models inherent to specific capabilities, for which were crafted dedicated benchmarks including long context, multilingual, tools calls, coding or memorization.\n\n**Red teaming**\n\nFor both scenarios, we conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. \n\nWe partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity.  The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n\n### Critical and other risks \n\nWe specifically focused our efforts on mitigating the following critical risk areas:\n\n**1- CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive materials) helpfulness**\n\nTo assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons. \n\n\n**2. Child Safety**\n\nChild Safety risk assessments were conducted using a team of experts, to assess the model\u2019s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development.  For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences. \n\n**3. Cyber attack enablement**\n\nOur cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\n\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention.\n\nOur study of Llama-3.1-405B\u2019s social engineering uplift for cyber attackers was conducted to assess the effectiveness of AI models in aiding cyber threat actors in spear phishing campaigns. Please read our Llama 3.1 Cyber security whitepaper to learn more.\n\n\n### Community \n\nGenerative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama). \n\nWe also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta\u2019s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists). \n\nFinally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n\n## Ethical Considerations and Limitations\n\nThe core values of Llama 3.1 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.1 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress. \n\nBut Llama 3.1 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.1\u2019s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.1 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development. "
    },
    {
      "@id": "ark:59852/model-jonatasgrosman-wav2vec2-large-xlsr-53-persian-6xmiq3igbqk",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "jonatasgrosman/wav2vec2-large-xlsr-53-persian",
      "description": "Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Persian using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice).",
      "author": "jonatasgrosman",
      "keywords": [
        "transformers",
        "pytorch",
        "jax",
        "wav2vec2",
        "automatic-speech-recognition",
        "audio",
        "speech",
        "xlsr-fine-tuning-week",
        "fa",
        "dataset:common_voice",
        "doi:10.57967/hf/3576",
        "license:apache-2.0",
        "model-index",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/common_voice"
        }
      ],
      "usageInformation": "The model can be used directly (without a language model) as follows...\n\nUsing the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-persian\")\naudio_paths = [\"/path/to/file.mp3\", \"/path/to/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"fa\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-persian\"\nSAMPLES = 5\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)",
      "contentUrl": "https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-persian/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-persian",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Fine-tuned XLSR-53 large model for speech recognition in Persian\n\nFine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Persian using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice).\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\nThis model has been fine-tuned thanks to the GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/) :)\n\nThe script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint\n\n## Usage\n\nThe model can be used directly (without a language model) as follows...\n\nUsing the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-persian\")\naudio_paths = [\"/path/to/file.mp3\", \"/path/to/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"fa\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-persian\"\nSAMPLES = 5\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n    batch[\"speech\"] = speech_array\n    batch[\"sentence\"] = batch[\"sentence\"].upper()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\npredicted_sentences = processor.batch_decode(predicted_ids)\n\nfor i, predicted_sentence in enumerate(predicted_sentences):\n    print(\"-\" * 100)\n    print(\"Reference:\", test_dataset[i][\"sentence\"])\n    print(\"Prediction:\", predicted_sentence)\n```\n\n| Reference  | Prediction |\n| ------------- | ------------- |\n| \u0627\u0632 \u0645\u0647\u0645\u0648\u0646\u062f\u0627\u0631\u06cc \u06a9\u0646\u0627\u0631 \u0628\u06a9\u0634\u0645 | \u0627\u0632 \u0645\u0647\u0645\u0627\u0646\u062f\u0627\u0631\u06cc \u06a9\u0646\u0627\u0631 \u0628\u06a9\u0634\u0645 |\n| \u0628\u0631\u0648 \u0627\u0632 \u0645\u0647\u0631\u062f\u0627\u062f \u0628\u067e\u0631\u0633. | \u0628\u0631\u0648 \u0627\u0632 \u0645\u0627\u0642\u062f\u0639\u0627\u062f \u0628\u0647 \u067e\u0631\u0633 |\n| \u062e\u0628 \u060c \u062a\u0648 \u0686\u06cc\u0643\u0627\u0631 \u0645\u06cc \u0643\u0646\u06cc\u061f | \u062e\u0648\u0628 \u062a\u0648 \u0686\u06cc\u06a9\u0627\u0631 \u0645\u06cc \u06a9\u0646\u06cc |\n| \u0645\u0633\u0642\u0637 \u067e\u0627\u06cc\u062a\u062e\u062a \u0639\u0645\u0627\u0646 \u062f\u0631 \u0639\u0631\u0628\u06cc \u0628\u0647 \u0645\u0639\u0646\u0627\u06cc \u0645\u062d\u0644 \u0633\u0642\u0648\u0637 \u0627\u0633\u062a | \u0645\u0633\u0642\u0637 \u067e\u0627\u06cc\u062a\u062e\u062a \u0639\u0645\u0627\u0646 \u062f\u0631 \u0639\u0631\u0628\u06cc \u0628\u0647 \u0628\u0639\u0646\u0627\u06cc \u0645\u062d\u0644 \u0633\u0642\u0648\u0637 \u0627\u0633\u062a |\n| \u0622\u0647\u060c \u0646\u0647 \u0627\u0635\u0644\u0627\u064f! | \u0627\u0647\u0646\u0647 \u0627\u0635\u0644\u0627 |\n| \u062a\u0648\u0627\u0646\u0633\u062a | \u062a\u0648\u0627\u0646\u0633\u062a |\n| \u0642\u0635\u06cc\u062f\u0647 \u0641\u0646 \u0634\u0639\u0631 \u0645\u06cc\u06af\u0648\u06cc\u062f \u0627\u06cc \u062f\u0648\u0633\u062a\u0627\u0646 | \u0642\u0635\u06cc\u062f\u0647 \u0641\u0646 \u0634\u0639\u0631 \u0645\u06cc\u06af\u0648\u06cc\u062f \u0627\u06cc\u062f\u0648\u0633\u062a\u0648\u0646 |\n| \u062f\u0648 \u0627\u0633\u062a\u0627\u06cc\u0644 \u0645\u062a\u0641\u0627\u0648\u062a \u062f\u0627\u0631\u06cc\u0646 | \u062f\u0648\u0628\u0648\u0633\u062a \u062f\u0627\u0631\u06cc\u0644 \u0648 \u0645\u062a\u0641\u0627\u0648\u062a \u0628\u0631\u06cc |\n| \u062f\u0648 \u0631\u0648\u0632 \u0642\u0628\u0644 \u0627\u0632 \u06a9\u0631\u06cc\u0633\u0645\u0633 \u061f | \u0627\u0648\u0646 \u0645\u0641\u062a\u0648\u062f \u067e\u0634 \u067e\u0634\u0634 |\n| \u0633\u0627\u0639\u062a \u0647\u0627\u06cc \u06a9\u0627\u0631\u06cc \u0686\u06cc\u0633\u062a\u061f | \u0627\u06cc\u0646 \u062a\u0648\u0631\u06cc \u06a9\u0647 \u0645\u0648\u0634\u06cc\u06a9\u0644 \u062e\u0628 |\n\n## Evaluation\n\nThe model can be evaluated as follows on the Persian test data of Common Voice.\n\n```python\nimport torch\nimport re\nimport librosa\nfrom datasets import load_dataset, load_metric\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"fa\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-persian\"\nDEVICE = \"cuda\"\n\nCHARS_TO_IGNORE = [\",\", \"?\", \"\u00bf\", \".\", \"!\", \"\u00a1\", \";\", \"\uff1b\", \":\", '\"\"', \"%\", '\"', \"\ufffd\", \"\u02bf\", \"\u00b7\", \"\u10fb\", \"~\", \"\u055e\",\n                   \"\u061f\", \"\u060c\", \"\u0964\", \"\u0965\", \"\u00ab\", \"\u00bb\", \"\u201e\", \"\u201c\", \"\u201d\", \"\u300c\", \"\u300d\", \"\u2018\", \"\u2019\", \"\u300a\", \"\u300b\", \"(\", \")\", \"[\", \"]\",\n                   \"{\", \"}\", \"=\", \"`\", \"_\", \"+\", \"<\", \">\", \"\u2026\", \"\u2013\", \"\u00b0\", \"\u00b4\", \"\u02be\", \"\u2039\", \"\u203a\", \"\u00a9\", \"\u00ae\", \"\u2014\", \"\u2192\", \"\u3002\",\n                   \"\u3001\", \"\ufe42\", \"\ufe41\", \"\u2027\", \"\uff5e\", \"\ufe4f\", \"\uff0c\", \"\uff5b\", \"\uff5d\", \"\uff08\", \"\uff09\", \"\uff3b\", \"\uff3d\", \"\u3010\", \"\u3011\", \"\u2025\", \"\u303d\",\n                   \"\u300e\", \"\u300f\", \"\u301d\", \"\u301f\", \"\u27e8\", \"\u27e9\", \"\u301c\", \"\uff1a\", \"\uff01\", \"\uff1f\", \"\u266a\", \"\u061b\", \"/\", \"\\\\\", \"\u00ba\", \"\u2212\", \"^\", \"\u02bb\", \"\u02c6\"]\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=\"test\")\n\nwer = load_metric(\"wer.py\") # https://github.com/jonatasgrosman/wav2vec2-sprint/blob/main/wer.py\ncer = load_metric(\"cer.py\") # https://github.com/jonatasgrosman/wav2vec2-sprint/blob/main/cer.py\n\nchars_to_ignore_regex = f\"[{re.escape(''.join(CHARS_TO_IGNORE))}]\"\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\nmodel.to(DEVICE)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n    batch[\"speech\"] = speech_array\n    batch[\"sentence\"] = re.sub(chars_to_ignore_regex, \"\", batch[\"sentence\"]).upper()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef evaluate(batch):\n    inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\n    with torch.no_grad():\n        logits = model(inputs.input_values.to(DEVICE), attention_mask=inputs.attention_mask.to(DEVICE)).logits\n\n    pred_ids = torch.argmax(logits, dim=-1)\n    batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n    return batch\n\nresult = test_dataset.map(evaluate, batched=True, batch_size=8)\n\npredictions = [x.upper() for x in result[\"pred_strings\"]]\nreferences = [x.upper() for x in result[\"sentence\"]]\n\nprint(f\"WER: {wer.compute(predictions=predictions, references=references, chunk_size=1000) * 100}\")\nprint(f\"CER: {cer.compute(predictions=predictions, references=references, chunk_size=1000) * 100}\")\n```\n\n**Test Result**:\n\nIn the table below I report the Word Error Rate (WER) and the Character Error Rate (CER) of the model. I ran the evaluation script described above on other models as well (on 2021-04-22). Note that the table below may show different results from those already reported, this may have been caused due to some specificity of the other evaluation scripts used.\n\n| Model | WER | CER |\n| ------------- | ------------- | ------------- |\n| jonatasgrosman/wav2vec2-large-xlsr-53-persian | **30.12%** | **7.37%** |\n| m3hrdadfi/wav2vec2-large-xlsr-persian-v2 | 33.85% | 8.79% |\n| m3hrdadfi/wav2vec2-large-xlsr-persian | 34.37% | 8.98% |\n\n## Citation\nIf you want to cite this model you can use this:\n\n```bibtex\n@misc{grosman2021xlsr53-large-persian,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {P}ersian},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-persian}},\n  year={2021}\n}\n```"
    },
    {
      "@id": "ark:59852/model-zhihan1996-dnabert-s-uka1jryl9nb",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "zhihan1996/DNABERT-S",
      "description": "No description provided",
      "author": "zhihan1996",
      "keywords": [
        "transformers",
        "pytorch",
        "bert",
        "feature-extraction",
        "biology",
        "genomics",
        "custom_code",
        "license:apache-2.0",
        "text-embeddings-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [],
      "contentUrl": "https://huggingface.co/zhihan1996/DNABERT-S/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/zhihan1996/DNABERT-S",
      "license": "apache-2.0",
      "isPartOf": []
    },
    {
      "@id": "ark:59852/model-sentence-transformers-multi-qa-mpnet-base-dot-v1-fuk3yqj9zyu",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "sentence-transformers/multi-qa-mpnet-base-dot-v1",
      "description": "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for **semantic search**. It has been trained on 215M (question, answer) pairs from diverse sources. For an introduction to semantic search, have a look at: [SBERT.net - Semantic Search](https://www.sbert.net/examples/applications/semantic-search/README.html)",
      "author": "sentence-transformers",
      "keywords": [
        "sentence-transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "openvino",
        "mpnet",
        "fill-mask",
        "feature-extraction",
        "sentence-similarity",
        "transformers",
        "text-embeddings-inference",
        "en",
        "dataset:flax-sentence-embeddings/stackexchange_xml",
        "dataset:ms_marco",
        "dataset:gooaq",
        "dataset:yahoo_answers_topics",
        "dataset:search_qa",
        "dataset:eli5",
        "dataset:natural_questions",
        "dataset:trivia_qa",
        "dataset:embedding-data/QQP",
        "dataset:embedding-data/PAQ_pairs",
        "dataset:embedding-data/Amazon-QA",
        "dataset:embedding-data/WikiAnswers",
        "autotrain_compatible",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml"
        },
        {
          "@id": "https://huggingface.co/datasets/ms_marco"
        },
        {
          "@id": "https://huggingface.co/datasets/gooaq"
        },
        {
          "@id": "https://huggingface.co/datasets/yahoo_answers_topics"
        },
        {
          "@id": "https://huggingface.co/datasets/search_qa"
        },
        {
          "@id": "https://huggingface.co/datasets/eli5"
        },
        {
          "@id": "https://huggingface.co/datasets/natural_questions"
        },
        {
          "@id": "https://huggingface.co/datasets/trivia_qa"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/QQP"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/PAQ_pairs"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/Amazon-QA"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/WikiAnswers"
        }
      ],
      "intendedUseCase": "Our model is intended to be used for semantic search: It encodes queries / questions and text paragraphs in a dense vector space. It finds relevant documents for the given passages.\n\nNote that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text.",
      "usageInformation": "Using this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer, util\n\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]",
      "contentUrl": "https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1/resolve/main/model.safetensors",
      "url": "https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1",
      "isPartOf": [],
      "README": "\n# multi-qa-mpnet-base-dot-v1\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for **semantic search**. It has been trained on 215M (question, answer) pairs from diverse sources. For an introduction to semantic search, have a look at: [SBERT.net - Semantic Search](https://www.sbert.net/examples/applications/semantic-search/README.html)\n\n\n## Usage (Sentence-Transformers)\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer, util\n\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n\n# Load the model\nmodel = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\n\n# Encode query and documents\nquery_emb = model.encode(query)\ndoc_emb = model.encode(docs)\n\n# Compute dot score between query and all document embeddings\nscores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n\n# Combine docs & scores\ndoc_score_pairs = list(zip(docs, scores))\n\n# Sort by decreasing score\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n\n# Output passages & scores\nfor doc, score in doc_score_pairs:\n    print(score, doc)\n```\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the correct pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n# CLS Pooling - Take output from first token\ndef cls_pooling(model_output):\n    return model_output.last_hidden_state[:,0]\n\n# Encode text\ndef encode(texts):\n    # Tokenize sentences\n    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n\n    # Compute token embeddings\n    with torch.no_grad():\n        model_output = model(**encoded_input, return_dict=True)\n\n    # Perform pooling\n    embeddings = cls_pooling(model_output)\n\n    return embeddings\n\n\n# Sentences we want sentence embeddings for\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\nmodel = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\n\n# Encode query and docs\nquery_emb = encode(query)\ndoc_emb = encode(docs)\n\n# Compute dot score between query and all document embeddings\nscores = torch.mm(query_emb, doc_emb.transpose(0, 1))[0].cpu().tolist()\n\n# Combine docs & scores\ndoc_score_pairs = list(zip(docs, scores))\n\n# Sort by decreasing score\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n\n# Output passages & scores\nfor doc, score in doc_score_pairs:\n    print(score, doc)\n```\n\n## Usage (Text Embeddings Inference (TEI))\n\n[Text Embeddings Inference (TEI)](https://github.com/huggingface/text-embeddings-inference) is a blazing fast inference solution for text embedding models.\n\n- CPU:\n```bash\ndocker run -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cpu-latest \\\n  --model-id sentence-transformers/multi-qa-mpnet-base-dot-v1 \\\n  --pooling cls \\\n  --dtype float16\n```\n\n- NVIDIA GPU:\n```bash\ndocker run --gpus all -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cuda-latest \\\n  --model-id sentence-transformers/multi-qa-mpnet-base-dot-v1 \\\n  --pooling cls \\\n  --dtype float16\n```\n\nSend a request to `/v1/embeddings` to generate embeddings via the [OpenAI Embeddings API](https://platform.openai.com/docs/api-reference/embeddings/create):\n```bash\ncurl http://localhost:8080/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"sentence-transformers/multi-qa-mpnet-base-dot-v1\",\n    \"input\": \"How many people live in London?\"\n  }'\n```\n\nOr check the [Text Embeddings Inference API specification](https://huggingface.github.io/text-embeddings-inference/) instead.\n\n----\n\n## Technical Details\n\nIn the following some technical details how this model must be used:\n\n| Setting | Value |\n| --- | :---: |\n| Dimensions | 768 |\n| Produces normalized embeddings | No |\n| Pooling-Method | CLS pooling |\n| Suitable score functions | dot-product (e.g. `util.dot_score`) |\n\n----\n\n## Background\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developed this model during the \n[Community week using JAX/Flax for NLP & CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), \norganized by Hugging Face. We developed this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Google's Flax, JAX, and Cloud team members about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intended to be used for semantic search: It encodes queries / questions and text paragraphs in a dense vector space. It finds relevant documents for the given passages.\n\nNote that there is a limit of 512 word pieces: Text longer than that will be truncated. Further note that the model was just trained on input text up to 250 word pieces. It might not work well for longer text. \n\n## Training procedure\n\nThe full training script is accessible in this current repository: `train_script.py`.\n\n### Pre-training \n\nWe use the pretrained [`mpnet-base`](https://huggingface.co/microsoft/mpnet-base) model. Please refer to the model card for more detailed information about the pre-training procedure.\n\n#### Training\n\nWe use the concatenation from multiple datasets to fine-tune our model. In total we have about 215M (question, answer) pairs.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\nThe model was trained with [MultipleNegativesRankingLoss](https://www.sbert.net/docs/package_reference/losses.html#multiplenegativesrankingloss) using CLS-pooling, dot-product as similarity function, and a scale of 1.\n\n| Dataset                    | Number of training tuples  |\n|--------------------------------------------------------|:--------------------------:|\n| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs from WikiAnswers |  77,427,422 |\n| [PAQ](https://github.com/facebookresearch/PAQ) Automatically generated (Question, Paragraph) pairs for each paragraph in Wikipedia | 64,371,441 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs from all StackExchanges  | 25,316,456 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Answer) pairs from all StackExchanges  |  21,396,559 |\n| [MS MARCO](https://microsoft.github.io/msmarco/) Triplets (query, answer, hard_negative) for 500k queries from Bing search engine |  17,579,773 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) (query, answer) pairs for 3M Google queries and Google featured snippet  | 3,012,496 |\n| [Amazon-QA](http://jmcauley.ucsd.edu/data/amazon/qa/) (Question, Answer) pairs from Amazon product pages | 2,448,839 \n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) pairs from Yahoo Answers | 1,198,260 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) pairs from Yahoo Answers | 681,164 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) pairs from Yahoo Answers | 659,896 |\n| [SearchQA](https://huggingface.co/datasets/search_qa) (Question, Answer) pairs for 140k questions, each with Top5 Google snippets on that question | 582,261 |\n| [ELI5](https://huggingface.co/datasets/eli5) (Question, Answer) pairs from Reddit ELI5 (explainlikeimfive) | 325,475 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions pairs (titles) | 304,525 |\n| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) (Question, Duplicate_Question, Hard_Negative) triplets for Quora Questions Pairs dataset | 103,663 |\n| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) (Question, Paragraph) pairs for 100k real Google queries with relevant Wikipedia paragraph | 100,231 |\n| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) (Question, Paragraph) pairs from SQuAD2.0 dataset |  87,599 |\n| [TriviaQA](https://huggingface.co/datasets/trivia_qa) (Question, Evidence) pairs | 73,346 |\n| **Total** | **214,988,242** |"
    },
    {
      "@id": "ark:59852/model-google-bert-bert-base-multilingual-uncased-n07phowqall",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "google-bert/bert-base-multilingual-uncased",
      "description": "Pretrained model on the top 102 languages with the largest Wikipedia using a masked language modeling (MLM) objective.",
      "author": "google-bert",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "safetensors",
        "bert",
        "fill-mask",
        "multilingual",
        "af",
        "sq",
        "ar",
        "an",
        "hy",
        "ast",
        "az",
        "ba",
        "eu",
        "bar",
        "be",
        "bn",
        "inc",
        "bs",
        "br",
        "bg",
        "my",
        "ca",
        "ceb",
        "ce",
        "zh",
        "cv",
        "hr",
        "cs",
        "da",
        "nl",
        "en",
        "et",
        "fi",
        "fr",
        "gl",
        "ka",
        "de",
        "el",
        "gu",
        "ht",
        "he",
        "hi",
        "hu",
        "is",
        "io",
        "id",
        "ga",
        "it",
        "ja",
        "jv",
        "kn",
        "kk",
        "ky",
        "ko",
        "la",
        "lv",
        "lt",
        "roa",
        "nds",
        "lm",
        "mk",
        "mg",
        "ms",
        "ml",
        "mr",
        "min",
        "ne",
        "new",
        "nb",
        "nn",
        "oc",
        "fa",
        "pms",
        "pl",
        "pt",
        "pa",
        "ro",
        "ru",
        "sco",
        "sr",
        "scn",
        "sk",
        "sl",
        "aze",
        "es",
        "su",
        "sw",
        "sv",
        "tl",
        "tg",
        "ta",
        "tt",
        "te",
        "tr",
        "uk",
        "ud",
        "uz",
        "vi",
        "vo",
        "war",
        "cy",
        "fry",
        "pnb",
        "yo",
        "dataset:wikipedia",
        "arxiv:1810.04805",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/wikipedia"
        }
      ],
      "hasBias": "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-multilingual-uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the man worked as a teacher. [SEP]',\n  'score': 0.07943806052207947,\n  'token': 21733,\n  'token_str': 'teacher'},\n {'sequence': '[CLS] the man worked as a lawyer. [SEP]',\n  'score': 0.0629938617348671,\n  'token': 34249,\n  'token_str': 'lawyer'},\n {'sequence': '[CLS] the man worked as a farmer. [SEP]',\n  'score': 0.03367974981665611,\n  'token': 36799,\n  'token_str': 'farmer'},\n {'sequence': '[CLS] the man worked as a journalist. [SEP]',\n  'score': 0.03172805905342102,\n  'token': 19477,\n  'token_str': 'journalist'},\n {'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n  'score': 0.031021825969219208,\n  'token': 33241,\n  'token_str': 'carpenter'}]\n\n>>> unmasker(\"The Black woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the black woman worked as a nurse. [SEP]',\n  'score': 0.07045423984527588,\n  'token': 52428,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the black woman worked as a teacher. [SEP]',\n  'score': 0.05178029090166092,\n  'token': 21733,\n  'token_str': 'teacher'},\n {'sequence': '[CLS] the black woman worked as a lawyer. [SEP]',\n  'score': 0.032601192593574524,\n  'token': 34249,\n  'token_str': 'lawyer'},\n {'sequence': '[CLS] the black woman worked as a slave. [SEP]',\n  'score': 0.030507225543260574,\n  'token': 31173,\n  'token_str': 'slave'},\n {'sequence': '[CLS] the black woman worked as a woman. [SEP]',\n  'score': 0.027691684663295746,\n  'token': 14050,\n  'token_str': 'woman'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.",
      "intendedUseCase": "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-multilingual-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a top model. [SEP]\",\n  'score': 0.1507750153541565,\n  'token': 11397,\n  'token_str': 'top'},\n {'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.13075384497642517,\n  'token': 23589,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a good model. [SEP]\",\n  'score': 0.036272723227739334,\n  'token': 12050,\n  'token_str': 'good'},\n {'sequence': \"[CLS] hello i'm a new model. [SEP]\",\n  'score': 0.035954564809799194,\n  'token': 10246,\n  'token_str': 'new'},\n {'sequence': \"[CLS] hello i'm a great model. [SEP]\",\n  'score': 0.028643041849136353,\n  'token': 11838,\n  'token_str': 'great'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-multilingual-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-multilingual-uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the man worked as a teacher. [SEP]',\n  'score': 0.07943806052207947,\n  'token': 21733,\n  'token_str': 'teacher'},\n {'sequence': '[CLS] the man worked as a lawyer. [SEP]',\n  'score': 0.0629938617348671,\n  'token': 34249,\n  'token_str': 'lawyer'},\n {'sequence': '[CLS] the man worked as a farmer. [SEP]',\n  'score': 0.03367974981665611,\n  'token': 36799,\n  'token_str': 'farmer'},\n {'sequence': '[CLS] the man worked as a journalist. [SEP]',\n  'score': 0.03172805905342102,\n  'token': 19477,\n  'token_str': 'journalist'},\n {'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n  'score': 0.031021825969219208,\n  'token': 33241,\n  'token_str': 'carpenter'}]\n\n>>> unmasker(\"The Black woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the black woman worked as a nurse. [SEP]',\n  'score': 0.07045423984527588,\n  'token': 52428,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the black woman worked as a teacher. [SEP]',\n  'score': 0.05178029090166092,\n  'token': 21733,\n  'token_str': 'teacher'},\n {'sequence': '[CLS] the black woman worked as a lawyer. [SEP]',\n  'score': 0.032601192593574524,\n  'token': 34249,\n  'token_str': 'lawyer'},\n {'sequence': '[CLS] the black woman worked as a slave. [SEP]',\n  'score': 0.030507225543260574,\n  'token': 31173,\n  'token_str': 'slave'},\n {'sequence': '[CLS] the black woman worked as a woman. [SEP]',\n  'score': 0.027691684663295746,\n  'token': 14050,\n  'token_str': 'woman'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.",
      "usageInformation": "You can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-multilingual-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a top model. [SEP]\",\n  'score': 0.1507750153541565,\n  'token': 11397,\n  'token_str': 'top'},\n {'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.13075384497642517,\n  'token': 23589,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a good model. [SEP]\",\n  'score': 0.036272723227739334,\n  'token': 12050,\n  'token_str': 'good'},\n {'sequence': \"[CLS] hello i'm a new model. [SEP]\",\n  'score': 0.035954564809799194,\n  'token': 10246,\n  'token_str': 'new'},\n {'sequence': \"[CLS] hello i'm a great model. [SEP]\",\n  'score': 0.028643041849136353,\n  'token': 11838,\n  'token_str': 'great'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-multilingual-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```",
      "contentUrl": "https://huggingface.co/google-bert/bert-base-multilingual-uncased/resolve/main/model.safetensors",
      "url": "https://huggingface.co/google-bert/bert-base-multilingual-uncased",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# BERT multilingual base model (uncased)\n\nPretrained model on the top 102 languages with the largest Wikipedia using a masked language modeling (MLM) objective.\nIt was introduced in [this paper](https://arxiv.org/abs/1810.04805) and first released in\n[this repository](https://github.com/google-research/bert). This model is uncased: it does not make a difference\nbetween english and English.\n\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nBERT is a transformers model pretrained on a large corpus of multilingual data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the languages in the training set that can then be used to\nextract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a\nstandard classifier using the features produced by the BERT model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-multilingual-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a top model. [SEP]\",\n  'score': 0.1507750153541565,\n  'token': 11397,\n  'token_str': 'top'},\n {'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.13075384497642517,\n  'token': 23589,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a good model. [SEP]\",\n  'score': 0.036272723227739334,\n  'token': 12050,\n  'token_str': 'good'},\n {'sequence': \"[CLS] hello i'm a new model. [SEP]\",\n  'score': 0.035954564809799194,\n  'token': 10246,\n  'token_str': 'new'},\n {'sequence': \"[CLS] hello i'm a great model. [SEP]\",\n  'score': 0.028643041849136353,\n  'token': 11838,\n  'token_str': 'great'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-multilingual-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-multilingual-uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the man worked as a teacher. [SEP]',\n  'score': 0.07943806052207947,\n  'token': 21733,\n  'token_str': 'teacher'},\n {'sequence': '[CLS] the man worked as a lawyer. [SEP]',\n  'score': 0.0629938617348671,\n  'token': 34249,\n  'token_str': 'lawyer'},\n {'sequence': '[CLS] the man worked as a farmer. [SEP]',\n  'score': 0.03367974981665611,\n  'token': 36799,\n  'token_str': 'farmer'},\n {'sequence': '[CLS] the man worked as a journalist. [SEP]',\n  'score': 0.03172805905342102,\n  'token': 19477,\n  'token_str': 'journalist'},\n {'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n  'score': 0.031021825969219208,\n  'token': 33241,\n  'token_str': 'carpenter'}]\n\n>>> unmasker(\"The Black woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the black woman worked as a nurse. [SEP]',\n  'score': 0.07045423984527588,\n  'token': 52428,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the black woman worked as a teacher. [SEP]',\n  'score': 0.05178029090166092,\n  'token': 21733,\n  'token_str': 'teacher'},\n {'sequence': '[CLS] the black woman worked as a lawyer. [SEP]',\n  'score': 0.032601192593574524,\n  'token': 34249,\n  'token_str': 'lawyer'},\n {'sequence': '[CLS] the black woman worked as a slave. [SEP]',\n  'score': 0.030507225543260574,\n  'token': 31173,\n  'token_str': 'slave'},\n {'sequence': '[CLS] the black woman worked as a woman. [SEP]',\n  'score': 0.027691684663295746,\n  'token': 14050,\n  'token_str': 'woman'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe BERT model was pretrained on the 102 languages with the largest Wikipedias. You can find the complete list\n[here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a shared vocabulary size of 110,000. The languages with a\nlarger Wikipedia are under-sampled and the ones with lower resources are oversampled. For languages like Chinese,\nJapanese Kanji and Korean Hanja that don't have space, a CJK Unicode block is added around every character. \n\nThe inputs of the model are then of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n"
    },
    {
      "@id": "ark:59852/model-baai-bge-large-en-v1-5-uka1jril9hn",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "BAAI/bge-large-en-v1.5",
      "description": "<h1 align=\"center\">FlagEmbedding</h1>",
      "author": "BAAI",
      "keywords": [
        "sentence-transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "bert",
        "feature-extraction",
        "sentence-similarity",
        "transformers",
        "mteb",
        "en",
        "arxiv:2401.03462",
        "arxiv:2312.15503",
        "arxiv:2311.13534",
        "arxiv:2310.07554",
        "arxiv:2309.07597",
        "license:mit",
        "model-index",
        "autotrain_compatible",
        "text-embeddings-inference",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "usageInformation": "Here are some examples for using `bge` models with \n[FlagEmbedding](#using-flagembedding), [Sentence-Transformers](#using-sentence-transformers), [Langchain](#using-langchain), or [Huggingface Transformers](#using-huggingface-transformers).\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\nIf it doesn't work for you, you can see [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md) for more methods to install FlagEmbedding.\n\n```python\nfrom FlagEmbedding import FlagModel\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = FlagModel('BAAI/bge-large-zh-v1.5', \n                  query_instruction_for_retrieval=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\",\n                  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)",
      "contentUrl": "https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/model.safetensors",
      "url": "https://huggingface.co/BAAI/bge-large-en-v1.5",
      "license": "mit",
      "isPartOf": [],
      "README": "\n\n<h1 align=\"center\">FlagEmbedding</h1>\n\n\n<h4 align=\"center\">\n    <p>\n        <a href=#model-list>Model List</a> | \n        <a href=#frequently-asked-questions>FAQ</a> |\n        <a href=#usage>Usage</a>  |\n        <a href=\"#evaluation\">Evaluation</a> |\n        <a href=\"#train\">Train</a> |\n        <a href=\"#contact\">Contact</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\nFor more details please refer to our Github: [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding).\n\nIf you are looking for a model that supports more languages, longer texts, and other retrieval methods, you can try using [bge-m3](https://huggingface.co/BAAI/bge-m3).\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\nFlagEmbedding focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n- **Long-Context LLM**: [Activation Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon)\n- **Fine-tuning of LM** : [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail)\n- **Dense Retrieval**: [BGE-M3](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3), [LLM Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), [BGE Embedding](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding)\n- **Reranker Model**: [BGE Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n- **Benchmark**: [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)\n\n## News \n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model that supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/BGE_M3/BGE_M3.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) :fire:\n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) :fire:\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) :fire:  \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) and [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n \n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n    \n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n</details>\n\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n|              Model              | Language | | Description | query instruction for retrieval [1] |\n|:-------------------------------|:--------:| :--------:| :--------:|:--------:|\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     |    [Inference](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3#usage) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3)    | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n|  [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)  |   English | [Inference](./FlagEmbedding/llm_embedder/README.md) [Fine-tune](./FlagEmbedding/llm_embedder/README.md) | a unified embedding model to support diverse retrieval augmentation needs for LLMs | See [README](./FlagEmbedding/llm_embedder/README.md) |\n|  [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)  |   Chinese and English | [Inference](#usage-for-reranker) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) | a cross-encoder model which is more accurate but less efficient [2] |   |\n|  [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) |   Chinese and English | [Inference](#usage-for-reranker) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) | a cross-encoder model which is more accurate but less efficient [2] |   |\n|  [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution  | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5) |   Chinese |  [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | :trophy: rank **1st** in [MTEB](https://huggingface.co/spaces/mteb/leaderboard) leaderboard | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a base-scale model but with similar ability to `bge-large-en` | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) |a small-scale model but with competitive performance  | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | :trophy: rank **1st** in [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB) benchmark | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) |   Chinese |  [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a base-scale model but with similar ability to `bge-large-zh` | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a small-scale model but with competitive performance | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n\n[1\\]: If you need to search the relevant passages to a query, we suggest to add the instruction to the query; in other cases, no instruction is needed, just use the original query directly. In all cases, **no instruction** needs to be added to passages.\n\n[2\\]: Different from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. To balance the accuracy and time cost, cross-encoder is widely used to re-rank top-k documents retrieved by other simple models. \nFor examples, use bge embedding model to retrieve top 100 relevant documents, and then use bge reranker to re-rank the top 100 document to get the final top-3 results.\n\nAll models have been uploaded to Huggingface Hub, and you can see them at https://huggingface.co/BAAI. \nIf you cannot open the Huggingface Hub, you also can download the models at https://model.baai.ac.cn/models .\n\n\n## Frequently asked questions\n\n<details>\n  <summary>1. How to fine-tune bge embedding model?</summary>\n\n  <!-- ### How to fine-tune bge embedding model? -->\nFollowing this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) to prepare data and fine-tune your model. \nSome suggestions:\n- Mine hard negatives following this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune#hard-negatives), which can improve the retrieval performance.\n- If you pre-train bge on your data, the pre-trained model cannot be directly used to calculate similarity, and it must be fine-tuned with contrastive learning before computing similarity.\n- If the accuracy of the fine-tuned model is still not high, it is recommended to use/fine-tune the cross-encoder model (bge-reranker) to re-rank top-k results. Hard negatives also are needed to fine-tune reranker.\n\n  \n</details>\n\n<details>\n  <summary>2. The similarity score between two dissimilar sentences is higher than 0.5</summary>\n\n  <!-- ### The similarity score between two dissimilar sentences is higher than 0.5 -->\n**Suggest to use bge v1.5, which alleviates the issue of the similarity distribution.** \n\nSince we finetune the models by contrastive learning with a temperature of 0.01, \nthe similarity distribution of the current BGE model is about in the interval \\[0.6, 1\\].\nSo a similarity score greater than 0.5 does not indicate that the two sentences are similar.\n\nFor downstream tasks, such as passage retrieval or semantic similarity, \n**what matters is the relative order of the scores, not the absolute value.**\nIf you need to filter similar sentences based on a similarity threshold, \nplease select an appropriate similarity threshold based on the similarity distribution on your data (such as 0.8, 0.85, or even 0.9).\n\n</details>\n\n<details>\n  <summary>3. When does the query instruction need to be used</summary>\n\n  <!-- ### When does the query instruction need to be used -->\n\nFor the `bge-*-v1.5`, we improve its retrieval ability when not using instruction. \nNo instruction only has a slight degradation in retrieval performance compared with using instruction. \nSo you can generate embedding without instruction in all cases for convenience.\n \nFor a retrieval task that uses short queries to find long related documents, \nit is recommended to add instructions for these short queries.\n**The best method to decide whether to add instructions for queries is choosing the setting that achieves better performance on your task.**\nIn all cases, the documents/passages do not need to add the instruction. \n\n</details>\n\n\n## Usage \n\n### Usage for Embedding Model\n\nHere are some examples for using `bge` models with \n[FlagEmbedding](#using-flagembedding), [Sentence-Transformers](#using-sentence-transformers), [Langchain](#using-langchain), or [Huggingface Transformers](#using-huggingface-transformers).\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\nIf it doesn't work for you, you can see [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md) for more methods to install FlagEmbedding.\n\n```python\nfrom FlagEmbedding import FlagModel\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = FlagModel('BAAI/bge-large-zh-v1.5', \n                  query_instruction_for_retrieval=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\",\n                  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n\n# for s2p(short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\nqueries = ['query_1', 'query_2']\npassages = [\"\u6837\u4f8b\u6587\u6863-1\", \"\u6837\u4f8b\u6587\u6863-2\"]\nq_embeddings = model.encode_queries(queries)\np_embeddings = model.encode(passages)\nscores = q_embeddings @ p_embeddings.T\n```\nFor the value of the argument `query_instruction_for_retrieval`, see [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list). \n\nBy default, FlagModel will use all available GPUs when encoding. Please set `os.environ[\"CUDA_VISIBLE_DEVICES\"]` to select specific GPUs.\nYou also can set `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"` to make all GPUs unavailable.\n\n\n#### Using Sentence-Transformers\n\nYou can also use the `bge` models with [sentence-transformers](https://www.SBERT.net):\n\n```\npip install -U sentence-transformers\n```\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nembeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\nembeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\nFor s2p(short query to long passage) retrieval task, \neach short query should start with an instruction (instructions see [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list)). \nBut the instruction is not needed for passages.\n```python\nfrom sentence_transformers import SentenceTransformer\nqueries = ['query_1', 'query_2']\npassages = [\"\u6837\u4f8b\u6587\u6863-1\", \"\u6837\u4f8b\u6587\u6863-2\"]\ninstruction = \"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nq_embeddings = model.encode([instruction+q for q in queries], normalize_embeddings=True)\np_embeddings = model.encode(passages, normalize_embeddings=True)\nscores = q_embeddings @ p_embeddings.T\n```\n\n#### Using Langchain \n\nYou can use `bge` in langchain like this:\n```python\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nmodel_name = \"BAAI/bge-large-en-v1.5\"\nmodel_kwargs = {'device': 'cuda'}\nencode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\nmodel = HuggingFaceBgeEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs,\n    query_instruction=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n)\nmodel.query_instruction = \"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n```\n\n\n#### Using HuggingFace Transformers\n\nWith the transformers package, you can use the model like this: First, you pass your input through the transformer model, then you select the last hidden state of the first token (i.e., [CLS]) as the sentence embedding.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n# Sentences we want sentence embeddings for\nsentences = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel.eval()\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n    # Perform pooling. In this case, cls pooling.\n    sentence_embeddings = model_output[0][:, 0]\n# normalize embeddings\nsentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\", sentence_embeddings)\n```\n\n#### Usage of the ONNX files\n\n```python\nfrom optimum.onnxruntime import ORTModelForFeatureExtraction  # type: ignore\n\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-en-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-large-en-v1.5', revision=\"refs/pr/13\")\nmodel_ort = ORTModelForFeatureExtraction.from_pretrained('BAAI/bge-large-en-v1.5', revision=\"refs/pr/13\",file_name=\"onnx/model.onnx\")\n\n# Sentences we want sentence embeddings for\nsentences = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n\nmodel_output_ort = model_ort(**encoded_input)\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# model_output and model_output_ort are identical\n\n```\n\nIts also possible to deploy the onnx files with the [infinity_emb](https://github.com/michaelfeil/infinity) pip package.\n```python\nimport asyncio\nfrom infinity_emb import AsyncEmbeddingEngine, EngineArgs\n\nsentences = [\"Embed this is sentence via Infinity.\", \"Paris is in France.\"]\nengine = AsyncEmbeddingEngine.from_args(\n    EngineArgs(model_name_or_path = \"BAAI/bge-large-en-v1.5\", device=\"cpu\", engine=\"optimum\" # or engine=\"torch\"\n))\n\nasync def main(): \n    async with engine:\n        embeddings, usage = await engine.embed(sentences=sentences)\nasyncio.run(main())\n```\n\n### Usage for Reranker\n\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. \nYou can get a relevance score by inputting query and passage to the reranker. \nThe reranker is optimized based cross-entropy loss, so the relevance score is not bounded to a specific range.\n\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\n\nGet relevance scores (higher scores indicate more relevance):\n```python\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\n```\n\n\n#### Using Huggingface transformers\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n```\n\n## Evaluation  \n\n`baai-general-embedding` models achieve **state-of-the-art performance on both MTEB and C-MTEB leaderboard!**\nFor more details and evaluation tools see our [scripts](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md). \n\n- **MTEB**:   \n\n| Model Name |  Dimension | Sequence Length | Average (56) | Retrieval (15) |Clustering (11) | Pair Classification (3) | Reranking (4) |  STS (10) | Summarization (1) | Classification (12) |\n|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) | 1024 | 512 |  **64.23** | **54.29** |  46.08 | 87.12 | 60.03 | 83.11 | 31.61 | 75.97 |  \n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |  768 | 512 | 63.55 | 53.25 |   45.77 | 86.55 | 58.86 | 82.4 | 31.07 | 75.53 |  \n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |  384 | 512 | 62.17 |51.68 | 43.82 |  84.92 | 58.36 | 81.59 | 30.12 | 74.14 |  \n| [bge-large-en](https://huggingface.co/BAAI/bge-large-en) |  1024 | 512 | 63.98 |  53.9 | 46.98 | 85.8 | 59.48 | 81.56 | 32.06 | 76.21 | \n| [bge-base-en](https://huggingface.co/BAAI/bge-base-en) |  768 | 512 |  63.36 | 53.0 | 46.32 | 85.86 | 58.7 | 81.84 | 29.27 | 75.27 | \n| [gte-large](https://huggingface.co/thenlper/gte-large) |  1024 | 512 | 63.13 | 52.22 | 46.84 | 85.00 | 59.13 | 83.35 | 31.66 | 73.33 |\n| [gte-base](https://huggingface.co/thenlper/gte-base) \t|  768 | 512 | 62.39 | 51.14 | 46.2 | 84.57 | 58.61 | 82.3 | 31.17 | 73.01 |\n| [e5-large-v2](https://huggingface.co/intfloat/e5-large-v2) |  1024| 512 | 62.25 | 50.56 | 44.49 | 86.03 | 56.61 | 82.05 | 30.19 | 75.24 |\n| [bge-small-en](https://huggingface.co/BAAI/bge-small-en) |  384 | 512 | 62.11 |  51.82 | 44.31 | 83.78 | 57.97 | 80.72 | 30.53 | 74.37 |  \n| [instructor-xl](https://huggingface.co/hkunlp/instructor-xl) |  768 | 512 | 61.79 | 49.26 | 44.74 | 86.62 | 57.29 | 83.06 | 32.32 | 61.79 |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2) |  768 | 512 | 61.5 | 50.29 | 43.80 | 85.73 | 55.91 | 81.05 | 30.28 | 73.84 |\n| [gte-small](https://huggingface.co/thenlper/gte-small) |  384 | 512 | 61.36 | 49.46 | 44.89 | 83.54 | 57.7 | 82.07 | 30.42 | 72.31 |\n| [text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings) | 1536 | 8192 | 60.99 | 49.25 | 45.9 | 84.89 | 56.32 | 80.97 | 30.8 | 70.93 |\n| [e5-small-v2](https://huggingface.co/intfloat/e5-base-v2) | 384 | 512 | 59.93 | 49.04 | 39.92 | 84.67 | 54.32 | 80.39 | 31.16 | 72.94 |\n| [sentence-t5-xxl](https://huggingface.co/sentence-transformers/sentence-t5-xxl) |  768 | 512 | 59.51 | 42.24 | 43.72 | 85.06 | 56.42 | 82.63 | 30.08 | 73.42 |\n| [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) \t|  768 | 514 \t| 57.78 | 43.81 | 43.69 | 83.04 | 59.36 | 80.28 | 27.49 | 65.07 |\n| [sgpt-bloom-7b1-msmarco](https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco) \t|  4096 | 2048 | 57.59 | 48.22 | 38.93 | 81.9 | 55.65 | 77.74 | 33.6 | 66.19 |\n\n\n\n- **C-MTEB**:  \nWe create the benchmark C-MTEB for Chinese text embedding which consists of 31 datasets from 6 tasks. \nPlease refer to [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md) for a detailed introduction.\n \n| Model | Embedding dimension | Avg | Retrieval | STS | PairClassification | Classification | Reranking | Clustering |\n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n| [**BAAI/bge-large-zh-v1.5**](https://huggingface.co/BAAI/bge-large-zh-v1.5) | 1024 |  **64.53** | 70.46 | 56.25 | 81.6 | 69.13 | 65.84 | 48.99 |  \n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5) | 768 |  63.13 | 69.49 | 53.72 | 79.75 | 68.07 | 65.39 | 47.53 |  \n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) | 512 | 57.82 | 61.77 | 49.11 | 70.41 | 63.96 | 60.92 | 44.18 |   \n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) | 1024 | 64.20 | 71.53 | 54.98 | 78.94 | 68.32 | 65.11 | 48.39 |\n| [bge-large-zh-noinstruct](https://huggingface.co/BAAI/bge-large-zh-noinstruct) | 1024 | 63.53 | 70.55 | 53 | 76.77 | 68.58 | 64.91 | 50.01 |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) | 768 | 62.96 | 69.53 | 54.12 | 77.5 | 67.07 | 64.91 | 47.63 |\n| [multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large) | 1024 | 58.79 | 63.66 | 48.44 | 69.89 | 67.34 | 56.00 | 48.23 |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) | 512 | 58.27 |  63.07 | 49.45 | 70.35 | 63.64 | 61.48 | 45.09 |\n| [m3e-base](https://huggingface.co/moka-ai/m3e-base) | 768 | 57.10 | 56.91 | 50.47 | 63.99 | 67.52 | 59.34 | 47.68 |\n| [m3e-large](https://huggingface.co/moka-ai/m3e-large) | 1024 |  57.05 | 54.75 | 50.42 | 64.3 | 68.2 | 59.66 | 48.88 |\n| [multilingual-e5-base](https://huggingface.co/intfloat/multilingual-e5-base) | 768 | 55.48 | 61.63 | 46.49 | 67.07 | 65.35 | 54.35 | 40.68 |\n| [multilingual-e5-small](https://huggingface.co/intfloat/multilingual-e5-small) | 384 | 55.38 | 59.95 | 45.27 | 66.45 | 65.85 | 53.86 | 45.26 |\n| [text-embedding-ada-002(OpenAI)](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings) | 1536 |  53.02 | 52.0 | 43.35 | 69.56 | 64.31 | 54.28 | 45.68 |\n| [luotuo](https://huggingface.co/silk-road/luotuo-bert-medium) | 1024 | 49.37 |  44.4 | 42.78 | 66.62 | 61 | 49.25 | 44.39 |\n| [text2vec-base](https://huggingface.co/shibing624/text2vec-base-chinese) | 768 |  47.63 | 38.79 | 43.41 | 67.41 | 62.19 | 49.45 | 37.66 |\n| [text2vec-large](https://huggingface.co/GanymedeNil/text2vec-large-chinese) | 1024 | 47.36 | 41.94 | 44.97 | 70.86 | 60.66 | 49.16 | 30.02 |\n\n\n- **Reranking**:\nSee [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/) for evaluation script.\n\n| Model | T2Reranking | T2RerankingZh2En\\* | T2RerankingEn2Zh\\* | MMarcoReranking | CMedQAv1 | CMedQAv2 | Avg |  \n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|  \n| text2vec-base-multilingual | 64.66 | 62.94 | 62.51 | 14.37 | 48.46 | 48.6 | 50.26 |  \n| multilingual-e5-small | 65.62 | 60.94 | 56.41 | 29.91 | 67.26 | 66.54 | 57.78 |  \n| multilingual-e5-large | 64.55 | 61.61 | 54.28 | 28.6 | 67.42 | 67.92 | 57.4 |  \n| multilingual-e5-base | 64.21 | 62.13 | 54.68 | 29.5 | 66.23 | 66.98 | 57.29 |  \n| m3e-base | 66.03 | 62.74 | 56.07 | 17.51 | 77.05 | 76.76 | 59.36 |  \n| m3e-large | 66.13 | 62.72 | 56.1 | 16.46 | 77.76 | 78.27 | 59.57 |  \n| bge-base-zh-v1.5 | 66.49 | 63.25 | 57.02 | 29.74 | 80.47 | 84.88 | 63.64 |  \n| bge-large-zh-v1.5 | 65.74 | 63.39 | 57.03 | 28.74 | 83.45 | 85.44 | 63.97 |  \n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) | 67.28 | 63.95 | 60.45 | 35.46 | 81.26 | 84.1 | 65.42 |  \n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) | 67.6 | 64.03 | 61.44 | 37.16 | 82.15 | 84.18 | 66.09 |  \n\n\\* : T2RerankingZh2En and T2RerankingEn2Zh are cross-language retrieval tasks\n\n## Train\n\n### BAAI Embedding \n\nWe pre-train the models using [retromae](https://github.com/staoxiao/RetroMAE) and train them on large-scale pairs data using contrastive learning. \n**You can fine-tune the embedding model on your data following our [examples](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune).**\nWe also provide a [pre-train example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/pretrain).\nNote that the goal of pre-training is to reconstruct the text, and the pre-trained model cannot be used for similarity calculation directly, it needs to be fine-tuned.\nMore training details for bge see [baai_general_embedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md).\n\n\n\n### BGE Reranker\n\nCross-encoder will perform full-attention over the input pair, \nwhich is more accurate than embedding model (i.e., bi-encoder) but more time-consuming than embedding model.\nTherefore, it can be used to re-rank the top-k documents returned by embedding model.\nWe train the cross-encoder on a multilingual pair data, \nThe data format is the same as embedding model, so you can fine-tune it easily following our [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker). \nMore details please refer to [./FlagEmbedding/reranker/README.md](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n\n\n## Contact\nIf you have any question or suggestion related to this project, feel free to open an issue or pull request.\nYou also can email Shitao Xiao(stxiao@baai.ac.cn) and Zheng Liu(liuzheng@baai.ac.cn). \n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge.\n\n"
    },
    {
      "@id": "ark:59852/model-qwen-qwen3-embedding-0-6b-cejgl97uqez",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "Qwen/Qwen3-Embedding-0.6B",
      "description": "<p align=\"center\">",
      "author": "Qwen",
      "keywords": [
        "sentence-transformers",
        "safetensors",
        "qwen3",
        "text-generation",
        "transformers",
        "sentence-similarity",
        "feature-extraction",
        "text-embeddings-inference",
        "arxiv:2506.05176",
        "base_model:Qwen/Qwen3-0.6B-Base",
        "base_model:finetune:Qwen/Qwen3-0.6B-Base",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-generation",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "intendedUseCase": "**Comprehensive Flexibility**: The Qwen3 Embedding series offers a full spectrum of sizes (from 0.6B to 8B) for both embedding and reranking models, catering to diverse use cases that prioritize efficiency and effectiveness. Developers can seamlessly combine these two modules. Additionally, the embedding model allows for flexible vector definitions across all dimensions, and both embedding and reranking models support user-defined instructions to enhance performance for specific tasks, languages, or scenarios.",
      "usageInformation": "With Transformers versions earlier than 4.51.0, you may encounter the following error:\n```\nKeyError: 'qwen3'\n```\n\n\n```python",
      "baseModel": "Qwen/Qwen3-0.6B-Base",
      "contentUrl": "https://huggingface.co/Qwen/Qwen3-Embedding-0.6B/resolve/main/model.safetensors",
      "url": "https://huggingface.co/Qwen/Qwen3-Embedding-0.6B",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "# Qwen3-Embedding-0.6B\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/logo_qwen3.png\" width=\"400\"/>\n<p>\n\n## Highlights\n\nThe Qwen3 Embedding model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks. Building upon the dense foundational models of the Qwen3 series, it provides a comprehensive range of text embeddings and reranking models in various sizes (0.6B, 4B, and 8B). This series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its foundational model. The Qwen3 Embedding series represents significant advancements in multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bitext mining.\n\n**Exceptional Versatility**: The embedding model has achieved state-of-the-art performance across a wide range of downstream application evaluations. The 8B size embedding model ranks **No.1** in the MTEB multilingual leaderboard (as of June 5, 2025, score **70.58**), while the reranking model excels in various text retrieval scenarios.\n\n**Comprehensive Flexibility**: The Qwen3 Embedding series offers a full spectrum of sizes (from 0.6B to 8B) for both embedding and reranking models, catering to diverse use cases that prioritize efficiency and effectiveness. Developers can seamlessly combine these two modules. Additionally, the embedding model allows for flexible vector definitions across all dimensions, and both embedding and reranking models support user-defined instructions to enhance performance for specific tasks, languages, or scenarios.\n\n**Multilingual Capability**: The Qwen3 Embedding series offer support for over 100 languages, thanks to the multilingual capabilites of Qwen3 models. This includes various programming languages, and provides robust multilingual, cross-lingual, and code retrieval capabilities.\n\n## Model Overview\n\n**Qwen3-Embedding-0.6B** has the following features:\n\n- Model Type: Text Embedding\n- Supported Languages: 100+ Languages\n- Number of Paramaters: 0.6B\n- Context Length: 32k\n- Embedding Dimension: Up to 1024, supports user-defined output dimensions ranging from 32 to 1024\n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3-embedding/), [GitHub](https://github.com/QwenLM/Qwen3-Embedding).\n\n## Qwen3 Embedding Series Model list\n\n| Model Type       | Models               | Size | Layers | Sequence Length | Embedding Dimension | MRL Support | Instruction Aware |\n|------------------|----------------------|------|--------|-----------------|---------------------|-------------|----------------|\n| Text Embedding   | [Qwen3-Embedding-0.6B](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B) | 0.6B | 28     | 32K             | 1024                | Yes         | Yes            |\n| Text Embedding   | [Qwen3-Embedding-4B](https://huggingface.co/Qwen/Qwen3-Embedding-4B)   | 4B   | 36     | 32K             | 2560                | Yes         | Yes            |\n| Text Embedding   | [Qwen3-Embedding-8B](https://huggingface.co/Qwen/Qwen3-Embedding-8B)   | 8B   | 36     | 32K             | 4096                | Yes         | Yes            |\n| Text Reranking   | [Qwen3-Reranker-0.6B](https://huggingface.co/Qwen/Qwen3-Reranker-0.6B) | 0.6B | 28     | 32K             | -                   | -           | Yes            |\n| Text Reranking   | [Qwen3-Reranker-4B](https://huggingface.co/Qwen/Qwen3-Reranker-4B)   | 4B   | 36     | 32K             | -                   | -           | Yes            |\n| Text Reranking   | [Qwen3-Reranker-8B](https://huggingface.co/Qwen/Qwen3-Reranker-8B)   | 8B   | 36     | 32K             | -                   | -           | Yes            |\n\n> **Note**:\n> - `MRL Support` indicates whether the embedding model supports custom dimensions for the final embedding. \n> - `Instruction Aware` notes whether the embedding or reranking model supports customizing the input instruction according to different tasks.\n> - Our evaluation indicates that, for most downstream tasks, using instructions (instruct) typically yields an improvement of 1% to 5% compared to not using them. Therefore, we recommend that developers create tailored instructions specific to their tasks and scenarios. In multilingual contexts, we also advise users to write their instructions in English, as most instructions utilized during the model training process were originally written in English.\n\n## Usage\n\nWith Transformers versions earlier than 4.51.0, you may encounter the following error:\n```\nKeyError: 'qwen3'\n```\n\n### Sentence Transformers Usage\n\n```python\n# Requires transformers>=4.51.0\n# Requires sentence-transformers>=2.7.0\n\nfrom sentence_transformers import SentenceTransformer\n\n# Load the model\nmodel = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving,\n# together with setting `padding_side` to \"left\":\n# model = SentenceTransformer(\n#     \"Qwen/Qwen3-Embedding-0.6B\",\n#     model_kwargs={\"attn_implementation\": \"flash_attention_2\", \"device_map\": \"auto\"},\n#     tokenizer_kwargs={\"padding_side\": \"left\"},\n# )\n\n# The queries and documents to embed\nqueries = [\n    \"What is the capital of China?\",\n    \"Explain gravity\",\n]\ndocuments = [\n    \"The capital of China is Beijing.\",\n    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n]\n\n# Encode the queries and documents. Note that queries benefit from using a prompt\n# Here we use the prompt called \"query\" stored under `model.prompts`, but you can\n# also pass your own prompt via the `prompt` argument\nquery_embeddings = model.encode(queries, prompt_name=\"query\")\ndocument_embeddings = model.encode(documents)\n\n# Compute the (cosine) similarity between the query and document embeddings\nsimilarity = model.similarity(query_embeddings, document_embeddings)\nprint(similarity)\n# tensor([[0.7646, 0.1414],\n#         [0.1355, 0.6000]])\n```\n\n### Transformers Usage\n\n```python\n# Requires transformers>=4.51.0\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef last_token_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n    if left_padding:\n        return last_hidden_states[:, -1]\n    else:\n        sequence_lengths = attention_mask.sum(dim=1) - 1\n        batch_size = last_hidden_states.shape[0]\n        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n\n\ndef get_detailed_instruct(task_description: str, query: str) -> str:\n    return f'Instruct: {task_description}\\nQuery:{query}'\n\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\n\nqueries = [\n    get_detailed_instruct(task, 'What is the capital of China?'),\n    get_detailed_instruct(task, 'Explain gravity')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n    \"The capital of China is Beijing.\",\n    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ninput_texts = queries + documents\n\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-0.6B', padding_side='left')\nmodel = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B')\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n# model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B', attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16).cuda()\n\nmax_length = 8192\n\n# Tokenize the input texts\nbatch_dict = tokenizer(\n    input_texts,\n    padding=True,\n    truncation=True,\n    max_length=max_length,\n    return_tensors=\"pt\",\n)\nbatch_dict.to(model.device)\noutputs = model(**batch_dict)\nembeddings = last_token_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.7645568251609802, 0.14142508804798126], [0.13549736142158508, 0.5999549627304077]]\n```\n\n### vLLM Usage\n\n```python\n# Requires vllm>=0.8.5\nimport torch\nimport vllm\nfrom vllm import LLM\n\ndef get_detailed_instruct(task_description: str, query: str) -> str:\n    return f'Instruct: {task_description}\\nQuery:{query}'\n\n# Each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\n\nqueries = [\n    get_detailed_instruct(task, 'What is the capital of China?'),\n    get_detailed_instruct(task, 'Explain gravity')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n    \"The capital of China is Beijing.\",\n    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n]\ninput_texts = queries + documents\n\nmodel = LLM(model=\"Qwen/Qwen3-Embedding-0.6B\", task=\"embed\")\n\noutputs = model.embed(input_texts)\nembeddings = torch.tensor([o.outputs.embedding for o in outputs])\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.7620252966880798, 0.14078938961029053], [0.1358368694782257, 0.6013815999031067]]\n```\n\n\ud83d\udccc **Tip**: We recommend that developers customize the `instruct` according to their specific scenarios, tasks, and languages. Our tests have shown that in most retrieval scenarios, not using an `instruct` on the query side can lead to a drop in retrieval performance by approximately 1% to 5%.\n\n### Text Embeddings Inference (TEI) Usage\n\nYou can either run / deploy TEI on NVIDIA GPUs as:\n\n```bash\ndocker run --gpus all -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cpu-1.7.2 --model-id Qwen/Qwen3-Embedding-0.6B --dtype float16\n```\n\nOr on CPU devices as:\n\n```bash\ndocker run -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.7.2 --model-id Qwen/Qwen3-Embedding-0.6B\n```\n\nAnd then, generate the embeddings sending a HTTP POST request as:\n\n```bash\ncurl http://localhost:8080/embed \\\n    -X POST \\\n    -d '{\"inputs\": [\"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: What is the capital of China?\", \"Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery: Explain gravity\"]}' \\\n    -H \"Content-Type: application/json\"\n```\n\n## Evaluation\n\n### MTEB (Multilingual)\n\n| Model                            |  Size   |  Mean (Task)  | Mean (Type) | Bitxt Mining | Class. | Clust. | Inst. Retri. | Multi. Class. | Pair. Class. | Rerank | Retri. | STS  |\n|----------------------------------|:-------:|:-------------:|:-------------:|:--------------:|:--------:|:--------:|:--------------:|:---------------:|:--------------:|:--------:|:--------:|:------:|\n| NV-Embed-v2                      |   7B    |     56.29     | 49.58       | 57.84        | 57.29  | 40.80  | 1.04         | 18.63         | 78.94        | 63.82  | 56.72  | 71.10|\n| GritLM-7B                        |   7B    |     60.92     | 53.74       | 70.53        | 61.83  | 49.75  | 3.45         | 22.77         | 79.94        | 63.78  | 58.31  | 73.33|\n| BGE-M3                           |  0.6B   |     59.56     | 52.18       | 79.11        | 60.35  | 40.88  | -3.11        | 20.1          | 80.76        | 62.79  | 54.60  | 74.12|\n| multilingual-e5-large-instruct   |  0.6B   |     63.22     | 55.08       | 80.13        | 64.94  | 50.75  | -0.40        | 22.91         | 80.86        | 62.61  | 57.12  | 76.81|\n| gte-Qwen2-1.5B-instruct          |  1.5B   |     59.45     | 52.69       | 62.51        | 58.32  | 52.05  | 0.74         | 24.02         | 81.58        | 62.58  | 60.78  | 71.61|\n| gte-Qwen2-7b-Instruct            |   7B    |     62.51     | 55.93       | 73.92        | 61.55  | 52.77  | 4.94         | 25.48         | 85.13        | 65.55  | 60.08  | 73.98|\n| text-embedding-3-large           |    -    |     58.93     | 51.41       | 62.17        | 60.27  | 46.89  | -2.68        | 22.03         | 79.17        | 63.89  | 59.27  | 71.68|\n| Cohere-embed-multilingual-v3.0   |    -    |     61.12     | 53.23       | 70.50        | 62.95  | 46.89  | -1.89        | 22.74         | 79.88        | 64.07  | 59.16  | 74.80|\n| Gemini Embedding                 |    -    |     68.37     | 59.59       | 79.28        | 71.82  | 54.59  | 5.18         | **29.16**     | 83.63        | 65.58  | 67.71  | 79.40|\n| **Qwen3-Embedding-0.6B**         |  0.6B   |     64.33     | 56.00       | 72.22        | 66.83  | 52.33  | 5.09         | 24.59         | 80.83        | 61.41  | 64.64  | 76.17|\n| **Qwen3-Embedding-4B**           |   4B    |     69.45     | 60.86       | 79.36        | 72.33  | 57.15  | **11.56**    | 26.77         | 85.05        | 65.08  | 69.60  | 80.86|\n| **Qwen3-Embedding-8B**           |   8B    |   **70.58**   | **61.69**   | **80.89**    | **74.00** | **57.65** | 10.06      | 28.66         | **86.40**    | **65.63** | **70.88** | **81.08** |\n\n> **Note**: For compared models, the scores are retrieved from MTEB online [leaderboard](https://huggingface.co/spaces/mteb/leaderboard) on May 24th, 2025.\n\n### MTEB (Eng v2)\n\n| MTEB English / Models          |  Param.  | Mean(Task) | Mean(Type) | Class. | Clust. | Pair Class. | Rerank. | Retri. | STS   | Summ. |\n|--------------------------------|:--------:|:------------:|:------------:|:--------:|:--------:|:-------------:|:---------:|:--------:|:-------:|:-------:|\n| multilingual-e5-large-instruct |   0.6B   | 65.53      | 61.21      | 75.54  | 49.89  | 86.24       | 48.74   | 53.47  | 84.72 | 29.89 |\n| NV-Embed-v2                    |   7.8B   | 69.81      | 65.00      | 87.19  | 47.66  | 88.69       | 49.61   | 62.84  | 83.82 | 35.21 |\n| GritLM-7B                      |   7.2B   | 67.07      | 63.22      | 81.25  | 50.82  | 87.29       | 49.59   | 54.95  | 83.03 | 35.65 |\n| gte-Qwen2-1.5B-instruct        |   1.5B   | 67.20      | 63.26      | 85.84  | 53.54  | 87.52       | 49.25   | 50.25  | 82.51 | 33.94 |\n| stella_en_1.5B_v5              |   1.5B   | 69.43      | 65.32      | 89.38  | 57.06  | 88.02       | 50.19   | 52.42  | 83.27 | 36.91 |\n| gte-Qwen2-7B-instruct          |   7.6B   | 70.72      | 65.77      | 88.52  | 58.97  | 85.9        | 50.47   | 58.09  | 82.69 | 35.74 |\n| gemini-embedding-exp-03-07     |    -     | 73.3       | 67.67      | 90.05  | 59.39  | 87.7        | 48.59   | 64.35  | 85.29 | 38.28 |\n| **Qwen3-Embedding-0.6B**       |   0.6B   | 70.70      | 64.88      | 85.76  | 54.05  | 84.37       | 48.18   | 61.83  | 86.57 | 33.43 |\n| **Qwen3-Embedding-4B**         |    4B    | 74.60      | 68.10      | 89.84  | 57.51  | 87.01       | 50.76   | 68.46  | 88.72 | 34.39 |\n| **Qwen3-Embedding-8B**         |    8B    | 75.22      | 68.71      | 90.43  | 58.57  | 87.52       | 51.56   | 69.44  | 88.58 | 34.83 |\n\n### C-MTEB (MTEB Chinese)\n\n| C-MTEB           | Param. | Mean(Task) | Mean(Type) | Class. | Clust. | Pair Class. | Rerank. | Retr. | STS   |\n|------------------|--------|------------|------------|--------|--------|-------------|---------|-------|-------|\n| multilingual-e5-large-instruct | 0.6B   | 58.08      | 58.24      | 69.80  | 48.23  | 64.52       | 57.45   | 63.65 | 45.81 |\n| bge-multilingual-gemma2 | 9B     | 67.64      | 75.31      | 59.30  | 86.67  | 68.28       | 73.73   | 55.19 | -     |\n| gte-Qwen2-1.5B-instruct  | 1.5B   | 67.12      | 67.79      | 72.53  | 54.61  | 79.5        | 68.21   | 71.86 | 60.05 |\n| gte-Qwen2-7B-instruct    | 7.6B   | 71.62      | 72.19      | 75.77  | 66.06  | 81.16       | 69.24   | 75.70 | 65.20 |\n| ritrieve_zh_v1          | 0.3B   | 72.71      | 73.85      | 76.88  | 66.5   | 85.98       | 72.86   | 76.97 | 63.92 |\n| **Qwen3-Embedding-0.6B** | 0.6B   | 66.33      | 67.45      | 71.40  | 68.74  | 76.42       | 62.58   | 71.03 | 54.52 |\n| **Qwen3-Embedding-4B**   | 4B     | 72.27      | 73.51      | 75.46  | 77.89  | 83.34       | 66.05   | 77.03 | 61.26 |\n| **Qwen3-Embedding-8B**   | 8B     | 73.84      | 75.00      | 76.97  | 80.08  | 84.23       | 66.99   | 78.21 | 63.53 |\n\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@article{qwen3embedding,\n  title={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},\n  author={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2506.05176},\n  year={2025}\n}\n```"
    },
    {
      "@id": "ark:59852/model-qwen-qwen2-5-1-5b-instruct-0djuzykth7",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "Qwen/Qwen2.5-1.5B-Instruct",
      "description": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:",
      "author": "Qwen",
      "keywords": [
        "transformers",
        "safetensors",
        "qwen2",
        "text-generation",
        "chat",
        "conversational",
        "en",
        "arxiv:2407.10671",
        "base_model:Qwen/Qwen2.5-1.5B",
        "base_model:finetune:Qwen/Qwen2.5-1.5B",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-generation",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "hasBias": "**This repo contains the instruction-tuned 1.5B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\n- Number of Parameters: 1.54B\n- Number of Paramaters (Non-Embedding): 1.31B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 12 for Q and 2 for KV\n- Context Length: Full 32,768 tokens and generation 8192 tokens",
      "baseModel": "Qwen/Qwen2.5-1.5B",
      "contentUrl": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct/resolve/main/model.safetensors",
      "url": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Qwen2.5-1.5B-Instruct\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 1.5B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\n- Number of Parameters: 1.54B\n- Number of Paramaters (Non-Embedding): 1.31B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 12 for Q and 2 for KV\n- Context Length: Full 32,768 tokens and generation 8192 tokens\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: 'qwen2'\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [\ud83d\udcd1 blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```"
    },
    {
      "@id": "ark:59852/model-jinaai-jina-embeddings-v3-314zxakwg5",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "jinaai/jina-embeddings-v3",
      "description": "<br><br>",
      "author": "jinaai",
      "keywords": [
        "transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "feature-extraction",
        "sentence-similarity",
        "mteb",
        "sentence-transformers",
        "custom_code",
        "multilingual",
        "af",
        "am",
        "ar",
        "as",
        "az",
        "be",
        "bg",
        "bn",
        "br",
        "bs",
        "ca",
        "cs",
        "cy",
        "da",
        "de",
        "el",
        "en",
        "eo",
        "es",
        "et",
        "eu",
        "fa",
        "fi",
        "fr",
        "fy",
        "ga",
        "gd",
        "gl",
        "gu",
        "ha",
        "he",
        "hi",
        "hr",
        "hu",
        "hy",
        "id",
        "is",
        "it",
        "ja",
        "jv",
        "ka",
        "kk",
        "km",
        "kn",
        "ko",
        "ku",
        "ky",
        "la",
        "lo",
        "lt",
        "lv",
        "mg",
        "mk",
        "ml",
        "mn",
        "mr",
        "ms",
        "my",
        "ne",
        "nl",
        "no",
        "om",
        "or",
        "pa",
        "pl",
        "ps",
        "pt",
        "ro",
        "ru",
        "sa",
        "sd",
        "si",
        "sk",
        "sl",
        "so",
        "sq",
        "sr",
        "su",
        "sv",
        "sw",
        "ta",
        "te",
        "th",
        "tl",
        "tr",
        "ug",
        "uk",
        "ur",
        "uz",
        "vi",
        "xh",
        "yi",
        "zh",
        "arxiv:2409.10173",
        "license:cc-by-nc-4.0",
        "model-index",
        "region:eu"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "intendedUseCase": "`jina-embeddings-v3` is a **multilingual multi-task text embedding model** designed for a variety of NLP applications.\nBased on the [Jina-XLM-RoBERTa architecture](https://huggingface.co/jinaai/xlm-roberta-flash-implementation), \nthis model supports Rotary Position Embeddings to handle long input sequences up to **8192 tokens**.\nAdditionally, it features 5 LoRA adapters to generate task-specific embeddings efficiently.",
      "usageInformation": "`jina-embeddings-v3` is a **multilingual multi-task text embedding model** designed for a variety of NLP applications.\nBased on the [Jina-XLM-RoBERTa architecture](https://huggingface.co/jinaai/xlm-roberta-flash-implementation), \nthis model supports Rotary Position Embeddings to handle long input sequences up to **8192 tokens**.\nAdditionally, it features 5 LoRA adapters to generate task-specific embeddings efficiently.\n\n### Key Features:\n- **Extended Sequence Length:** Supports up to 8192 tokens with RoPE.\n- **Task-Specific Embedding:** Customize embeddings through the `task` argument with the following options:\n    - `retrieval.query`: Used for query embeddings in asymmetric retrieval tasks\n    - `retrieval.passage`: Used for passage embeddings in asymmetric retrieval tasks\n    - `separation`: Used for embeddings in clustering and re-ranking applications\n    - `classification`: Used for embeddings in classification tasks\n    - `text-matching`: Used for embeddings in tasks that quantify similarity between two texts, such as STS or symmetric retrieval tasks\n- **Matryoshka Embeddings**: Supports flexible embedding sizes (`32, 64, 128, 256, 512, 768, 1024`), allowing for truncating embeddings to fit your application.\n\n### Supported Languages:\nWhile the foundation model supports 100 languages, we've focused our tuning efforts on the following 30 languages: \n**Arabic, Bengali, Chinese, Danish, Dutch, English, Finnish, French, Georgian, German, Greek, \nHindi, Indonesian, Italian, Japanese, Korean, Latvian, Norwegian, Polish, Portuguese, Romanian, \nRussian, Slovak, Spanish, Swedish, Thai, Turkish, Ukrainian, Urdu,** and **Vietnamese.**\n\n\n> **\u26a0\ufe0f Important Notice:**  \n> We fixed a bug in the `encode` function [#60](https://huggingface.co/jinaai/jina-embeddings-v3/discussions/60) where **Matryoshka embedding truncation** occurred *after normalization*, leading to non-normalized truncated embeddings. This issue has been resolved in the latest code revision.  \n>  \n> If you have encoded data using the previous version and wish to maintain consistency, please use the specific code revision when loading the model: `AutoModel.from_pretrained('jinaai/jina-embeddings-v3', code_revision='da863dd04a4e5dce6814c6625adfba87b83838aa', ...)`\n\n\n\n**<details><summary>Apply mean pooling when integrating the model.</summary>**\n<p>\n\n### Why Use Mean Pooling?\n\nMean pooling takes all token embeddings from the model's output and averages them at the sentence or paragraph level. \nThis approach has been shown to produce high-quality sentence embeddings.\n\nWe provide an `encode` function that handles this for you automatically.\n\nHowever, if you're working with the model directly, outside of the `encode` function, \nyou'll need to apply mean pooling manually. Here's how you can do it:\n\n\n```python\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]\n    input_mask_expanded = (\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    )\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n        input_mask_expanded.sum(1), min=1e-9\n    )\n\n\nsentences = [\"How is the weather today?\", \"What is the current weather like today?\"]\n\ntokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v3\")\nmodel = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\ntask = 'retrieval.query'\ntask_id = model._adaptation_map[task]\nadapter_mask = torch.full((len(sentences),), task_id, dtype=torch.int32)\nwith torch.no_grad():\n    model_output = model(**encoded_input, adapter_mask=adapter_mask)\n\nembeddings = mean_pooling(model_output, encoded_input[\"attention_mask\"])\nembeddings = F.normalize(embeddings, p=2, dim=1)\n\n```\n\n</p>\n</details>\n\nThe easiest way to start using `jina-embeddings-v3` is with the [Jina Embedding API](https://jina.ai/embeddings/).\n\nAlternatively, you can use `jina-embeddings-v3` directly via Transformers package:\n```bash\n!pip install transformers torch einops\n!pip install 'numpy<2'\n```\nIf you run it on a GPU that support [FlashAttention-2](https://github.com/Dao-AILab/flash-attention). By 2024.9.12, it supports Ampere, Ada, or Hopper GPUs (e.g., A100, RTX 3090, RTX 4090, H100),\n\n```bash\n!pip install flash-attn --no-build-isolation\n```\n\n```python\nfrom transformers import AutoModel",
      "contentUrl": "https://huggingface.co/jinaai/jina-embeddings-v3/resolve/main/model.safetensors",
      "url": "https://huggingface.co/jinaai/jina-embeddings-v3",
      "license": "cc-by-nc-4.0",
      "isPartOf": [],
      "README": "\n<br><br>\n\n<p align=\"center\">\n<img src=\"https://huggingface.co/datasets/jinaai/documentation-images/resolve/main/logo.webp\" alt=\"Jina AI: Your Search Foundation, Supercharged!\" width=\"150px\">\n</p>\n\n\n<p align=\"center\">\n<b>The embedding model trained by <a href=\"https://jina.ai/\"><b>Jina AI</b></a>.</b>\n</p>\n\n<p align=\"center\">\n<b>jina-embeddings-v3: Multilingual Embeddings With Task LoRA</b>\n</p>\n\n## Quick Start\n\n[Blog](https://jina.ai/news/jina-embeddings-v3-a-frontier-multilingual-embedding-model/#parameter-dimensions) | [Azure](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/jinaai.jina-embeddings-v3-vm) | [AWS SageMaker](https://aws.amazon.com/marketplace/pp/prodview-kdi3xkt62lo32) | [API](https://jina.ai/embeddings)\n\n\n## Intended Usage & Model Info\n\n\n`jina-embeddings-v3` is a **multilingual multi-task text embedding model** designed for a variety of NLP applications.\nBased on the [Jina-XLM-RoBERTa architecture](https://huggingface.co/jinaai/xlm-roberta-flash-implementation), \nthis model supports Rotary Position Embeddings to handle long input sequences up to **8192 tokens**.\nAdditionally, it features 5 LoRA adapters to generate task-specific embeddings efficiently.\n\n### Key Features:\n- **Extended Sequence Length:** Supports up to 8192 tokens with RoPE.\n- **Task-Specific Embedding:** Customize embeddings through the `task` argument with the following options:\n    - `retrieval.query`: Used for query embeddings in asymmetric retrieval tasks\n    - `retrieval.passage`: Used for passage embeddings in asymmetric retrieval tasks\n    - `separation`: Used for embeddings in clustering and re-ranking applications\n    - `classification`: Used for embeddings in classification tasks\n    - `text-matching`: Used for embeddings in tasks that quantify similarity between two texts, such as STS or symmetric retrieval tasks\n- **Matryoshka Embeddings**: Supports flexible embedding sizes (`32, 64, 128, 256, 512, 768, 1024`), allowing for truncating embeddings to fit your application.\n\n### Supported Languages:\nWhile the foundation model supports 100 languages, we've focused our tuning efforts on the following 30 languages: \n**Arabic, Bengali, Chinese, Danish, Dutch, English, Finnish, French, Georgian, German, Greek, \nHindi, Indonesian, Italian, Japanese, Korean, Latvian, Norwegian, Polish, Portuguese, Romanian, \nRussian, Slovak, Spanish, Swedish, Thai, Turkish, Ukrainian, Urdu,** and **Vietnamese.**\n\n\n> **\u26a0\ufe0f Important Notice:**  \n> We fixed a bug in the `encode` function [#60](https://huggingface.co/jinaai/jina-embeddings-v3/discussions/60) where **Matryoshka embedding truncation** occurred *after normalization*, leading to non-normalized truncated embeddings. This issue has been resolved in the latest code revision.  \n>  \n> If you have encoded data using the previous version and wish to maintain consistency, please use the specific code revision when loading the model: `AutoModel.from_pretrained('jinaai/jina-embeddings-v3', code_revision='da863dd04a4e5dce6814c6625adfba87b83838aa', ...)`\n\n\n## Usage\n\n**<details><summary>Apply mean pooling when integrating the model.</summary>**\n<p>\n\n### Why Use Mean Pooling?\n\nMean pooling takes all token embeddings from the model's output and averages them at the sentence or paragraph level. \nThis approach has been shown to produce high-quality sentence embeddings.\n\nWe provide an `encode` function that handles this for you automatically.\n\nHowever, if you're working with the model directly, outside of the `encode` function, \nyou'll need to apply mean pooling manually. Here's how you can do it:\n\n\n```python\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]\n    input_mask_expanded = (\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    )\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n        input_mask_expanded.sum(1), min=1e-9\n    )\n\n\nsentences = [\"How is the weather today?\", \"What is the current weather like today?\"]\n\ntokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v3\")\nmodel = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\ntask = 'retrieval.query'\ntask_id = model._adaptation_map[task]\nadapter_mask = torch.full((len(sentences),), task_id, dtype=torch.int32)\nwith torch.no_grad():\n    model_output = model(**encoded_input, adapter_mask=adapter_mask)\n\nembeddings = mean_pooling(model_output, encoded_input[\"attention_mask\"])\nembeddings = F.normalize(embeddings, p=2, dim=1)\n\n```\n\n</p>\n</details>\n\nThe easiest way to start using `jina-embeddings-v3` is with the [Jina Embedding API](https://jina.ai/embeddings/).\n\nAlternatively, you can use `jina-embeddings-v3` directly via Transformers package:\n```bash\n!pip install transformers torch einops\n!pip install 'numpy<2'\n```\nIf you run it on a GPU that support [FlashAttention-2](https://github.com/Dao-AILab/flash-attention). By 2024.9.12, it supports Ampere, Ada, or Hopper GPUs (e.g., A100, RTX 3090, RTX 4090, H100),\n\n```bash\n!pip install flash-attn --no-build-isolation\n```\n\n```python\nfrom transformers import AutoModel\n\n# Initialize the model\nmodel = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n\ntexts = [\n    \"Follow the white rabbit.\",  # English\n    \"Sigue al conejo blanco.\",  # Spanish\n    \"Suis le lapin blanc.\",  # French\n    \"\u8ddf\u7740\u767d\u5154\u8d70\u3002\",  # Chinese\n    \"\u0627\u062a\u0628\u0639 \u0627\u0644\u0623\u0631\u0646\u0628 \u0627\u0644\u0623\u0628\u064a\u0636.\",  # Arabic\n    \"Folge dem wei\u00dfen Kaninchen.\",  # German\n]\n\n# When calling the `encode` function, you can choose a `task` based on the use case:\n# 'retrieval.query', 'retrieval.passage', 'separation', 'classification', 'text-matching'\n# Alternatively, you can choose not to pass a `task`, and no specific LoRA adapter will be used.\nembeddings = model.encode(texts, task=\"text-matching\")\n\n# Compute similarities\nprint(embeddings[0] @ embeddings[1].T)\n```\n\nBy default, the model supports a maximum sequence length of 8192 tokens. \nHowever, if you want to truncate your input texts to a shorter length, you can pass the `max_length` parameter to the `encode` function:\n```python\nembeddings = model.encode([\"Very long ... document\"], max_length=2048)\n\n```\n\nIn case you want to use **Matryoshka embeddings** and switch to a different dimension, \nyou can adjust it by passing the `truncate_dim` parameter to the `encode` function:\n```python\nembeddings = model.encode(['Sample text'], truncate_dim=256)\n```\n\n\nThe latest version (3.1.0) of [SentenceTransformers](https://github.com/UKPLab/sentence-transformers) also supports `jina-embeddings-v3`:\n\n```bash\n!pip install -U sentence-transformers\n```\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\n\ntask = \"retrieval.query\"\nembeddings = model.encode(\n    [\"What is the weather like in Berlin today?\"],\n    task=task,\n    prompt_name=task,\n)\n```\n\nYou can fine-tune `jina-embeddings-v3` using [SentenceTransformerTrainer](https://sbert.net/docs/package_reference/sentence_transformer/trainer.html). \nTo fine-tune for a specific task, you should set the task before passing the model to the ST Trainer, either during initialization:\n```python\nmodel = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True, model_kwargs={'default_task': 'classification'})\n```\nOr afterwards:\n```python\nmodel = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True)\nmodel[0].default_task = 'classification'\n```\nThis way you can fine-tune the LoRA adapter for the chosen task.\n\nHowever, If you want to fine-tune the entire model, make sure the main parameters are set as trainable when loading the model:\n```python\nmodel = SentenceTransformer(\"jinaai/jina-embeddings-v3\", trust_remote_code=True, model_kwargs={'lora_main_params_trainable': True})\n```\nThis will allow fine-tuning the whole model instead of just the LoRA adapters.\n\n\n**<details><summary>ONNX Inference.</summary>**\n<p>\n\nYou can use ONNX for efficient inference with `jina-embeddings-v3`:\n```python\nimport onnxruntime\nimport numpy as np\nfrom transformers import AutoTokenizer, PretrainedConfig\n\n# Mean pool function\ndef mean_pooling(model_output: np.ndarray, attention_mask: np.ndarray):\n    token_embeddings = model_output\n    input_mask_expanded = np.expand_dims(attention_mask, axis=-1)\n    input_mask_expanded = np.broadcast_to(input_mask_expanded, token_embeddings.shape)\n    sum_embeddings = np.sum(token_embeddings * input_mask_expanded, axis=1)\n    sum_mask = np.clip(np.sum(input_mask_expanded, axis=1), a_min=1e-9, a_max=None)\n    return sum_embeddings / sum_mask\n\n# Load tokenizer and model config\ntokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v3')\nconfig = PretrainedConfig.from_pretrained('jinaai/jina-embeddings-v3')\n\n# Tokenize input\ninput_text = tokenizer('sample text', return_tensors='np')\n\n# ONNX session\nmodel_path = 'jina-embeddings-v3/onnx/model.onnx'\nsession = onnxruntime.InferenceSession(model_path)\n\n# Prepare inputs for ONNX model\ntask_type = 'text-matching'\ntask_id = np.array(config.lora_adaptations.index(task_type), dtype=np.int64)\ninputs = {\n    'input_ids': input_text['input_ids'],\n    'attention_mask': input_text['attention_mask'],\n    'task_id': task_id\n}\n\n# Run model\noutputs = session.run(None, inputs)[0]\n\n# Apply mean pooling and normalization to the model outputs\nembeddings = mean_pooling(outputs, input_text[\"attention_mask\"])\nembeddings = embeddings / np.linalg.norm(embeddings, ord=2, axis=1, keepdims=True)\n```\n\n</p>\n</details>\n\n\n## Contact\n\nJoin our [Discord community](https://discord.jina.ai) and chat with other community members about ideas.\n\n## License\n\n`jina-embeddings-v3` is listed on AWS & Azure. If you need to use it beyond those platforms or on-premises within your company, note that the models is licensed under CC BY-NC 4.0. For commercial usage inquiries, feel free to [contact us](https://jina.ai/contact-sales/).\n\n## Citation\n\nIf you find `jina-embeddings-v3` useful in your research, please cite the following paper:\n\n```bibtex\n@misc{sturua2024jinaembeddingsv3multilingualembeddingstask,\n      title={jina-embeddings-v3: Multilingual Embeddings With Task LoRA}, \n      author={Saba Sturua and Isabelle Mohr and Mohammad Kalim Akram and Michael G\u00fcnther and Bo Wang and Markus Krimmel and Feng Wang and Georgios Mastrapas and Andreas Koukounas and Andreas Koukounas and Nan Wang and Han Xiao},\n      year={2024},\n      eprint={2409.10173},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2409.10173}, \n}\n\n```\n"
    },
    {
      "@id": "ark:59852/model-qwen-qwen3-8b-gbnvaw9hka",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "Qwen/Qwen3-8B",
      "description": "<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">",
      "author": "Qwen",
      "keywords": [
        "transformers",
        "safetensors",
        "qwen3",
        "text-generation",
        "conversational",
        "arxiv:2309.00071",
        "arxiv:2505.09388",
        "base_model:Qwen/Qwen3-8B-Base",
        "base_model:finetune:Qwen/Qwen3-8B-Base",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-generation",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "intendedUseCase": "For local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.",
      "usageInformation": "We provide a soft switch mechanism that allows users to dynamically control the model's behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-8B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)",
      "baseModel": "Qwen/Qwen3-8B-Base",
      "contentUrl": "https://huggingface.co/Qwen/Qwen3-8B/resolve/main/model-00001-of-00005.safetensors",
      "url": "https://huggingface.co/Qwen/Qwen3-8B",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Qwen3-8B\n<a href=\"https://chat.qwen.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Qwen3 Highlights\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n- **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n## Model Overview\n\n**Qwen3-8B** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 8.2B\n- Number of Paramaters (Non-Embedding): 6.95B\n- Number of Layers: 36\n- Number of Attention Heads (GQA): 32 for Q and 8 for KV\n- Context Length: 32,768 natively and [131,072 tokens with YaRN](#processing-long-texts). \n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Quickstart\n\nThe code of Qwen3 has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: 'qwen3'\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen3-8B\"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\n\n# prepare the model input\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n\nprint(\"thinking content:\", thinking_content)\nprint(\"content:\", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-8B --reasoning-parser qwen3\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-8B --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Switching Between Thinking and Non-Thinking Mode\n\n> [!TIP]\n> The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. \n> Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### `enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `<think>...</think>` block, followed by the final response.\n\n> [!NOTE]\n> For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n\n### `enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model's thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `<think>...</think>` block.\n\n> [!NOTE]\n> For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n### Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model's behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name=\"Qwen/Qwen3-8B\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors=\"pt\")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({\"role\": \"user\", \"content\": user_input})\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n# Example Usage\nif __name__ == \"__main__\":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = \"How many r's in strawberries?\"\n    print(f\"User: {user_input_1}\")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f\"Bot: {response_1}\")\n    print(\"----------------------\")\n\n    # Second input with /no_think\n    user_input_2 = \"Then, how many r's in blueberries? /no_think\"\n    print(f\"User: {user_input_2}\")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f\"Bot: {response_2}\") \n    print(\"----------------------\")\n\n    # Third input with /think\n    user_input_3 = \"Really? /think\"\n    print(f\"User: {user_input_3}\")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f\"Bot: {response_3}\")\n```\n\n> [!NOTE]\n> For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `<think>...</think>`. However, the content inside this block may be empty if thinking is disabled.\n> When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `<think>...</think>` block.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    'model': 'Qwen3-8B',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # 'model_type': 'qwen_dashscope',\n    # 'api_key': os.getenv('DASHSCOPE_API_KEY'),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    'model_server': 'http://localhost:8000/v1',  # api_base\n    'api_key': 'EMPTY',\n\n    # Other parameters:\n    # 'generate_cfg': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         'thought_in_content': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {'mcpServers': {  # You can specify the MCP configuration file\n            'time': {\n                'command': 'uvx',\n                'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai']\n            },\n            \"fetch\": {\n                \"command\": \"uvx\",\n                \"args\": [\"mcp-server-fetch\"]\n            }\n        }\n    },\n  'code_interpreter',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen'}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Processing Long Texts\n\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model's performance on context lengths of up to 131,072 tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\n\nYaRN is currently supported by several inference frameworks, e.g., `transformers` and `llama.cpp` for local use, `vllm` and `sglang` for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\n\n- Modifying the model files:\n  In the `config.json` file, add the `rope_scaling` fields:\n    ```json\n    {\n        ...,\n        \"rope_scaling\": {\n            \"rope_type\": \"yarn\",\n            \"factor\": 4.0,\n            \"original_max_position_embeddings\": 32768\n        }\n    }\n    ```\n  For `llama.cpp`, you need to regenerate the GGUF file after the modification.\n\n- Passing command line arguments:\n\n  For `vllm`, you can use\n    ```shell\n    vllm serve ... --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}' --max-model-len 131072  \n    ```\n\n  For `sglang`, you can use\n    ```shell\n    python -m sglang.launch_server ... --json-model-override-args '{\"rope_scaling\":{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}}'\n    ```\n\n  For `llama-server` from `llama.cpp`, you can use\n    ```shell\n    llama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\n    ```\n\n> [!IMPORTANT]\n> If you encounter the following warning\n> ```\n> Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'original_max_position_embeddings'}\n> ```\n> please upgrade `transformers>=4.51.0`.\n\n> [!NOTE]\n> All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\n> We advise adding the `rope_scaling` configuration only when processing long contexts is required. \n> It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set `factor` as 2.0. \n\n> [!NOTE]\n> The default `max_position_embeddings` in `config.json` is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\n\n> [!TIP]\n> The endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n   - For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include \"Please reason step by step, and put your final answer within \\boxed{}.\" in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: \"Please show your choice in the `answer` field with only the choice letter, e.g., `\"answer\": \"C\"`.\"\n\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```"
    },
    {
      "@id": "ark:59852/model-openai-whisper-large-v3-fuk3yql9zm2",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "openai/whisper-large-v3",
      "description": "Whisper is a state-of-the-art model for automatic speech recognition (ASR) and speech translation, proposed in the paper",
      "author": "openai",
      "keywords": [
        "transformers",
        "pytorch",
        "jax",
        "safetensors",
        "whisper",
        "automatic-speech-recognition",
        "audio",
        "hf-asr-leaderboard",
        "en",
        "zh",
        "de",
        "es",
        "ru",
        "ko",
        "fr",
        "ja",
        "pt",
        "tr",
        "pl",
        "ca",
        "nl",
        "ar",
        "sv",
        "it",
        "id",
        "hi",
        "fi",
        "vi",
        "he",
        "uk",
        "el",
        "ms",
        "cs",
        "ro",
        "da",
        "hu",
        "ta",
        "no",
        "th",
        "ur",
        "hr",
        "bg",
        "lt",
        "la",
        "mi",
        "ml",
        "cy",
        "sk",
        "te",
        "fa",
        "lv",
        "bn",
        "sr",
        "az",
        "sl",
        "kn",
        "et",
        "mk",
        "br",
        "eu",
        "is",
        "hy",
        "ne",
        "mn",
        "bs",
        "kk",
        "sq",
        "sw",
        "gl",
        "mr",
        "pa",
        "si",
        "km",
        "sn",
        "yo",
        "so",
        "af",
        "oc",
        "ka",
        "be",
        "tg",
        "sd",
        "gu",
        "am",
        "yi",
        "lo",
        "uz",
        "fo",
        "ht",
        "ps",
        "tk",
        "nn",
        "mt",
        "sa",
        "lb",
        "my",
        "bo",
        "tl",
        "mg",
        "as",
        "tt",
        "haw",
        "ln",
        "ha",
        "ba",
        "jw",
        "su",
        "arxiv:2212.04356",
        "license:apache-2.0",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "hasBias": "The primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only \u201cintended\u201d uses or to draw reasonable guidelines around what is or is not research.",
      "intendedUseCase": "Our studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf). \n\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.",
      "usageInformation": "Whisper large-v3 is supported in Hugging Face \ud83e\udd17 Transformers. To run the model, first install the Transformers \nlibrary. For this example, we'll also install \ud83e\udd17 Datasets to load toy audio dataset from the Hugging Face Hub, and \n\ud83e\udd17 Accelerate to reduce the model loading time:\n\n```bash\npip install --upgrade pip\npip install --upgrade transformers datasets[audio] accelerate\n```\n\nThe model can be used with the [`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)\nclass to transcribe audios of arbitrary length:\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\nresult = pipe(sample)\nprint(result[\"text\"])\n```\n\nTo transcribe a local audio file, simply pass the path to your audio file when you call the pipeline:\n\n```python\nresult = pipe(\"audio.mp3\")\n```\n\nMultiple audio files can be transcribed in parallel by specifying them as a list and setting the `batch_size` parameter:\n\n```python\nresult = pipe([\"audio_1.mp3\", \"audio_2.mp3\"], batch_size=2)\n```\n\nTransformers is compatible with all Whisper decoding strategies, such as temperature fallback and condition on previous \ntokens. The following example demonstrates how to enable these heuristics:\n\n```python\ngenerate_kwargs = {\n    \"max_new_tokens\": 448,\n    \"num_beams\": 1,\n    \"condition_on_prev_tokens\": False,\n    \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    \"logprob_threshold\": -1.0,\n    \"no_speech_threshold\": 0.6,\n    \"return_timestamps\": True,\n}\n\nresult = pipe(sample, generate_kwargs=generate_kwargs)\n```\n\nWhisper predicts the language of the source audio automatically. If the source audio language is known *a-priori*, it \ncan be passed as an argument to the pipeline:\n\n```python\nresult = pipe(sample, generate_kwargs={\"language\": \"english\"})\n```\n\nBy default, Whisper performs the task of *speech transcription*, where the source audio language is the same as the target\ntext language. To perform *speech translation*, where the target text is in English, set the task to `\"translate\"`:\n\n```python\nresult = pipe(sample, generate_kwargs={\"task\": \"translate\"})\n```\n\nFinally, the model can be made to predict timestamps. For sentence-level timestamps, pass the `return_timestamps` argument:\n\n```python\nresult = pipe(sample, return_timestamps=True)\nprint(result[\"chunks\"])\n```\n\nAnd for word-level timestamps:\n\n```python\nresult = pipe(sample, return_timestamps=\"word\")\nprint(result[\"chunks\"])\n```\n\nThe above arguments can be used in isolation or in combination. For example, to perform the task of speech transcription \nwhere the source audio is in French, and we want to return sentence-level timestamps, the following can be used:\n\n```python\nresult = pipe(sample, return_timestamps=True, generate_kwargs={\"language\": \"french\", \"task\": \"translate\"})\nprint(result[\"chunks\"])\n```\n\n<details>\n\n<summary> For more control over the generation parameters, use the model + processor API directly: </summary>\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\nfrom datasets import Audio, load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\ndataset = dataset.cast_column(\"audio\", Audio(processor.feature_extractor.sampling_rate))\nsample = dataset[0][\"audio\"]\n\ninputs = processor(\n    sample[\"array\"],\n    sampling_rate=sample[\"sampling_rate\"],\n    return_tensors=\"pt\",\n    truncation=False,\n    padding=\"longest\",\n    return_attention_mask=True,\n)\ninputs = inputs.to(device, dtype=torch_dtype)\n\ngen_kwargs = {\n    \"max_new_tokens\": 448,\n    \"num_beams\": 1,\n    \"condition_on_prev_tokens\": False,\n    \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    \"logprob_threshold\": -1.0,\n    \"no_speech_threshold\": 0.6,\n    \"return_timestamps\": True,\n}\n\npred_ids = model.generate(**inputs, **gen_kwargs)\npred_text = processor.batch_decode(pred_ids, skip_special_tokens=True, decode_with_timestamps=False)\n\nprint(pred_text)\n```\n\n</details>",
      "contentUrl": "https://huggingface.co/openai/whisper-large-v3/resolve/main/model.fp32-00001-of-00002.safetensors",
      "url": "https://huggingface.co/openai/whisper-large-v3",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Whisper\n\nWhisper is a state-of-the-art model for automatic speech recognition (ASR) and speech translation, proposed in the paper \n[Robust Speech Recognition via Large-Scale Weak Supervision](https://huggingface.co/papers/2212.04356) by Alec Radford \net al. from OpenAI. Trained on >5M hours of labeled data, Whisper demonstrates a strong ability to generalise to many \ndatasets and domains in a zero-shot setting.\n\nWhisper large-v3 has the same architecture as the previous [large](https://huggingface.co/openai/whisper-large) and [large-v2](https://huggingface.co/openai/whisper-large-v2) \nmodels, except for the following minor differences:\n\n1. The spectrogram input uses 128 Mel frequency bins instead of 80\n2. A new language token for Cantonese\n\nThe Whisper large-v3 model was trained on 1 million hours of weakly labeled audio and 4 million hours of pseudo-labeled \naudio collected using Whisper [large-v2](https://huggingface.co/openai/whisper-large-v2) . The model was trained for 2.0 epochs over this mixture dataset.\n\nThe large-v3 model shows improved performance over a wide variety of languages, showing 10% to 20% reduction of errors \ncompared to Whisper [large-v2](https://huggingface.co/openai/whisper-large-v2) . For more details on the different checkpoints available, refer to the section [Model details](#model-details).\n\n**Disclaimer**: Content for this model card has partly been written by the \ud83e\udd17 Hugging Face team, and partly copied and \npasted from the original model card.\n\n## Usage\n\nWhisper large-v3 is supported in Hugging Face \ud83e\udd17 Transformers. To run the model, first install the Transformers \nlibrary. For this example, we'll also install \ud83e\udd17 Datasets to load toy audio dataset from the Hugging Face Hub, and \n\ud83e\udd17 Accelerate to reduce the model loading time:\n\n```bash\npip install --upgrade pip\npip install --upgrade transformers datasets[audio] accelerate\n```\n\nThe model can be used with the [`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)\nclass to transcribe audios of arbitrary length:\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\nresult = pipe(sample)\nprint(result[\"text\"])\n```\n\nTo transcribe a local audio file, simply pass the path to your audio file when you call the pipeline:\n\n```python\nresult = pipe(\"audio.mp3\")\n```\n\nMultiple audio files can be transcribed in parallel by specifying them as a list and setting the `batch_size` parameter:\n\n```python\nresult = pipe([\"audio_1.mp3\", \"audio_2.mp3\"], batch_size=2)\n```\n\nTransformers is compatible with all Whisper decoding strategies, such as temperature fallback and condition on previous \ntokens. The following example demonstrates how to enable these heuristics:\n\n```python\ngenerate_kwargs = {\n    \"max_new_tokens\": 448,\n    \"num_beams\": 1,\n    \"condition_on_prev_tokens\": False,\n    \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    \"logprob_threshold\": -1.0,\n    \"no_speech_threshold\": 0.6,\n    \"return_timestamps\": True,\n}\n\nresult = pipe(sample, generate_kwargs=generate_kwargs)\n```\n\nWhisper predicts the language of the source audio automatically. If the source audio language is known *a-priori*, it \ncan be passed as an argument to the pipeline:\n\n```python\nresult = pipe(sample, generate_kwargs={\"language\": \"english\"})\n```\n\nBy default, Whisper performs the task of *speech transcription*, where the source audio language is the same as the target\ntext language. To perform *speech translation*, where the target text is in English, set the task to `\"translate\"`:\n\n```python\nresult = pipe(sample, generate_kwargs={\"task\": \"translate\"})\n```\n\nFinally, the model can be made to predict timestamps. For sentence-level timestamps, pass the `return_timestamps` argument:\n\n```python\nresult = pipe(sample, return_timestamps=True)\nprint(result[\"chunks\"])\n```\n\nAnd for word-level timestamps:\n\n```python\nresult = pipe(sample, return_timestamps=\"word\")\nprint(result[\"chunks\"])\n```\n\nThe above arguments can be used in isolation or in combination. For example, to perform the task of speech transcription \nwhere the source audio is in French, and we want to return sentence-level timestamps, the following can be used:\n\n```python\nresult = pipe(sample, return_timestamps=True, generate_kwargs={\"language\": \"french\", \"task\": \"translate\"})\nprint(result[\"chunks\"])\n```\n\n<details>\n\n<summary> For more control over the generation parameters, use the model + processor API directly: </summary>\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\nfrom datasets import Audio, load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\ndataset = dataset.cast_column(\"audio\", Audio(processor.feature_extractor.sampling_rate))\nsample = dataset[0][\"audio\"]\n\ninputs = processor(\n    sample[\"array\"],\n    sampling_rate=sample[\"sampling_rate\"],\n    return_tensors=\"pt\",\n    truncation=False,\n    padding=\"longest\",\n    return_attention_mask=True,\n)\ninputs = inputs.to(device, dtype=torch_dtype)\n\ngen_kwargs = {\n    \"max_new_tokens\": 448,\n    \"num_beams\": 1,\n    \"condition_on_prev_tokens\": False,\n    \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    \"logprob_threshold\": -1.0,\n    \"no_speech_threshold\": 0.6,\n    \"return_timestamps\": True,\n}\n\npred_ids = model.generate(**inputs, **gen_kwargs)\npred_text = processor.batch_decode(pred_ids, skip_special_tokens=True, decode_with_timestamps=False)\n\nprint(pred_text)\n```\n\n</details>\n\n## Additional Speed & Memory Improvements\n\nYou can apply additional speed and memory improvements to Whisper to further reduce the inference speed and VRAM \nrequirements.\n\n### Chunked Long-Form\n\nWhisper has a receptive field of 30-seconds. To transcribe audios longer than this, one of two long-form algorithms are\nrequired:\n1. **Sequential:** uses a \"sliding window\" for buffered inference, transcribing 30-second slices one after the other\n2. **Chunked:** splits long audio files into shorter ones (with a small overlap between segments), transcribes each segment independently, and stitches the resulting transcriptions at the boundaries\n\nThe sequential long-form algorithm should be used in either of the following scenarios:\n1. Transcription accuracy is the most important factor, and speed is less of a consideration\n2. You are transcribing **batches** of long audio files, in which case the latency of sequential is comparable to chunked, while being up to 0.5% WER more accurate\n\nConversely, the chunked algorithm should be used when:\n1. Transcription speed is the most important factor\n2. You are transcribing a **single** long audio file\n\nBy default, Transformers uses the sequential algorithm. To enable the chunked algorithm, pass the `chunk_length_s` \nparameter to the `pipeline`. For large-v3, a chunk length of 30-seconds is optimal. To activate batching over long \naudio files, pass the argument `batch_size`:\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    chunk_length_s=30,\n    batch_size=16,  # batch size for inference - set based on your device\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\nresult = pipe(sample)\nprint(result[\"text\"])\n```\n\n#### Torch compile\n\nThe Whisper forward pass is compatible with [`torch.compile`](https://pytorch.org/docs/stable/generated/torch.compile.html)\nfor 4.5x speed-ups.\n\n**Note:** `torch.compile` is currently not compatible with the Chunked long-form algorithm or Flash Attention 2 \u26a0\ufe0f\n\n```python\nimport torch\nfrom torch.nn.attention import SDPBackend, sdpa_kernel\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\ntorch.set_float32_matmul_precision(\"high\")\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n).to(device)\n\n# Enable static cache and compile the forward pass\nmodel.generation_config.cache_implementation = \"static\"\nmodel.generation_config.max_new_tokens = 256\nmodel.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\n# 2 warmup steps\nfor _ in tqdm(range(2), desc=\"Warm-up step\"):\n    with sdpa_kernel(SDPBackend.MATH):\n        result = pipe(sample.copy(), generate_kwargs={\"min_new_tokens\": 256, \"max_new_tokens\": 256})\n\n# fast run\nwith sdpa_kernel(SDPBackend.MATH):\n    result = pipe(sample.copy())\n\nprint(result[\"text\"])\n```\n\n#### Flash Attention 2\n\nWe recommend using [Flash-Attention 2](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#flashattention-2) if your GPU supports it and you are not using [torch.compile](#torch-compile). \nTo do so, first install [Flash Attention](https://github.com/Dao-AILab/flash-attention):\n\n```\npip install flash-attn --no-build-isolation\n```\n\nThen pass `attn_implementation=\"flash_attention_2\"` to `from_pretrained`:\n\n```python\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation=\"flash_attention_2\")\n```\n\n#### Torch Scale-Product-Attention (SDPA)\n\nIf your GPU does not support Flash Attention, we recommend making use of PyTorch [scaled dot-product attention (SDPA)](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html). \nThis attention implementation is activated **by default** for PyTorch versions 2.1.1 or greater. To check \nwhether you have a compatible PyTorch version, run the following Python code snippet:\n\n```python\nfrom transformers.utils import is_torch_sdpa_available\n\nprint(is_torch_sdpa_available())\n```\n\nIf the above returns `True`, you have a valid version of PyTorch installed and SDPA is activated by default. If it \nreturns `False`, you need to upgrade your PyTorch version according to the [official instructions](https://pytorch.org/get-started/locally/)\n\nOnce a valid PyTorch version is installed, SDPA is activated by default. It can also be set explicitly by specifying \n`attn_implementation=\"sdpa\"` as follows:\n\n```python\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation=\"sdpa\")\n```\n\nFor more information about how to use the SDPA refer to the [Transformers SDPA documentation](https://huggingface.co/docs/transformers/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention).\n\n\n## Model details\n\nWhisper is a Transformer based encoder-decoder model, also referred to as a _sequence-to-sequence_ model. There are two\nflavours of Whisper model: English-only and multilingual. The English-only models were trained on the task of English \nspeech recognition. The multilingual models were trained simultaneously on multilingual speech recognition and speech \ntranslation. For speech recognition, the model predicts transcriptions in the *same* language as the audio. For speech \ntranslation, the model predicts transcriptions to a *different* language to the audio.\n\nWhisper checkpoints come in five configurations of varying model sizes. The smallest four are available as English-only \nand multilingual. The largest checkpoints are multilingual only. All ten of the pre-trained checkpoints \nare available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The \ncheckpoints are summarised in the following table with links to the models on the Hub:\n\n| Size     | Parameters | English-only                                         | Multilingual                                        |\n|----------|------------|------------------------------------------------------|-----------------------------------------------------|\n| tiny     | 39 M       | [\u2713](https://huggingface.co/openai/whisper-tiny.en)   | [\u2713](https://huggingface.co/openai/whisper-tiny)     |\n| base     | 74 M       | [\u2713](https://huggingface.co/openai/whisper-base.en)   | [\u2713](https://huggingface.co/openai/whisper-base)     |\n| small    | 244 M      | [\u2713](https://huggingface.co/openai/whisper-small.en)  | [\u2713](https://huggingface.co/openai/whisper-small)    |\n| medium   | 769 M      | [\u2713](https://huggingface.co/openai/whisper-medium.en) | [\u2713](https://huggingface.co/openai/whisper-medium)   |\n| large    | 1550 M     | x                                                    | [\u2713](https://huggingface.co/openai/whisper-large)    |\n| large-v2 | 1550 M     | x                                                    | [\u2713](https://huggingface.co/openai/whisper-large-v2) |\n| large-v3 | 1550 M     | x                                                    | [\u2713](https://huggingface.co/openai/whisper-large-v3) |\n\n\n## Fine-Tuning\n\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However, \nits predictive capabilities can be improved further for certain languages and tasks through *fine-tuning*. The blog \npost [Fine-Tune Whisper with \ud83e\udd17 Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step \nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\n\n### Evaluated Use\n\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only \u201cintended\u201d uses or to draw reasonable guidelines around what is or is not research.\n\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\n\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\n\n\n## Training Data\n\nThe large-v3 checkpoint is trained on 1 million hours of weakly labeled audio and 4 million hours of pseudo-labeled audio collected using Whisper large-v2. \n\nAs discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.\n\n\n## Performance and Limitations\n\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf). \n\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\n\n\n## Broader Implications\n\nWe anticipate that Whisper models\u2019 transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box \u2013 their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\n\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\n\n\n### BibTeX entry and citation info\n```bibtex\n@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n```"
    },
    {
      "@id": "ark:59852/model-autogluon-chronos-bolt-base-6xmiq3ggblp",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "autogluon/chronos-bolt-base",
      "description": "\ud83d\ude80 **Update Feb 14, 2025**: Chronos-Bolt models are now available on Amazon SageMaker JumpStart! Check out the [tutorial notebook](https://github.com/amazon-science/chronos-forecasting/blob/main/notebooks/deploy-chronos-to-amazon-sagemaker.ipynb) to learn how to deploy Chronos endpoints for production use in a few lines of code.",
      "author": "autogluon",
      "keywords": [
        "safetensors",
        "t5",
        "time series",
        "forecasting",
        "pretrained models",
        "foundation models",
        "time series foundation models",
        "time-series",
        "time-series-forecasting",
        "arxiv:1910.10683",
        "arxiv:2403.07815",
        "license:apache-2.0",
        "region:us"
      ],
      "version": "1.0",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "usageInformation": "Install the required dependencies.\n```\npip install autogluon\n```\nForecast with the Chronos-Bolt model.\n```python\nfrom autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n\ndf = TimeSeriesDataFrame(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_hourly/train.csv\")\n\npredictor = TimeSeriesPredictor(prediction_length=48).fit(\n    df,\n    hyperparameters={\n        \"Chronos\": {\"model_path\": \"autogluon/chronos-bolt-base\"},\n    },\n)\n\npredictions = predictor.predict(df)\n```\n\nFor more advanced features such as **fine-tuning** and **forecasting with covariates**, check out [this tutorial](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-chronos.html).",
      "contentUrl": "https://huggingface.co/autogluon/chronos-bolt-base/resolve/main/model.safetensors",
      "url": "https://huggingface.co/autogluon/chronos-bolt-base",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Chronos-Bolt\u26a1 (Base)\n\n\ud83d\ude80 **Update Feb 14, 2025**: Chronos-Bolt models are now available on Amazon SageMaker JumpStart! Check out the [tutorial notebook](https://github.com/amazon-science/chronos-forecasting/blob/main/notebooks/deploy-chronos-to-amazon-sagemaker.ipynb) to learn how to deploy Chronos endpoints for production use in a few lines of code.\n\nChronos-Bolt is a family of pretrained time series forecasting models which can be used for zero-shot forecasting. It is based on the [T5 encoder-decoder architecture](https://arxiv.org/abs/1910.10683) and has been trained on nearly 100 billion time series observations. It chunks the historical time series context into patches of multiple observations, which are then input into the encoder. The decoder then uses these representations to directly generate quantile forecasts across multiple future steps\u2014a method known as direct multi-step forecasting. Chronos-Bolt models are **more accurate**, up to **250 times faster** and **20 times more memory-efficient** than the [original Chronos](https://arxiv.org/abs/2403.07815) models of the same size.\n\n## Performance\n\nThe following plot compares the inference time of Chronos-Bolt against the original Chronos models for forecasting 1024 time series with a context length of 512 observations and a prediction horizon of 64 steps.\n\n<center>\n<img src=\"https://autogluon.s3.amazonaws.com/images/chronos_bolt_speed.svg\" width=\"50%\"/>\n</center>\n\nChronos-Bolt models are not only significantly faster but also more accurate than the original Chronos models. The following plot reports the probabilistic and point forecasting performance of Chronos-Bolt in terms of the [Weighted Quantile Loss (WQL)](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-metrics.html#autogluon.timeseries.metrics.WQL) and the [Mean Absolute Scaled Error (MASE)](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-metrics.html#autogluon.timeseries.metrics.MASE), respectively, aggregated over 27 datasets (see the [Chronos paper](https://arxiv.org/abs/2403.07815) for details on this benchmark). Remarkably, despite having no prior exposure to these datasets during training, the zero-shot Chronos-Bolt models outperform commonly used statistical models and deep learning models that have been trained on these datasets (highlighted by *). Furthermore, they also perform better than other FMs, denoted by a +, which indicates that these models were pretrained on certain datasets in our benchmark and are not entirely zero-shot. Notably, Chronos-Bolt (Base) also surpasses the original Chronos (Large) model in terms of the forecasting accuracy while being over 600 times faster.\n\n<center>\n<img src=\"https://autogluon.s3.amazonaws.com/images/chronos_bolt_accuracy.svg\" width=\"80%\"/>\n</center>\n\nChronos-Bolt models are available in the following sizes.\n\n\n<div align=\"center\">\n\n| Model                                                                  | Parameters | Based on                                                               |\n| ---------------------------------------------------------------------- | ---------- | ---------------------------------------------------------------------- |\n| [**chronos-bolt-tiny**](https://huggingface.co/autogluon/chronos-bolt-tiny)   | 9M         | [t5-efficient-tiny](https://huggingface.co/google/t5-efficient-tiny)   |\n| [**chronos-bolt-mini**](https://huggingface.co/autogluon/chronos-bolt-mini)   | 21M        | [t5-efficient-mini](https://huggingface.co/google/t5-efficient-mini)   |\n| [**chronos-bolt-small**](https://huggingface.co/autogluon/chronos-bolt-small) | 48M        | [t5-efficient-small](https://huggingface.co/google/t5-efficient-small) |\n| [**chronos-bolt-base**](https://huggingface.co/autogluon/chronos-bolt-base)   | 205M       | [t5-efficient-base](https://huggingface.co/google/t5-efficient-base)   |\n\n</div>\n\n\n## Usage\n\n### Zero-shot inference with Chronos-Bolt in AutoGluon\n\nInstall the required dependencies.\n```\npip install autogluon\n```\nForecast with the Chronos-Bolt model.\n```python\nfrom autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n\ndf = TimeSeriesDataFrame(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/m4_hourly/train.csv\")\n\npredictor = TimeSeriesPredictor(prediction_length=48).fit(\n    df,\n    hyperparameters={\n        \"Chronos\": {\"model_path\": \"autogluon/chronos-bolt-base\"},\n    },\n)\n\npredictions = predictor.predict(df)\n```\n\nFor more advanced features such as **fine-tuning** and **forecasting with covariates**, check out [this tutorial](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-chronos.html).\n\n### Deploying a Chronos-Bolt endpoint to SageMaker\nFirst, update the SageMaker SDK to make sure that all the latest models are available.\n```\npip install -U sagemaker\n```\nDeploy an inference endpoint to SageMaker.\n```python\nfrom sagemaker.jumpstart.model import JumpStartModel\n\nmodel = JumpStartModel(\n    model_id=\"autogluon-forecasting-chronos-bolt-base\",\n    instance_type=\"ml.c5.2xlarge\",\n)\npredictor = model.deploy()\n```\nNow you can send time series data to the endpoint in JSON format.\n```python\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\")\n\npayload = {\n    \"inputs\": [\n        {\"target\": df[\"#Passengers\"].tolist()}\n    ],\n    \"parameters\": {\n        \"prediction_length\": 12,\n    }\n}\nforecast = predictor.predict(payload)[\"predictions\"]\n```\nChronos-Bolt models can be deployed to both CPU and GPU instances. These models also support **forecasting with covariates**. For more details about the endpoint API, check out the [example notebook](https://github.com/amazon-science/chronos-forecasting/blob/main/notebooks/deploy-chronos-bolt-to-amazon-sagemaker.ipynb).\n\n## Citation\n\nIf you find Chronos or Chronos-Bolt models useful for your research, please consider citing the associated [paper](https://arxiv.org/abs/2403.07815):\n\n```\n@article{ansari2024chronos,\n    title={Chronos: Learning the Language of Time Series},\n    author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan, and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},\n    journal={Transactions on Machine Learning Research},\n    issn={2835-8856},\n    year={2024},\n    url={https://openreview.net/forum?id=gerNCVqqtR}\n}\n```\n\n## License\n\nThis project is licensed under the Apache-2.0 License.\n"
    },
    {
      "@id": "ark:59852/model-dphn-dolphin-2-9-1-yi-1-5-34b-2bfvdgvymxw",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "dphn/dolphin-2.9.1-yi-1.5-34b",
      "description": "Curated and trained by Eric Hartford, Lucas Atkins, and Fernando Fernandes, and Cognitive Computations",
      "author": "dphn",
      "keywords": [
        "transformers",
        "safetensors",
        "llama",
        "text-generation",
        "generated_from_trainer",
        "axolotl",
        "conversational",
        "dataset:cognitivecomputations/Dolphin-2.9",
        "dataset:teknium/OpenHermes-2.5",
        "dataset:m-a-p/CodeFeedback-Filtered-Instruction",
        "dataset:cognitivecomputations/dolphin-coder",
        "dataset:cognitivecomputations/samantha-data",
        "dataset:microsoft/orca-math-word-problems-200k",
        "dataset:Locutusque/function-calling-chatml",
        "dataset:internlm/Agent-FLAN",
        "base_model:01-ai/Yi-1.5-34B",
        "base_model:finetune:01-ai/Yi-1.5-34B",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-generation",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/cognitivecomputations/Dolphin-2.9"
        },
        {
          "@id": "https://huggingface.co/datasets/teknium/OpenHermes-2.5"
        },
        {
          "@id": "https://huggingface.co/datasets/m-a-p/CodeFeedback-Filtered-Instruction"
        },
        {
          "@id": "https://huggingface.co/datasets/cognitivecomputations/dolphin-coder"
        },
        {
          "@id": "https://huggingface.co/datasets/cognitivecomputations/samantha-data"
        },
        {
          "@id": "https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k"
        },
        {
          "@id": "https://huggingface.co/datasets/Locutusque/function-calling-chatml"
        },
        {
          "@id": "https://huggingface.co/datasets/internlm/Agent-FLAN"
        }
      ],
      "hasBias": "Dolphin is uncensored. We have filtered the dataset to remove alignment and bias. This makes the model more compliant. You are advised to implement your own alignment layer before exposing the model as a service. It will be highly compliant with any requests, even unethical ones. Please read my blog post about uncensored models. https://erichartford.com/uncensored-models You are responsible for any content you create using this model. Enjoy responsibly.",
      "intendedUseCase": "More information needed",
      "usageInformation": "Our appreciation for the sponsors of Dolphin 2.9.1:\n- [Crusoe Cloud](https://crusoe.ai/) - provided excellent on-demand 8xH100 node\n- [OnDemand](https://on-demand.io/) - provided inference sponsorship",
      "baseModel": "01-ai/Yi-1.5-34B",
      "contentUrl": "https://huggingface.co/dphn/dolphin-2.9.1-yi-1.5-34b/resolve/main/model-00001-of-00015.safetensors",
      "url": "https://huggingface.co/dphn/dolphin-2.9.1-yi-1.5-34b",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Dolphin 2.9.1 Yi 1.5 34b \ud83d\udc2c\n\nCurated and trained by Eric Hartford, Lucas Atkins, and Fernando Fernandes, and Cognitive Computations\n\nThis is our most spectacular outcome ever. FFT, all parameters, 16bit.  77.4 MMLU on 34b.  And it talks like a dream.\n\nAlthough the max positional embeddings is 4k, we used rope theta of 1000000.0 and we trained with sequence length 8k.  We plan to train on the upcoming 32k version as well.\n\nWebsite: https://dphn.ai  \nTwitter: https://x.com/dphnAI  \nWeb Chat: https://chat.dphn.ai  \nTelegram bot: https://t.me/DolphinAI_bot\n\n<img src=\"https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/ldkN1J0WIDQwU4vutGYiD.png\" width=\"600\" />\n\nOur appreciation for the sponsors of Dolphin 2.9.1:\n- [Crusoe Cloud](https://crusoe.ai/) - provided excellent on-demand 8xH100 node\n- [OnDemand](https://on-demand.io/) - provided inference sponsorship\n\nThis model is based on Yi-1.5-34b, and is governed by apache 2.0 license.\n\nThe base model has 4k context, but we used rope theta of 1000000.0 and the full-weight fine-tuning was with 8k sequence length.\n\nDolphin 2.9.1 uses ChatML prompt template format.\n\nexample:\n\n```\n<|im_start|>system\nYou are Dolphin, a helpful AI assistant.<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n\n```\n\nDolphin-2.9.1 has a variety of instruction, conversational, and coding skills. It also has initial agentic abilities and supports function calling.\n\nDolphin is uncensored. We have filtered the dataset to remove alignment and bias. This makes the model more compliant. You are advised to implement your own alignment layer before exposing the model as a service. It will be highly compliant with any requests, even unethical ones. Please read my blog post about uncensored models. https://erichartford.com/uncensored-models You are responsible for any content you create using this model. Enjoy responsibly.\n\nDolphin is licensed according to apache 2.0 license.  We grant permission for any use, including commercial. Dolphin was trained on data generated from GPT4, among other models.\n\n## Evals\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/coI4WEJEJD4lhSWgMOjIr.png)\n\n## Training\n\n[<img src=\"https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/image/axolotl-badge-web.png\" alt=\"Built with Axolotl\" width=\"200\" height=\"32\"/>](https://github.com/OpenAccess-AI-Collective/axolotl)\n<details><summary>See axolotl config</summary>\n\naxolotl version: `0.4.0`\n```yaml\nbase_model: 01-ai/Yi-1.5-34B\nmodel_type: LlamaForCausalLM\ntokenizer_type: LlamaTokenizer\ntrust_remote_code: true\n\n# load_in_8bit: false\n# load_in_4bit: true\n# strict: false\n\n# adapter: qlora\n# lora_modules_to_save: [embed_tokens, lm_head]\n\n# lora_r: 32\n# lora_alpha: 16\n# lora_dropout: 0.05\n# lora_target_linear: True\n# lora_fan_in_fan_out:\n\ndatasets:\n  - path: /workspace/datasets/dolphin-2.9/dolphin201-sharegpt2.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/dolphin-coder-translate-sharegpt2.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/dolphin-coder-codegen-sharegpt2.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/m-a-p_Code-Feedback-sharegpt-unfiltered.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/m-a-p_CodeFeedback-Filtered-Instruction-sharegpt-unfiltered.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/not_samantha_norefusals.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/Orca-Math-resort-unfiltered.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/agent_instruct_react_unfiltered.jsonl\n    type: sharegpt  \n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/toolbench_instruct_j1s1_3k_unfiltered.jsonl\n    type: sharegpt  \n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/toolbench_negative_unfiltered.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/toolbench_react_10p_unfiltered.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/toolbench_tflan_cot_30p_unfiltered.jsonl\n    type: sharegpt\n    conversation: chatml\n  - path: /workspace/datasets/dolphin-2.9/openhermes200k_unfiltered.jsonl\n    type: sharegpt \n    conversation: chatml\n\nchat_template: chatml\n\ndataset_prepared_path: yi34b\nval_set_size: 0.01\noutput_dir: ./out-yi\n\nsequence_len: 8192\nsample_packing: true\npad_to_sequence_len: true\n\nwandb_project: dolphin-2.9-yi-34b\nwandb_watch:\nwandb_run_id:\nwandb_log_model:\n\ngradient_accumulation_steps: 8\nmicro_batch_size: 1\nnum_epochs: 3\noptimizer: adamw_8bit\nlr_scheduler: cosine\nlearning_rate: 1e-5\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: auto\nfp16:\ntf32: true\n\ngradient_checkpointing: true\ngradient_checkpointing_kwargs:\n  use_reentrant: false\nearly_stopping_patience:\n# resume_from_checkpoint: /workspace/axolotl/dbrx-checkpoint\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\nwarmup_steps: 10\nevals_per_epoch: 4\neval_table_size:\nsaves_per_epoch: 4\nsave_total_limit: 2\nsave_steps:\ndebug:\ndeepspeed: /workspace/axolotl/deepspeed_configs/zero3_bf16.json\nweight_decay: 0.05\nfsdp:\nfsdp_config:\nspecial_tokens:\n  bos_token: \"<|startoftext|>\"\n  eos_token: \"<|im_end|>\"\n  pad_token: \"<unk>\"\n  unk_token: \"<unk>\"\ntokens:\n  - \"<|im_start|>\"\n  \n\n```\n\n</details><br>\n\n# out-yi\n\nThis model is a fine-tuned version of [01-ai/Yi-1.5-34B](https://huggingface.co/01-ai/Yi-1.5-34B) on the None dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.4425\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-05\n- train_batch_size: 1\n- eval_batch_size: 1\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 8\n- gradient_accumulation_steps: 8\n- total_train_batch_size: 64\n- total_eval_batch_size: 8\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_steps: 10\n- num_epochs: 3\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss |\n|:-------------:|:-----:|:----:|:---------------:|\n| 0.6265        | 0.0   | 1    | 0.6035          |\n| 0.4674        | 0.25  | 327  | 0.4344          |\n| 0.4337        | 0.5   | 654  | 0.4250          |\n| 0.4346        | 0.75  | 981  | 0.4179          |\n| 0.3985        | 1.0   | 1308 | 0.4118          |\n| 0.3128        | 1.23  | 1635 | 0.4201          |\n| 0.3261        | 1.48  | 1962 | 0.4157          |\n| 0.3259        | 1.73  | 2289 | 0.4122          |\n| 0.3126        | 1.98  | 2616 | 0.4079          |\n| 0.2265        | 2.21  | 2943 | 0.4441          |\n| 0.2297        | 2.46  | 3270 | 0.4427          |\n| 0.2424        | 2.71  | 3597 | 0.4425          |\n\n\n### Framework versions\n\n- Transformers 4.40.0.dev0\n- Pytorch 2.2.2+cu121\n- Datasets 2.15.0\n- Tokenizers 0.15.0"
    },
    {
      "@id": "ark:59852/model-comfy-org-wan2-1comfyuirepackaged-hfgzzplb5we",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "Comfy-Org/Wan_2.1_ComfyUI_repackaged",
      "description": "Wan 2.1 repackaged for ComfyUI use. For examples see: https://comfyanonymous.github.io/ComfyUI_examples/wan",
      "author": "Comfy-Org",
      "keywords": [
        "diffusion-single-file",
        "comfyui",
        "region:us"
      ],
      "version": "1.0",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "contentUrl": "https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors",
      "url": "https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged",
      "isPartOf": [],
      "README": "\nWan 2.1 repackaged for ComfyUI use. For examples see: https://comfyanonymous.github.io/ComfyUI_examples/wan"
    },
    {
      "@id": "ark:59852/model-cardiffnlp-twitter-roberta-base-sentiment-latest-s6vypzf8fb",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
      "description": "This is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark.",
      "author": "cardiffnlp",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "roberta",
        "text-classification",
        "en",
        "dataset:tweet_eval",
        "arxiv:2202.03829",
        "license:cc-by-4.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-classification",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/tweet_eval"
        }
      ],
      "contentUrl": "https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest",
      "license": "cc-by-4.0",
      "isPartOf": [],
      "README": "\n\n# Twitter-roBERTa-base for Sentiment Analysis - UPDATED (2022)\n\nThis is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. \nThe original Twitter-based RoBERTa model can be found [here](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m) and the original reference paper is [TweetEval](https://github.com/cardiffnlp/tweeteval). This model is suitable for English. \n\n- Reference Paper: [TimeLMs paper](https://arxiv.org/abs/2202.03829). \n- Git Repo: [TimeLMs official repository](https://github.com/cardiffnlp/timelms).\n\n<b>Labels</b>: \n0 -> Negative;\n1 -> Neutral;\n2 -> Positive\n\nThis sentiment analysis model has been integrated into [TweetNLP](https://github.com/cardiffnlp/tweetnlp). You can access the demo [here](https://tweetnlp.org).\n\n## Example Pipeline\n```python\nfrom transformers import pipeline\nsentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)\nsentiment_task(\"Covid cases are increasing fast!\")\n```\n```\n[{'label': 'Negative', 'score': 0.7236}]\n```\n\n## Full classification example\n\n```python\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer, AutoConfig\nimport numpy as np\nfrom scipy.special import softmax\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\nMODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nconfig = AutoConfig.from_pretrained(MODEL)\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\n#model.save_pretrained(MODEL)\ntext = \"Covid cases are increasing fast!\"\ntext = preprocess(text)\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\nscores = output[0][0].detach().numpy()\nscores = softmax(scores)\n# # TF\n# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n# model.save_pretrained(MODEL)\n# text = \"Covid cases are increasing fast!\"\n# encoded_input = tokenizer(text, return_tensors='tf')\n# output = model(encoded_input)\n# scores = output[0][0].numpy()\n# scores = softmax(scores)\n# Print labels and scores\nranking = np.argsort(scores)\nranking = ranking[::-1]\nfor i in range(scores.shape[0]):\n    l = config.id2label[ranking[i]]\n    s = scores[ranking[i]]\n    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n```\n\nOutput: \n\n```\n1) Negative 0.7236\n2) Neutral 0.2287\n3) Positive 0.0477\n```\n\n\n### References \n```\n@inproceedings{camacho-collados-etal-2022-tweetnlp,\n    title = \"{T}weet{NLP}: Cutting-Edge Natural Language Processing for Social Media\",\n    author = \"Camacho-collados, Jose  and\n      Rezaee, Kiamehr  and\n      Riahi, Talayeh  and\n      Ushio, Asahi  and\n      Loureiro, Daniel  and\n      Antypas, Dimosthenis  and\n      Boisson, Joanne  and\n      Espinosa Anke, Luis  and\n      Liu, Fangyu  and\n      Mart{\\'\\i}nez C{\\'a}mara, Eugenio\" and others,\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = dec,\n    year = \"2022\",\n    address = \"Abu Dhabi, UAE\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.emnlp-demos.5\",\n    pages = \"38--49\"\n}\n\n```\n\n```\n@inproceedings{loureiro-etal-2022-timelms,\n    title = \"{T}ime{LM}s: Diachronic Language Models from {T}witter\",\n    author = \"Loureiro, Daniel  and\n      Barbieri, Francesco  and\n      Neves, Leonardo  and\n      Espinosa Anke, Luis  and\n      Camacho-collados, Jose\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-demo.25\",\n    doi = \"10.18653/v1/2022.acl-demo.25\",\n    pages = \"251--260\"\n}\n\n```"
    },
    {
      "@id": "ark:59852/model-facebook-opt-125m-1otyncb7s4s",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "facebook/opt-125m",
      "description": "OPT was first introduced in [Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) and first released in [metaseq's repository](https://github.com/facebookresearch/metaseq) on May 3rd 2022 by Meta AI.",
      "author": "facebook",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "opt",
        "text-generation",
        "en",
        "arxiv:2205.01068",
        "arxiv:2005.14165",
        "license:other",
        "autotrain_compatible",
        "text-generation-inference",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-generation",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [],
      "hasBias": "As mentioned in Meta AI's model card, given that the training data used for this model contains a lot of\nunfiltered content from the internet, which is far from neutral the model is strongly biased : \n\n> Like other large language models for which the diversity (or lack thereof) of training\n> data induces downstream impact on the quality of our model, OPT-175B has limitations in terms\n> of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and\n> hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern\n> large language models. \n\nThis bias will also affect all fine-tuned versions of this model.",
      "intendedUseCase": "The pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.\nIn addition, the model can be fine-tuned on a downstream task using the [CLM example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling). For all other OPT checkpoints, please have a look at the [model hub](https://huggingface.co/models?filter=opt).\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation.\n\n```python\n>>> from transformers import pipeline\n\n>>> generator = pipeline('text-generation', model=\"facebook/opt-125m\")\n>>> generator(\"What are we having for dinner?\")\n[{'generated_text': 'What are we having for dinner?\\nA nice dinner with a friend.\\nI'm not sure'}]\n```\n\nBy default, generation is deterministic. In order to use the top-k sampling, please set `do_sample` to `True`. \n\n```python\n>>> from transformers import pipeline, set_seed\n\n>>> set_seed(32)\n>>> generator = pipeline('text-generation', model=\"facebook/opt-125m\", do_sample=True)\n>>> generator(\"What are we having for dinner?\")\n[{'generated_text': 'What are we having for dinner?\\nCoffee, sausage and cream cheese at Chili's.'}]\n```\n\n\nAs mentioned in Meta AI's model card, given that the training data used for this model contains a lot of\nunfiltered content from the internet, which is far from neutral the model is strongly biased : \n\n> Like other large language models for which the diversity (or lack thereof) of training\n> data induces downstream impact on the quality of our model, OPT-175B has limitations in terms\n> of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and\n> hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern\n> large language models. \n\nThis bias will also affect all fine-tuned versions of this model.",
      "usageInformation": "You can use this model directly with a pipeline for text generation.\n\n```python\n>>> from transformers import pipeline\n\n>>> generator = pipeline('text-generation', model=\"facebook/opt-125m\")\n>>> generator(\"What are we having for dinner?\")\n[{'generated_text': 'What are we having for dinner?\\nA nice dinner with a friend.\\nI'm not sure'}]\n```\n\nBy default, generation is deterministic. In order to use the top-k sampling, please set `do_sample` to `True`. \n\n```python\n>>> from transformers import pipeline, set_seed\n\n>>> set_seed(32)\n>>> generator = pipeline('text-generation', model=\"facebook/opt-125m\", do_sample=True)\n>>> generator(\"What are we having for dinner?\")\n[{'generated_text': 'What are we having for dinner?\\nCoffee, sausage and cream cheese at Chili's.'}]\n```",
      "contentUrl": "https://huggingface.co/facebook/opt-125m/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/facebook/opt-125m",
      "license": "other",
      "isPartOf": [],
      "README": "\n# OPT : Open Pre-trained Transformer Language Models\n\nOPT was first introduced in [Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) and first released in [metaseq's repository](https://github.com/facebookresearch/metaseq) on May 3rd 2022 by Meta AI.\n\n**Disclaimer**: The team releasing OPT wrote an official model card, which is available in Appendix D of the [paper](https://arxiv.org/pdf/2205.01068.pdf). \nContent from **this** model card has been written by the Hugging Face team.\n\n## Intro\n\nTo quote the first two paragraphs of the [official paper](https://arxiv.org/abs/2205.01068)\n\n\n> Large language models trained on massive text collections have shown surprising emergent\n> capabilities to generate text and perform zero- and few-shot learning. While in some cases the public\n> can interact with these models through paid APIs, full model access is currently limited to only a\n> few highly resourced labs. This restricted access has limited researchers\u2019 ability to study how and\n> why these large language models work, hindering progress on improving known challenges in areas\n> such as robustness, bias, and toxicity.\n\n> We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M\n> to 175B parameters, which we aim to fully and responsibly share with interested researchers. We train the OPT models to roughly match \n> the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data\n> collection and efficient training. Our aim in developing this suite of OPT models is to enable reproducible and responsible research at scale, and\n> to bring more voices to the table in studying the impact of these LLMs. Definitions of risk, harm, bias, and toxicity, etc., should be articulated by the\n> collective research community as a whole, which is only possible when models are available for study.\n\n## Model description\n\nOPT was predominantly pretrained with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective.\nOPT belongs to the same family of decoder-only models like [GPT-3](https://arxiv.org/abs/2005.14165). As such, it was pretrained using the self-supervised causal language modedling objective.\n\nFor evaluation, OPT follows [GPT-3](https://arxiv.org/abs/2005.14165) by using their prompts and overall experimental setup. For more details, please read \nthe [official paper](https://arxiv.org/abs/2205.01068).\n## Intended uses & limitations\n\nThe pretrained-only model can be used for prompting for evaluation of downstream tasks as well as text generation.\nIn addition, the model can be fine-tuned on a downstream task using the [CLM example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling). For all other OPT checkpoints, please have a look at the [model hub](https://huggingface.co/models?filter=opt).\n\n### How to use\n\nYou can use this model directly with a pipeline for text generation.\n\n```python\n>>> from transformers import pipeline\n\n>>> generator = pipeline('text-generation', model=\"facebook/opt-125m\")\n>>> generator(\"What are we having for dinner?\")\n[{'generated_text': 'What are we having for dinner?\\nA nice dinner with a friend.\\nI'm not sure'}]\n```\n\nBy default, generation is deterministic. In order to use the top-k sampling, please set `do_sample` to `True`. \n\n```python\n>>> from transformers import pipeline, set_seed\n\n>>> set_seed(32)\n>>> generator = pipeline('text-generation', model=\"facebook/opt-125m\", do_sample=True)\n>>> generator(\"What are we having for dinner?\")\n[{'generated_text': 'What are we having for dinner?\\nCoffee, sausage and cream cheese at Chili's.'}]\n```\n\n### Limitations and bias\n\nAs mentioned in Meta AI's model card, given that the training data used for this model contains a lot of\nunfiltered content from the internet, which is far from neutral the model is strongly biased : \n\n> Like other large language models for which the diversity (or lack thereof) of training\n> data induces downstream impact on the quality of our model, OPT-175B has limitations in terms\n> of bias and safety. OPT-175B can also have quality issues in terms of generation diversity and\n> hallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern\n> large language models. \n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe Meta AI team wanted to train this model on a corpus as large as possible. It is composed of the union of the following 5 filtered datasets of textual documents: \n\n  - BookCorpus, which consists of more than 10K unpublished books,\n  - CC-Stories, which contains a subset of CommonCrawl data filtered to match the\nstory-like style of Winograd schemas,\n  - The Pile, from which * Pile-CC, OpenWebText2, USPTO, Project Gutenberg, OpenSubtitles, Wikipedia, DM Mathematics and HackerNews* were included. \n  - Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in\nRoller et al. (2021)\n  - CCNewsV2 containing an updated version of the English portion of the CommonCrawl News\ndataset that was used in RoBERTa (Liu et al., 2019b)\n\nThe final training data contains 180B tokens corresponding to 800GB of data. The validation split was made of 200MB of the pretraining data, sampled proportionally\nto each dataset\u2019s size in the pretraining corpus. \n\nThe dataset might contains offensive content as parts of the dataset are a subset of\npublic Common Crawl data, along with a subset of public Reddit data, which could contain sentences\nthat, if viewed directly, can be insulting, threatening, or might otherwise cause anxiety.\n\n### Collection process\n\nThe dataset was collected form internet, and went through classic data processing algorithms  and\nre-formatting practices, including removing repetitive/non-informative text like *Chapter One* or\n*This ebook by Project Gutenberg.*\n\n## Training procedure\n\n\n\n### Preprocessing\n\nThe texts are tokenized using the **GPT2** byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a\nvocabulary size of 50272. The inputs are sequences of 2048 consecutive tokens.\n\nThe 175B model was trained on 992 *80GB A100 GPUs*. The training duration was roughly ~33 days of continuous training.\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{zhang2022opt,\n      title={OPT: Open Pre-trained Transformer Language Models}, \n      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},\n      year={2022},\n      eprint={2205.01068},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```"
    },
    {
      "@id": "ark:59852/model-openai-whisper-large-v3-turbo-minkyuyxjob",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "openai/whisper-large-v3-turbo",
      "description": "Whisper is a state-of-the-art model for automatic speech recognition (ASR) and speech translation, proposed in the paper",
      "author": "openai",
      "keywords": [
        "transformers",
        "safetensors",
        "whisper",
        "automatic-speech-recognition",
        "audio",
        "en",
        "zh",
        "de",
        "es",
        "ru",
        "ko",
        "fr",
        "ja",
        "pt",
        "tr",
        "pl",
        "ca",
        "nl",
        "ar",
        "sv",
        "it",
        "id",
        "hi",
        "fi",
        "vi",
        "he",
        "uk",
        "el",
        "ms",
        "cs",
        "ro",
        "da",
        "hu",
        "ta",
        "no",
        "th",
        "ur",
        "hr",
        "bg",
        "lt",
        "la",
        "mi",
        "ml",
        "cy",
        "sk",
        "te",
        "fa",
        "lv",
        "bn",
        "sr",
        "az",
        "sl",
        "kn",
        "et",
        "mk",
        "br",
        "eu",
        "is",
        "hy",
        "ne",
        "mn",
        "bs",
        "kk",
        "sq",
        "sw",
        "gl",
        "mr",
        "pa",
        "si",
        "km",
        "sn",
        "yo",
        "so",
        "af",
        "oc",
        "ka",
        "be",
        "tg",
        "sd",
        "gu",
        "am",
        "yi",
        "lo",
        "uz",
        "fo",
        "ht",
        "ps",
        "tk",
        "nn",
        "mt",
        "sa",
        "lb",
        "my",
        "bo",
        "tl",
        "mg",
        "as",
        "tt",
        "haw",
        "ln",
        "ha",
        "ba",
        "jw",
        "su",
        "arxiv:2212.04356",
        "base_model:openai/whisper-large-v3",
        "base_model:finetune:openai/whisper-large-v3",
        "license:mit",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "hasBias": "The primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only \u201cintended\u201d uses or to draw reasonable guidelines around what is or is not research.",
      "intendedUseCase": "Our studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf). \n\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.",
      "usageInformation": "Whisper large-v3-turbo is supported in Hugging Face \ud83e\udd17 Transformers. To run the model, first install the Transformers \nlibrary. For this example, we'll also install \ud83e\udd17 Datasets to load toy audio dataset from the Hugging Face Hub, and \n\ud83e\udd17 Accelerate to reduce the model loading time:\n\n```bash\npip install --upgrade pip\npip install --upgrade transformers datasets[audio] accelerate\n```\n\nThe model can be used with the [`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)\nclass to transcribe audios of arbitrary length:\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3-turbo\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\nresult = pipe(sample)\nprint(result[\"text\"])\n```\n\nTo transcribe a local audio file, simply pass the path to your audio file when you call the pipeline:\n\n```python\nresult = pipe(\"audio.mp3\")\n```\n\nMultiple audio files can be transcribed in parallel by specifying them as a list and setting the `batch_size` parameter:\n\n```python\nresult = pipe([\"audio_1.mp3\", \"audio_2.mp3\"], batch_size=2)\n```\n\nTransformers is compatible with all Whisper decoding strategies, such as temperature fallback and condition on previous \ntokens. The following example demonstrates how to enable these heuristics:\n\n```python\ngenerate_kwargs = {\n    \"max_new_tokens\": 448,\n    \"num_beams\": 1,\n    \"condition_on_prev_tokens\": False,\n    \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    \"logprob_threshold\": -1.0,\n    \"no_speech_threshold\": 0.6,\n    \"return_timestamps\": True,\n}\n\nresult = pipe(sample, generate_kwargs=generate_kwargs)\n```\n\nWhisper predicts the language of the source audio automatically. If the source audio language is known *a-priori*, it \ncan be passed as an argument to the pipeline:\n\n```python\nresult = pipe(sample, generate_kwargs={\"language\": \"english\"})\n```\n\nBy default, Whisper performs the task of *speech transcription*, where the source audio language is the same as the target\ntext language. To perform *speech translation*, where the target text is in English, set the task to `\"translate\"`:\n\n```python\nresult = pipe(sample, generate_kwargs={\"task\": \"translate\"})\n```\n\nFinally, the model can be made to predict timestamps. For sentence-level timestamps, pass the `return_timestamps` argument:\n\n```python\nresult = pipe(sample, return_timestamps=True)\nprint(result[\"chunks\"])\n```\n\nAnd for word-level timestamps:\n\n```python\nresult = pipe(sample, return_timestamps=\"word\")\nprint(result[\"chunks\"])\n```\n\nThe above arguments can be used in isolation or in combination. For example, to perform the task of speech transcription \nwhere the source audio is in French, and we want to return sentence-level timestamps, the following can be used:\n\n```python\nresult = pipe(sample, return_timestamps=True, generate_kwargs={\"language\": \"french\", \"task\": \"translate\"})\nprint(result[\"chunks\"])\n```\n\n<details>\n\n<summary> For more control over the generation parameters, use the model + processor API directly: </summary>\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\nfrom datasets import Audio, load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3-turbo\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\ndataset = dataset.cast_column(\"audio\", Audio(processor.feature_extractor.sampling_rate))\nsample = dataset[0][\"audio\"]\n\ninputs = processor(\n    sample[\"array\"],\n    sampling_rate=sample[\"sampling_rate\"],\n    return_tensors=\"pt\",\n    truncation=False,\n    padding=\"longest\",\n    return_attention_mask=True,\n)\ninputs = inputs.to(device, dtype=torch_dtype)\n\ngen_kwargs = {\n    \"max_new_tokens\": 448,\n    \"num_beams\": 1,\n    \"condition_on_prev_tokens\": False,\n    \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    \"logprob_threshold\": -1.0,\n    \"no_speech_threshold\": 0.6,\n    \"return_timestamps\": True,\n}\n\npred_ids = model.generate(**inputs, **gen_kwargs)\npred_text = processor.batch_decode(pred_ids, skip_special_tokens=True, decode_with_timestamps=False)\n\nprint(pred_text)\n```\n\n</details>",
      "baseModel": "openai/whisper-large-v3",
      "contentUrl": "https://huggingface.co/openai/whisper-large-v3-turbo/resolve/main/model.safetensors",
      "url": "https://huggingface.co/openai/whisper-large-v3-turbo",
      "license": "mit",
      "isPartOf": [],
      "README": "\n# Whisper\n\nWhisper is a state-of-the-art model for automatic speech recognition (ASR) and speech translation, proposed in the paper \n[Robust Speech Recognition via Large-Scale Weak Supervision](https://huggingface.co/papers/2212.04356) by Alec Radford \net al. from OpenAI. Trained on >5M hours of labeled data, Whisper demonstrates a strong ability to generalise to many \ndatasets and domains in a zero-shot setting.\n\nWhisper large-v3-turbo is a finetuned version of a pruned [Whisper large-v3](https://huggingface.co/openai/whisper-large-v3). In other words, it's the exact same model, except that the number of decoding layers have reduced from 32 to 4.\nAs a result, the model is way faster, at the expense of a minor quality degradation. You can find more details about it [in this GitHub discussion](https://github.com/openai/whisper/discussions/2363).\n\n**Disclaimer**: Content for this model card has partly been written by the \ud83e\udd17 Hugging Face team, and partly copied and \npasted from the original model card.\n\n## Usage\n\nWhisper large-v3-turbo is supported in Hugging Face \ud83e\udd17 Transformers. To run the model, first install the Transformers \nlibrary. For this example, we'll also install \ud83e\udd17 Datasets to load toy audio dataset from the Hugging Face Hub, and \n\ud83e\udd17 Accelerate to reduce the model loading time:\n\n```bash\npip install --upgrade pip\npip install --upgrade transformers datasets[audio] accelerate\n```\n\nThe model can be used with the [`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline)\nclass to transcribe audios of arbitrary length:\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3-turbo\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\nresult = pipe(sample)\nprint(result[\"text\"])\n```\n\nTo transcribe a local audio file, simply pass the path to your audio file when you call the pipeline:\n\n```python\nresult = pipe(\"audio.mp3\")\n```\n\nMultiple audio files can be transcribed in parallel by specifying them as a list and setting the `batch_size` parameter:\n\n```python\nresult = pipe([\"audio_1.mp3\", \"audio_2.mp3\"], batch_size=2)\n```\n\nTransformers is compatible with all Whisper decoding strategies, such as temperature fallback and condition on previous \ntokens. The following example demonstrates how to enable these heuristics:\n\n```python\ngenerate_kwargs = {\n    \"max_new_tokens\": 448,\n    \"num_beams\": 1,\n    \"condition_on_prev_tokens\": False,\n    \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    \"logprob_threshold\": -1.0,\n    \"no_speech_threshold\": 0.6,\n    \"return_timestamps\": True,\n}\n\nresult = pipe(sample, generate_kwargs=generate_kwargs)\n```\n\nWhisper predicts the language of the source audio automatically. If the source audio language is known *a-priori*, it \ncan be passed as an argument to the pipeline:\n\n```python\nresult = pipe(sample, generate_kwargs={\"language\": \"english\"})\n```\n\nBy default, Whisper performs the task of *speech transcription*, where the source audio language is the same as the target\ntext language. To perform *speech translation*, where the target text is in English, set the task to `\"translate\"`:\n\n```python\nresult = pipe(sample, generate_kwargs={\"task\": \"translate\"})\n```\n\nFinally, the model can be made to predict timestamps. For sentence-level timestamps, pass the `return_timestamps` argument:\n\n```python\nresult = pipe(sample, return_timestamps=True)\nprint(result[\"chunks\"])\n```\n\nAnd for word-level timestamps:\n\n```python\nresult = pipe(sample, return_timestamps=\"word\")\nprint(result[\"chunks\"])\n```\n\nThe above arguments can be used in isolation or in combination. For example, to perform the task of speech transcription \nwhere the source audio is in French, and we want to return sentence-level timestamps, the following can be used:\n\n```python\nresult = pipe(sample, return_timestamps=True, generate_kwargs={\"language\": \"french\", \"task\": \"translate\"})\nprint(result[\"chunks\"])\n```\n\n<details>\n\n<summary> For more control over the generation parameters, use the model + processor API directly: </summary>\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\nfrom datasets import Audio, load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3-turbo\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\ndataset = dataset.cast_column(\"audio\", Audio(processor.feature_extractor.sampling_rate))\nsample = dataset[0][\"audio\"]\n\ninputs = processor(\n    sample[\"array\"],\n    sampling_rate=sample[\"sampling_rate\"],\n    return_tensors=\"pt\",\n    truncation=False,\n    padding=\"longest\",\n    return_attention_mask=True,\n)\ninputs = inputs.to(device, dtype=torch_dtype)\n\ngen_kwargs = {\n    \"max_new_tokens\": 448,\n    \"num_beams\": 1,\n    \"condition_on_prev_tokens\": False,\n    \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    \"logprob_threshold\": -1.0,\n    \"no_speech_threshold\": 0.6,\n    \"return_timestamps\": True,\n}\n\npred_ids = model.generate(**inputs, **gen_kwargs)\npred_text = processor.batch_decode(pred_ids, skip_special_tokens=True, decode_with_timestamps=False)\n\nprint(pred_text)\n```\n\n</details>\n\n## Additional Speed & Memory Improvements\n\nYou can apply additional speed and memory improvements to Whisper to further reduce the inference speed and VRAM \nrequirements.\n\n### Chunked Long-Form\n\nWhisper has a receptive field of 30-seconds. To transcribe audios longer than this, one of two long-form algorithms are\nrequired:\n1. **Sequential:** uses a \"sliding window\" for buffered inference, transcribing 30-second slices one after the other\n2. **Chunked:** splits long audio files into shorter ones (with a small overlap between segments), transcribes each segment independently, and stitches the resulting transcriptions at the boundaries\n\nThe sequential long-form algorithm should be used in either of the following scenarios:\n1. Transcription accuracy is the most important factor, and speed is less of a consideration\n2. You are transcribing **batches** of long audio files, in which case the latency of sequential is comparable to chunked, while being up to 0.5% WER more accurate\n\nConversely, the chunked algorithm should be used when:\n1. Transcription speed is the most important factor\n2. You are transcribing a **single** long audio file\n\nBy default, Transformers uses the sequential algorithm. To enable the chunked algorithm, pass the `chunk_length_s` \nparameter to the `pipeline`. For large-v3, a chunk length of 30-seconds is optimal. To activate batching over long \naudio files, pass the argument `batch_size`:\n\n```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3-turbo\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    chunk_length_s=30,\n    batch_size=16,  # batch size for inference - set based on your device\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\nresult = pipe(sample)\nprint(result[\"text\"])\n```\n\n#### Torch compile\n\nThe Whisper forward pass is compatible with [`torch.compile`](https://pytorch.org/docs/stable/generated/torch.compile.html)\nfor 4.5x speed-ups.\n\n**Note:** `torch.compile` is currently not compatible with the Chunked long-form algorithm or Flash Attention 2 \u26a0\ufe0f\n\n```python\nimport torch\nfrom torch.nn.attention import SDPBackend, sdpa_kernel\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\ntorch.set_float32_matmul_precision(\"high\")\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id = \"openai/whisper-large-v3-turbo\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n).to(device)\n\n# Enable static cache and compile the forward pass\nmodel.generation_config.cache_implementation = \"static\"\nmodel.generation_config.max_new_tokens = 256\nmodel.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\n# 2 warmup steps\nfor _ in tqdm(range(2), desc=\"Warm-up step\"):\n    with sdpa_kernel(SDPBackend.MATH):\n        result = pipe(sample.copy(), generate_kwargs={\"min_new_tokens\": 256, \"max_new_tokens\": 256})\n\n# fast run\nwith sdpa_kernel(SDPBackend.MATH):\n    result = pipe(sample.copy())\n\nprint(result[\"text\"])\n```\n\n#### Flash Attention 2\n\nWe recommend using [Flash-Attention 2](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#flashattention-2) if your GPU supports it and you are not using [torch.compile](#torch-compile). \nTo do so, first install [Flash Attention](https://github.com/Dao-AILab/flash-attention):\n\n```\npip install flash-attn --no-build-isolation\n```\n\nThen pass `attn_implementation=\"flash_attention_2\"` to `from_pretrained`:\n\n```python\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation=\"flash_attention_2\")\n```\n\n#### Torch Scale-Product-Attention (SDPA)\n\nIf your GPU does not support Flash Attention, we recommend making use of PyTorch [scaled dot-product attention (SDPA)](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html). \nThis attention implementation is activated **by default** for PyTorch versions 2.1.1 or greater. To check \nwhether you have a compatible PyTorch version, run the following Python code snippet:\n\n```python\nfrom transformers.utils import is_torch_sdpa_available\n\nprint(is_torch_sdpa_available())\n```\n\nIf the above returns `True`, you have a valid version of PyTorch installed and SDPA is activated by default. If it \nreturns `False`, you need to upgrade your PyTorch version according to the [official instructions](https://pytorch.org/get-started/locally/)\n\nOnce a valid PyTorch version is installed, SDPA is activated by default. It can also be set explicitly by specifying \n`attn_implementation=\"sdpa\"` as follows:\n\n```python\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, attn_implementation=\"sdpa\")\n```\n\nFor more information about how to use the SDPA refer to the [Transformers SDPA documentation](https://huggingface.co/docs/transformers/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention).\n\n\n## Model details\n\nWhisper is a Transformer based encoder-decoder model, also referred to as a _sequence-to-sequence_ model. There are two\nflavours of Whisper model: English-only and multilingual. The English-only models were trained on the task of English \nspeech recognition. The multilingual models were trained simultaneously on multilingual speech recognition and speech \ntranslation. For speech recognition, the model predicts transcriptions in the *same* language as the audio. For speech \ntranslation, the model predicts transcriptions to a *different* language to the audio.\n\nWhisper checkpoints come in five configurations of varying model sizes. The smallest four are available as English-only \nand multilingual. The largest checkpoints are multilingual only. All ten of the pre-trained checkpoints \nare available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The \ncheckpoints are summarised in the following table with links to the models on the Hub:\n\n| Size     | Parameters | English-only                                         | Multilingual                                        |\n|----------|------------|------------------------------------------------------|-----------------------------------------------------|\n| tiny     | 39 M       | [\u2713](https://huggingface.co/openai/whisper-tiny.en)   | [\u2713](https://huggingface.co/openai/whisper-tiny)     |\n| base     | 74 M       | [\u2713](https://huggingface.co/openai/whisper-base.en)   | [\u2713](https://huggingface.co/openai/whisper-base)     |\n| small    | 244 M      | [\u2713](https://huggingface.co/openai/whisper-small.en)  | [\u2713](https://huggingface.co/openai/whisper-small)    |\n| medium   | 769 M      | [\u2713](https://huggingface.co/openai/whisper-medium.en) | [\u2713](https://huggingface.co/openai/whisper-medium)   |\n| large    | 1550 M     | x                                                    | [\u2713](https://huggingface.co/openai/whisper-large)    |\n| large-v2 | 1550 M     | x                                                    | [\u2713](https://huggingface.co/openai/whisper-large-v2) |\n| large-v3 | 1550 M     | x                                                    | [\u2713](https://huggingface.co/openai/whisper-large-v3) |\n| large-v3-turbo | 809 M     | x                                                    | [\u2713](https://huggingface.co/openai/whisper-large-v3-turbo) |\n\n\n## Fine-Tuning\n\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However, \nits predictive capabilities can be improved further for certain languages and tasks through *fine-tuning*. The blog \npost [Fine-Tune Whisper with \ud83e\udd17 Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step \nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\n\n### Evaluated Use\n\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only \u201cintended\u201d uses or to draw reasonable guidelines around what is or is not research.\n\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\n\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\n\n\n## Training Data\n\nNo information provided.\n\n## Performance and Limitations\n\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf). \n\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\n\n\n## Broader Implications\n\nWe anticipate that Whisper models\u2019 transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box \u2013 their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\n\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\n\n\n### BibTeX entry and citation info\n```bibtex\n@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n```"
    },
    {
      "@id": "ark:59852/model-nlpaueb-legal-bert-base-uncased-jriwtx1aar",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "nlpaueb/legal-bert-base-uncased",
      "description": "<img align=\"left\" src=\"https://i.ibb.co/p3kQ7Rw/Screenshot-2020-10-06-at-12-16-36-PM.png\" width=\"100\"/>",
      "author": "nlpaueb",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "bert",
        "pretraining",
        "legal",
        "fill-mask",
        "en",
        "license:cc-by-sa-4.0",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [],
      "intendedUseCase": "LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications.  To pre-train the different variations of LEGAL-BERT, we collected 12 GB of diverse English legal text from several fields (e.g., legislation, court cases,  contracts) scraped from publicly available resources. Sub-domain variants (CONTRACTS-, EURLEX-, ECHR-) and/or general LEGAL-BERT perform better than using BERT out of the box for domain-specific tasks. A light-weight model (33% the size of BERT-BASE) pre-trained from scratch on legal data with competitive performance is also available.\n<br/><br/>",
      "usageInformation": "| Corpus                             | Model                               | Masked token | Predictions  |\n| --------------------------------- | ---------------------------------- | ------------ | ------------ |\n|  | **BERT-BASE-UNCASED**                 |\n| (Contracts) | This [MASK] Agreement is between General Motors and John Murray . | employment | ('new', '0.09'), ('current', '0.04'), ('proposed', '0.03'), ('marketing', '0.03'), ('joint', '0.02')\n| (ECHR) | The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of Adana Security Directorate | torture | ('torture', '0.32'), ('rape', '0.22'), ('abuse', '0.14'), ('death', '0.04'), ('violence', '0.03')\n| (EURLEX) | Establishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products . | bovine | ('farm', '0.25'), ('livestock', '0.08'), ('draft', '0.06'), ('domestic', '0.05'), ('wild', '0.05')\n|  | **CONTRACTS-BERT-BASE**                 |\n| (Contracts) | This [MASK] Agreement is between General Motors and John Murray . | employment | ('letter', '0.38'), ('dealer', '0.04'), ('employment', '0.03'), ('award', '0.03'), ('contribution', '0.02')\n| (ECHR) | The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of Adana Security Directorate | torture | ('death', '0.39'), ('imprisonment', '0.07'), ('contempt', '0.05'), ('being', '0.03'), ('crime', '0.02')\n| (EURLEX) | Establishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products . | bovine | (('domestic', '0.18'), ('laboratory', '0.07'), ('household', '0.06'), ('personal', '0.06'), ('the', '0.04')\n|  | **EURLEX-BERT-BASE**                 |\n| (Contracts) | This [MASK] Agreement is between General Motors and John Murray . | employment | ('supply', '0.11'), ('cooperation', '0.08'), ('service', '0.07'), ('licence', '0.07'), ('distribution', '0.05')\n| (ECHR) | The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of Adana Security Directorate | torture | ('torture', '0.66'), ('death', '0.07'), ('imprisonment', '0.07'), ('murder', '0.04'), ('rape', '0.02')\n| (EURLEX) | Establishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products . | bovine | ('live', '0.43'), ('pet', '0.28'), ('certain', '0.05'), ('fur', '0.03'), ('the', '0.02')\n|  | **ECHR-BERT-BASE**                 |\n| (Contracts) | This [MASK] Agreement is between General Motors and John Murray . | employment | ('second', '0.24'), ('latter', '0.10'), ('draft', '0.05'), ('bilateral', '0.05'), ('arbitration', '0.04')\n| (ECHR) | The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of Adana Security Directorate | torture | ('torture', '0.99'), ('death', '0.01'), ('inhuman', '0.00'), ('beating', '0.00'), ('rape', '0.00')\n| (EURLEX) | Establishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products . | bovine | ('pet', '0.17'), ('all', '0.12'), ('slaughtered', '0.10'), ('domestic', '0.07'), ('individual', '0.05')\n|  | **LEGAL-BERT-BASE**                |\n| (Contracts) | This [MASK] Agreement is between General Motors and John Murray . | employment | ('settlement', '0.26'), ('letter', '0.23'), ('dealer', '0.04'), ('master', '0.02'), ('supplemental', '0.02')\n| (ECHR) | The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of Adana Security Directorate | torture | ('torture', '1.00'), ('detention', '0.00'), ('arrest', '0.00'), ('rape', '0.00'), ('death', '0.00')\n| (EURLEX) | Establishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products . | bovine | ('live', '0.67'), ('beef', '0.17'), ('farm', '0.03'), ('pet', '0.02'), ('dairy', '0.01')\n|  | **LEGAL-BERT-SMALL**                |\n| (Contracts) | This [MASK] Agreement is between General Motors and John Murray . | employment | ('license', '0.09'), ('transition', '0.08'), ('settlement', '0.04'), ('consent', '0.03'), ('letter', '0.03')\n| (ECHR) | The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of Adana Security Directorate | torture | ('torture', '0.59'), ('pain', '0.05'), ('ptsd', '0.05'), ('death', '0.02'), ('tuberculosis', '0.02')\n| (EURLEX) | Establishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products . | bovine | ('all', '0.08'), ('live', '0.07'), ('certain', '0.07'), ('the', '0.07'), ('farm', '0.05')",
      "contentUrl": "https://huggingface.co/nlpaueb/legal-bert-base-uncased/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/nlpaueb/legal-bert-base-uncased",
      "license": "cc-by-sa-4.0",
      "isPartOf": [],
      "README": "\n# LEGAL-BERT: The Muppets straight out of Law School\n\n<img align=\"left\" src=\"https://i.ibb.co/p3kQ7Rw/Screenshot-2020-10-06-at-12-16-36-PM.png\" width=\"100\"/> \n\nLEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications.  To pre-train the different variations of LEGAL-BERT, we collected 12 GB of diverse English legal text from several fields (e.g., legislation, court cases,  contracts) scraped from publicly available resources. Sub-domain variants (CONTRACTS-, EURLEX-, ECHR-) and/or general LEGAL-BERT perform better than using BERT out of the box for domain-specific tasks. A light-weight model (33% the size of BERT-BASE) pre-trained from scratch on legal data with competitive performance is also available.\n<br/><br/>\n\n---\n\nI. Chalkidis, M. Fergadiotis, P. Malakasiotis, N. Aletras and I. Androutsopoulos. \"LEGAL-BERT: The Muppets straight out of Law School\". In Findings of Empirical Methods in Natural Language Processing (EMNLP 2020) (Short Papers), to be held online, 2020. (https://aclanthology.org/2020.findings-emnlp.261)\n\n---\n\n## Pre-training corpora\n\nThe pre-training corpora of LEGAL-BERT include:\n\n* 116,062 documents of EU legislation, publicly available from EURLEX (http://eur-lex.europa.eu), the repository of EU Law running under the EU Publication Office.\n    \n* 61,826 documents of UK legislation, publicly available from the UK legislation portal (http://www.legislation.gov.uk).\n    \n* 19,867 cases from the European Court of Justice (ECJ), also available from EURLEX.\n    \n* 12,554 cases from HUDOC, the repository of the European Court of Human Rights (ECHR) (http://hudoc.echr.coe.int/eng).\n    \n* 164,141 cases from various courts across the USA, hosted in the Case Law Access Project portal (https://case.law).\n    \n* 76,366 US contracts from EDGAR, the database of US Securities and Exchange Commission (SECOM) (https://www.sec.gov/edgar.shtml).\n\n## Pre-training details\n\n* We trained BERT using the official code provided in Google BERT's GitHub repository (https://github.com/google-research/bert).\n* We released a model similar to the English BERT-BASE model (12-layer, 768-hidden, 12-heads, 110M parameters).\n* We chose to follow the same training set-up: 1 million training steps with batches of 256 sequences of length 512 with an initial learning rate 1e-4.\n* We were able to use a single Google Cloud TPU v3-8 provided for free from [TensorFlow Research Cloud (TFRC)](https://www.tensorflow.org/tfrc), while also utilizing [GCP research credits](https://edu.google.com/programs/credits/research). Huge thanks to both Google programs for supporting us!\n* Part of LEGAL-BERT is a light-weight model pre-trained from scratch on legal data, which achieves comparable performance to larger models, while being much more efficient (approximately 4 times faster) with a smaller environmental footprint.\n\n## Models list\n\n| Model name          | Model Path                            | Training corpora    |\n| ------------------- | ------------------------------------  | ------------------- |\n| CONTRACTS-BERT-BASE | `nlpaueb/bert-base-uncased-contracts` | US contracts        |\n| EURLEX-BERT-BASE    | `nlpaueb/bert-base-uncased-eurlex`    | EU legislation      |\n| ECHR-BERT-BASE      | `nlpaueb/bert-base-uncased-echr`      | ECHR cases          |\n| LEGAL-BERT-BASE *     | `nlpaueb/legal-bert-base-uncased`     | All                 |\n| LEGAL-BERT-SMALL    | `nlpaueb/legal-bert-small-uncased`    | All                 |\n\n\\* LEGAL-BERT-BASE is the model referred to as LEGAL-BERT-SC in Chalkidis et al. (2020); a model trained from scratch in the legal corpora mentioned below using a newly created vocabulary by a sentence-piece tokenizer trained on the very same corpora.\n\n\\*\\* As many of you expressed interest in the LEGAL-BERT-FP models (those relying on the original BERT-BASE checkpoint), they have been released in Archive.org (https://archive.org/details/legal_bert_fp), as these models are secondary and possibly only interesting for those who aim to dig deeper in the open questions of Chalkidis et al. (2020).\n\n## Load Pretrained Model\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n```\n\n## Use LEGAL-BERT variants as Language Models\n\n| Corpus                             | Model                               | Masked token | Predictions  |\n| --------------------------------- | ---------------------------------- | ------------ | ------------ |\n|  | **BERT-BASE-UNCASED**                 |\n| (Contracts) | This [MASK] Agreement is between General Motors and John Murray . | employment | ('new', '0.09'), ('current', '0.04'), ('proposed', '0.03'), ('marketing', '0.03'), ('joint', '0.02')\n| (ECHR) | The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of Adana Security Directorate | torture | ('torture', '0.32'), ('rape', '0.22'), ('abuse', '0.14'), ('death', '0.04'), ('violence', '0.03')\n| (EURLEX) | Establishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products . | bovine | ('farm', '0.25'), ('livestock', '0.08'), ('draft', '0.06'), ('domestic', '0.05'), ('wild', '0.05')\n|  | **CONTRACTS-BERT-BASE**                 |\n| (Contracts) | This [MASK] Agreement is between General Motors and John Murray . | employment | ('letter', '0.38'), ('dealer', '0.04'), ('employment', '0.03'), ('award', '0.03'), ('contribution', '0.02')\n| (ECHR) | The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of Adana Security Directorate | torture | ('death', '0.39'), ('imprisonment', '0.07'), ('contempt', '0.05'), ('being', '0.03'), ('crime', '0.02')\n| (EURLEX) | Establishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products . | bovine | (('domestic', '0.18'), ('laboratory', '0.07'), ('household', '0.06'), ('personal', '0.06'), ('the', '0.04')\n|  | **EURLEX-BERT-BASE**                 |\n| (Contracts) | This [MASK] Agreement is between General Motors and John Murray . | employment | ('supply', '0.11'), ('cooperation', '0.08'), ('service', '0.07'), ('licence', '0.07'), ('distribution', '0.05')\n| (ECHR) | The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of Adana Security Directorate | torture | ('torture', '0.66'), ('death', '0.07'), ('imprisonment', '0.07'), ('murder', '0.04'), ('rape', '0.02')\n| (EURLEX) | Establishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products . | bovine | ('live', '0.43'), ('pet', '0.28'), ('certain', '0.05'), ('fur', '0.03'), ('the', '0.02')\n|  | **ECHR-BERT-BASE**                 |\n| (Contracts) | This [MASK] Agreement is between General Motors and John Murray . | employment | ('second', '0.24'), ('latter', '0.10'), ('draft', '0.05'), ('bilateral', '0.05'), ('arbitration', '0.04')\n| (ECHR) | The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of Adana Security Directorate | torture | ('torture', '0.99'), ('death', '0.01'), ('inhuman', '0.00'), ('beating', '0.00'), ('rape', '0.00')\n| (EURLEX) | Establishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products . | bovine | ('pet', '0.17'), ('all', '0.12'), ('slaughtered', '0.10'), ('domestic', '0.07'), ('individual', '0.05')\n|  | **LEGAL-BERT-BASE**                |\n| (Contracts) | This [MASK] Agreement is between General Motors and John Murray . | employment | ('settlement', '0.26'), ('letter', '0.23'), ('dealer', '0.04'), ('master', '0.02'), ('supplemental', '0.02')\n| (ECHR) | The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of Adana Security Directorate | torture | ('torture', '1.00'), ('detention', '0.00'), ('arrest', '0.00'), ('rape', '0.00'), ('death', '0.00')\n| (EURLEX) | Establishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products . | bovine | ('live', '0.67'), ('beef', '0.17'), ('farm', '0.03'), ('pet', '0.02'), ('dairy', '0.01')\n|  | **LEGAL-BERT-SMALL**                |\n| (Contracts) | This [MASK] Agreement is between General Motors and John Murray . | employment | ('license', '0.09'), ('transition', '0.08'), ('settlement', '0.04'), ('consent', '0.03'), ('letter', '0.03')\n| (ECHR) | The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of Adana Security Directorate | torture | ('torture', '0.59'), ('pain', '0.05'), ('ptsd', '0.05'), ('death', '0.02'), ('tuberculosis', '0.02')\n| (EURLEX) | Establishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products . | bovine | ('all', '0.08'), ('live', '0.07'), ('certain', '0.07'), ('the', '0.07'), ('farm', '0.05')\n\n\n## Evaluation on downstream tasks\n\nConsider the experiments in the article \"LEGAL-BERT: The Muppets straight out of Law School\". Chalkidis et al., 2020, (https://aclanthology.org/2020.findings-emnlp.261)\n\n## Author - Publication\n\n```\n@inproceedings{chalkidis-etal-2020-legal,\n    title = \"{LEGAL}-{BERT}: The Muppets straight out of Law School\",\n    author = \"Chalkidis, Ilias  and\n      Fergadiotis, Manos  and\n      Malakasiotis, Prodromos  and\n      Aletras, Nikolaos  and\n      Androutsopoulos, Ion\",\n    booktitle = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\n    month = nov,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    doi = \"10.18653/v1/2020.findings-emnlp.261\",\n    pages = \"2898--2904\"\n}\n```\n\n## About Us\n\n[AUEB's Natural Language Processing Group](http://nlp.cs.aueb.gr) develops algorithms, models, and systems that allow computers to process and generate natural language texts.\n\nThe group's current research interests include:\n* question answering systems for databases, ontologies, document collections, and the Web, especially biomedical question answering,\n* natural language generation from databases and ontologies, especially Semantic Web ontologies,\ntext classification, including filtering spam and abusive content,\n* information extraction and opinion mining, including legal text analytics and sentiment analysis,\n* natural language processing tools for Greek, for example parsers and named-entity recognizers,\nmachine learning in natural language processing, especially deep learning.\n\nThe group is part of the Information Processing Laboratory of the Department of Informatics of the Athens University of Economics and Business.\n\n[Ilias Chalkidis](https://iliaschalkidis.github.io) on behalf of [AUEB's Natural Language Processing Group](http://nlp.cs.aueb.gr)\n\n| Github: [@ilias.chalkidis](https://github.com/iliaschalkidis) | Twitter: [@KiddoThe2B](https://twitter.com/KiddoThe2B) |\n"
    },
    {
      "@id": "ark:59852/model-timm-resnet50-a1in1k-n07phoxq1k",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "timm/resnet50.a1_in1k",
      "description": "A ResNet-B image classification model.",
      "author": "timm",
      "keywords": [
        "timm",
        "pytorch",
        "safetensors",
        "image-classification",
        "transformers",
        "arxiv:2110.00476",
        "arxiv:1512.03385",
        "license:apache-2.0",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "image-classification",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "usageInformation": "### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimport torch\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet50.a1_in1k', pretrained=True)\nmodel = model.eval()",
      "contentUrl": "https://huggingface.co/timm/resnet50.a1_in1k/resolve/main/model.safetensors",
      "url": "https://huggingface.co/timm/resnet50.a1_in1k",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "# Model card for resnet50.a1_in1k\n\nA ResNet-B image classification model.\n\nThis model features:\n * ReLU activations\n * single layer 7x7 convolution with pooling\n * 1x1 convolution shortcut downsample\n\nTrained on ImageNet-1k in `timm` using recipe template described below.\n\nRecipe details:\n * ResNet Strikes Back `A1` recipe\n * LAMB optimizer with BCE loss\n * Cosine LR schedule with warmup\n\n\n## Model Details\n- **Model Type:** Image classification / feature backbone\n- **Model Stats:**\n  - Params (M): 25.6\n  - GMACs: 4.1\n  - Activations (M): 11.1\n  - Image size: train = 224 x 224, test = 288 x 288\n- **Papers:**\n  - ResNet strikes back: An improved training procedure in timm: https://arxiv.org/abs/2110.00476\n  - Deep Residual Learning for Image Recognition: https://arxiv.org/abs/1512.03385\n- **Original:** https://github.com/huggingface/pytorch-image-models\n\n## Model Usage\n### Image Classification\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\nimport torch\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model('resnet50.a1_in1k', pretrained=True)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\ntop5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)\n```\n\n### Feature Map Extraction\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.a1_in1k',\n    pretrained=True,\n    features_only=True,\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n\nfor o in output:\n    # print shape of each feature map in output\n    # e.g.:\n    #  torch.Size([1, 64, 112, 112])\n    #  torch.Size([1, 256, 56, 56])\n    #  torch.Size([1, 512, 28, 28])\n    #  torch.Size([1, 1024, 14, 14])\n    #  torch.Size([1, 2048, 7, 7])\n\n    print(o.shape)\n```\n\n### Image Embeddings\n```python\nfrom urllib.request import urlopen\nfrom PIL import Image\nimport timm\n\nimg = Image.open(urlopen(\n    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n))\n\nmodel = timm.create_model(\n    'resnet50.a1_in1k',\n    pretrained=True,\n    num_classes=0,  # remove classifier nn.Linear\n)\nmodel = model.eval()\n\n# get model specific transforms (normalization, resize)\ndata_config = timm.data.resolve_model_data_config(model)\ntransforms = timm.data.create_transform(**data_config, is_training=False)\n\noutput = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor\n\n# or equivalently (without needing to set num_classes=0)\n\noutput = model.forward_features(transforms(img).unsqueeze(0))\n# output is unpooled, a (1, 2048, 7, 7) shaped tensor\n\noutput = model.forward_head(output, pre_logits=True)\n# output is a (1, num_features) shaped tensor\n```\n\n## Model Comparison\nExplore the dataset and runtime metrics of this model in timm [model results](https://github.com/huggingface/pytorch-image-models/tree/main/results).\n\n|model                                     |img_size|top1 |top5 |param_count|gmacs|macts|img/sec|\n|------------------------------------------|--------|-----|-----|-----------|-----|-----|-------|\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|320     |86.72|98.17|93.6       |35.2 |69.7 |451    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k_288](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k_288)|288     |86.51|98.08|93.6       |28.5 |56.4 |560    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|288     |86.49|98.03|93.6       |28.5 |56.4 |557    |\n|[seresnextaa101d_32x8d.sw_in12k_ft_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.sw_in12k_ft_in1k)|224     |85.96|97.82|93.6       |17.2 |34.2 |923    |\n|[resnext101_32x32d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x32d.fb_wsl_ig1b_ft_in1k)|224     |85.11|97.44|468.5      |87.3 |91.1 |254    |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|416     |85.0 |97.12|191.9      |108.4|213.8|134    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|352     |84.96|97.22|102.1      |50.2 |101.2|291    |\n|[ecaresnet269d.ra2_in1k](https://huggingface.co/timm/ecaresnet269d.ra2_in1k)|320     |84.73|97.18|102.1      |41.5 |83.7 |353    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|384     |84.71|96.99|164.0      |77.6 |154.7|183    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|288     |84.57|97.08|93.6       |28.5 |56.4 |557    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|320     |84.45|97.08|93.2       |31.5 |67.8 |446    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|352     |84.43|96.97|129.9      |51.1 |105.5|280    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|288     |84.36|96.92|93.6       |27.6 |53.0 |595    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|320     |84.35|97.04|66.8       |24.1 |47.7 |610    |\n|[resnetrs350.tf_in1k](https://huggingface.co/timm/resnetrs350.tf_in1k)|288     |84.3 |96.94|164.0      |43.7 |87.1 |333    |\n|[resnext101_32x8d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_swsl_ig1b_ft_in1k)|224     |84.28|97.17|88.8       |16.5 |31.2 |1100   |\n|[resnetrs420.tf_in1k](https://huggingface.co/timm/resnetrs420.tf_in1k)|320     |84.24|96.86|191.9      |64.2 |126.6|228    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|288     |84.19|96.87|93.6       |27.2 |51.6 |613    |\n|[resnext101_32x16d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_wsl_ig1b_ft_in1k)|224     |84.18|97.19|194.0      |36.3 |51.2 |581    |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|288     |84.11|97.11|44.6       |15.1 |29.0 |1144   |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|320     |83.97|96.82|64.7       |31.2 |67.3 |518    |\n|[resnetrs200.tf_in1k](https://huggingface.co/timm/resnetrs200.tf_in1k)|256     |83.87|96.75|93.2       |20.2 |43.4 |692    |\n|[seresnextaa101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnextaa101d_32x8d.ah_in1k)|224     |83.86|96.65|93.6       |17.2 |34.2 |923    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|320     |83.72|96.61|86.6       |24.3 |48.1 |617    |\n|[seresnet152d.ra2_in1k](https://huggingface.co/timm/seresnet152d.ra2_in1k)|256     |83.69|96.78|66.8       |15.4 |30.6 |943    |\n|[seresnext101d_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101d_32x8d.ah_in1k)|224     |83.68|96.61|93.6       |16.7 |32.0 |986    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|320     |83.67|96.74|60.2       |24.1 |47.7 |706    |\n|[resnetrs270.tf_in1k](https://huggingface.co/timm/resnetrs270.tf_in1k)|256     |83.59|96.61|129.9      |27.1 |55.8 |526    |\n|[seresnext101_32x8d.ah_in1k](https://huggingface.co/timm/seresnext101_32x8d.ah_in1k)|224     |83.58|96.4 |93.6       |16.5 |31.2 |1013   |\n|[resnetaa101d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa101d.sw_in12k_ft_in1k)|224     |83.54|96.83|44.6       |9.1  |17.6 |1864   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|288     |83.46|96.54|60.2       |19.1 |37.3 |904    |\n|[resnext101_32x16d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_swsl_ig1b_ft_in1k)|224     |83.35|96.85|194.0      |36.3 |51.2 |582    |\n|[resnet200d.ra2_in1k](https://huggingface.co/timm/resnet200d.ra2_in1k)|256     |83.23|96.53|64.7       |20.0 |43.1 |809    |\n|[resnext101_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_swsl_ig1b_ft_in1k)|224     |83.22|96.75|44.2       |8.0  |21.2 |1814   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|288     |83.16|96.38|83.5       |25.7 |51.6 |590    |\n|[resnet152d.ra2_in1k](https://huggingface.co/timm/resnet152d.ra2_in1k)|256     |83.14|96.38|60.2       |15.4 |30.5 |1096   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|320     |83.02|96.45|44.6       |16.5 |34.8 |992    |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|288     |82.98|96.54|44.6       |13.4 |28.2 |1077   |\n|[resnext101_64x4d.tv_in1k](https://huggingface.co/timm/resnext101_64x4d.tv_in1k)|224     |82.98|96.25|83.5       |15.5 |31.2 |989    |\n|[resnetrs152.tf_in1k](https://huggingface.co/timm/resnetrs152.tf_in1k)|256     |82.86|96.28|86.6       |15.6 |30.8 |951    |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|224     |82.83|96.22|88.8       |16.5 |31.2 |1099   |\n|[resnet152.a1h_in1k](https://huggingface.co/timm/resnet152.a1h_in1k)|224     |82.8 |96.13|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|288     |82.8 |96.32|44.6       |13.0 |26.8 |1291   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|288     |82.74|95.71|60.2       |19.1 |37.3 |905    |\n|[resnext101_32x8d.fb_wsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_wsl_ig1b_ft_in1k)|224     |82.69|96.63|88.8       |16.5 |31.2 |1100   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|288     |82.62|95.75|60.2       |19.1 |37.3 |904    |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|288     |82.61|96.49|25.6       |8.9  |20.6 |1729   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|288     |82.53|96.13|36.8       |9.9  |21.5 |1773   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|224     |82.5 |96.02|126.9      |22.8 |21.2 |1078   |\n|[resnext101_64x4d.c1_in1k](https://huggingface.co/timm/resnext101_64x4d.c1_in1k)|224     |82.46|95.92|83.5       |15.5 |31.2 |987    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|288     |82.36|96.18|35.7       |8.1  |20.9 |1964   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|320     |82.35|96.14|25.6       |8.8  |24.1 |1386   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|288     |82.31|95.63|44.6       |13.0 |26.8 |1291   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|288     |82.29|96.01|63.6       |13.6 |28.5 |1078   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|224     |82.29|96.0 |60.2       |11.6 |22.6 |1484   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|288     |82.27|96.06|68.9       |18.9 |23.8 |1176   |\n|[resnet101d.ra2_in1k](https://huggingface.co/timm/resnet101d.ra2_in1k)|256     |82.26|96.07|44.6       |10.6 |22.2 |1542   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|288     |82.24|95.73|44.6       |13.0 |26.8 |1290   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|288     |82.2 |96.14|27.6       |7.0  |23.8 |1547   |\n|[ecaresnet101d.miil_in1k](https://huggingface.co/timm/ecaresnet101d.miil_in1k)|224     |82.18|96.05|44.6       |8.1  |17.1 |1771   |\n|[resnext50_32x4d.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_swsl_ig1b_ft_in1k)|224     |82.17|96.22|25.0       |4.3  |14.4 |2943   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|288     |82.12|95.65|25.6       |7.1  |19.6 |1704   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|288     |82.03|95.94|25.0       |7.0  |23.8 |1745   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|288     |82.0 |96.15|24.9       |5.8  |12.7 |1787   |\n|[resnet61q.ra2_in1k](https://huggingface.co/timm/resnet61q.ra2_in1k)|256     |81.99|95.85|36.8       |7.8  |17.0 |2230   |\n|[resnext101_32x8d.tv2_in1k](https://huggingface.co/timm/resnext101_32x8d.tv2_in1k)|176     |81.98|95.72|88.8       |10.3 |19.4 |1768   |\n|[resnet152.a1_in1k](https://huggingface.co/timm/resnet152.a1_in1k)|224     |81.97|95.24|60.2       |11.6 |22.6 |1486   |\n|[resnet101.a1h_in1k](https://huggingface.co/timm/resnet101.a1h_in1k)|224     |81.93|95.75|44.6       |7.8  |16.2 |2122   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|224     |81.9 |95.77|44.6       |7.8  |16.2 |2118   |\n|[resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k)|224     |81.84|96.1 |194.0      |36.3 |51.2 |583    |\n|[resnet51q.ra2_in1k](https://huggingface.co/timm/resnet51q.ra2_in1k)|256     |81.78|95.94|35.7       |6.4  |16.6 |2471   |\n|[resnet152.a2_in1k](https://huggingface.co/timm/resnet152.a2_in1k)|224     |81.77|95.22|60.2       |11.6 |22.6 |1485   |\n|[resnetaa50d.sw_in12k_ft_in1k](https://huggingface.co/timm/resnetaa50d.sw_in12k_ft_in1k)|224     |81.74|96.06|25.6       |5.4  |12.4 |2813   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|288     |81.65|95.54|25.6       |7.1  |19.6 |1703   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|288     |81.64|95.88|25.6       |7.2  |19.7 |1694   |\n|[resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k)|224     |81.62|96.04|88.8       |16.5 |31.2 |1101   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|224     |81.61|95.76|68.9       |11.4 |14.4 |1930   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|288     |81.61|95.83|25.6       |8.5  |19.2 |1868   |\n|[resnet101.a1_in1k](https://huggingface.co/timm/resnet101.a1_in1k)|224     |81.5 |95.16|44.6       |7.8  |16.2 |2125   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|288     |81.48|95.16|25.0       |7.0  |23.8 |1745   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|288     |81.47|95.71|25.9       |6.9  |18.6 |2071   |\n|[wide_resnet50_2.racm_in1k](https://huggingface.co/timm/wide_resnet50_2.racm_in1k)|224     |81.45|95.53|68.9       |11.4 |14.4 |1929   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|288     |81.44|95.22|25.6       |7.2  |19.7 |1908   |\n|[ecaresnet50t.ra2_in1k](https://huggingface.co/timm/ecaresnet50t.ra2_in1k)|256     |81.44|95.67|25.6       |5.6  |15.4 |2168   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|288     |81.4 |95.82|30.2       |6.8  |13.9 |2132   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|288     |81.37|95.74|25.6       |7.2  |19.7 |1910   |\n|[resnet101.a2_in1k](https://huggingface.co/timm/resnet101.a2_in1k)|224     |81.32|95.19|44.6       |7.8  |16.2 |2125   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|288     |81.3 |95.65|28.1       |6.8  |18.4 |1803   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|288     |81.3 |95.11|25.0       |7.0  |23.8 |1746   |\n|[seresnext50_32x4d.racm_in1k](https://huggingface.co/timm/seresnext50_32x4d.racm_in1k)|224     |81.27|95.62|27.6       |4.3  |14.4 |2591   |\n|[ecaresnet50t.a1_in1k](https://huggingface.co/timm/ecaresnet50t.a1_in1k)|224     |81.26|95.16|25.6       |4.3  |11.8 |2823   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|288     |81.23|95.54|15.7       |4.8  |19.6 |2117   |\n|[senet154.gluon_in1k](https://huggingface.co/timm/senet154.gluon_in1k)|224     |81.23|95.35|115.1      |20.8 |38.7 |545    |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|288     |81.22|95.11|25.6       |6.8  |18.4 |2089   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|288     |81.22|95.63|25.6       |6.8  |18.4 |676    |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|288     |81.18|95.09|25.6       |7.2  |19.7 |1908   |\n|[resnet50.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet50.fb_swsl_ig1b_ft_in1k)|224     |81.18|95.98|25.6       |4.1  |11.1 |3455   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|224     |81.17|95.34|25.0       |4.3  |14.4 |2933   |\n|[resnext50_32x4d.a1h_in1k](https://huggingface.co/timm/resnext50_32x4d.a1h_in1k)|224     |81.1 |95.33|25.0       |4.3  |14.4 |2934   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|288     |81.1 |95.23|28.1       |6.8  |18.4 |1801   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|288     |81.1 |95.12|28.1       |6.8  |18.4 |1799   |\n|[resnet152s.gluon_in1k](https://huggingface.co/timm/resnet152s.gluon_in1k)|224     |81.02|95.41|60.3       |12.9 |25.0 |1347   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|288     |80.97|95.44|25.6       |6.8  |18.4 |2085   |\n|[gcresnet50t.ra2_in1k](https://huggingface.co/timm/gcresnet50t.ra2_in1k)|256     |80.94|95.45|25.9       |5.4  |14.7 |2571   |\n|[resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.93|95.73|44.2       |8.0  |21.2 |1814   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|288     |80.91|95.55|25.6       |6.8  |18.4 |2084   |\n|[seresnext101_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_32x4d.gluon_in1k)|224     |80.9 |95.31|49.0       |8.0  |21.3 |1585   |\n|[seresnext101_64x4d.gluon_in1k](https://huggingface.co/timm/seresnext101_64x4d.gluon_in1k)|224     |80.9 |95.3 |88.2       |15.5 |31.2 |918    |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|288     |80.86|95.52|25.6       |6.8  |18.4 |2085   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|224     |80.85|95.43|25.6       |4.1  |11.1 |3450   |\n|[ecaresnet50t.a2_in1k](https://huggingface.co/timm/ecaresnet50t.a2_in1k)|224     |80.84|95.02|25.6       |4.3  |11.8 |2821   |\n|[ecaresnet101d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet101d_pruned.miil_in1k)|224     |80.79|95.62|24.9       |3.5  |7.7  |2961   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|288     |80.79|95.36|19.8       |6.0  |14.8 |2506   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|288     |80.79|95.58|19.9       |4.2  |10.6 |2349   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|288     |80.78|94.99|25.6       |6.8  |18.4 |2088   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|288     |80.71|95.43|25.6       |6.8  |18.4 |2087   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|288     |80.7 |95.39|25.0       |7.0  |23.8 |1749   |\n|[resnetrs101.tf_in1k](https://huggingface.co/timm/resnetrs101.tf_in1k)|192     |80.69|95.24|63.6       |6.0  |12.7 |2270   |\n|[resnet50d.a1_in1k](https://huggingface.co/timm/resnet50d.a1_in1k)|224     |80.68|94.71|25.6       |4.4  |11.9 |3162   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|288     |80.68|95.36|19.7       |6.0  |14.8 |2637   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|224     |80.67|95.3 |25.6       |4.1  |11.1 |3452   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|288     |80.67|95.42|25.0       |7.4  |25.1 |1626   |\n|[resnetaa50.a1h_in1k](https://huggingface.co/timm/resnetaa50.a1h_in1k)|224     |80.63|95.21|25.6       |5.2  |11.6 |3034   |\n|[ecaresnet50d.miil_in1k](https://huggingface.co/timm/ecaresnet50d.miil_in1k)|224     |80.61|95.32|25.6       |4.4  |11.9 |2813   |\n|[resnext101_64x4d.gluon_in1k](https://huggingface.co/timm/resnext101_64x4d.gluon_in1k)|224     |80.61|94.99|83.5       |15.5 |31.2 |989    |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|288     |80.6 |95.31|19.9       |6.0  |14.8 |2578   |\n|[gcresnext50ts.ch_in1k](https://huggingface.co/timm/gcresnext50ts.ch_in1k)|256     |80.57|95.17|15.7       |3.8  |15.5 |2710   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|224     |80.56|95.0 |60.2       |11.6 |22.6 |1483   |\n|[resnet50d.ra2_in1k](https://huggingface.co/timm/resnet50d.ra2_in1k)|224     |80.53|95.16|25.6       |4.4  |11.9 |3164   |\n|[resnext50_32x4d.a1_in1k](https://huggingface.co/timm/resnext50_32x4d.a1_in1k)|224     |80.53|94.46|25.0       |4.3  |14.4 |2930   |\n|[wide_resnet101_2.tv2_in1k](https://huggingface.co/timm/wide_resnet101_2.tv2_in1k)|176     |80.48|94.98|126.9      |14.3 |13.2 |1719   |\n|[resnet152d.gluon_in1k](https://huggingface.co/timm/resnet152d.gluon_in1k)|224     |80.47|95.2 |60.2       |11.8 |23.4 |1428   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|288     |80.45|95.32|25.6       |6.8  |18.4 |2086   |\n|[ecaresnetlight.miil_in1k](https://huggingface.co/timm/ecaresnetlight.miil_in1k)|224     |80.45|95.24|30.2       |4.1  |8.4  |3530   |\n|[resnext50_32x4d.a2_in1k](https://huggingface.co/timm/resnext50_32x4d.a2_in1k)|224     |80.45|94.63|25.0       |4.3  |14.4 |2936   |\n|[wide_resnet50_2.tv2_in1k](https://huggingface.co/timm/wide_resnet50_2.tv2_in1k)|176     |80.43|95.09|68.9       |7.3  |9.0  |3015   |\n|[resnet101d.gluon_in1k](https://huggingface.co/timm/resnet101d.gluon_in1k)|224     |80.42|95.01|44.6       |8.1  |17.0 |2007   |\n|[resnet50.a1_in1k](https://huggingface.co/timm/resnet50.a1_in1k)|224     |80.38|94.6 |25.6       |4.1  |11.1 |3461   |\n|[seresnet33ts.ra2_in1k](https://huggingface.co/timm/seresnet33ts.ra2_in1k)|256     |80.36|95.1 |19.8       |4.8  |11.7 |3267   |\n|[resnext101_32x4d.gluon_in1k](https://huggingface.co/timm/resnext101_32x4d.gluon_in1k)|224     |80.34|94.93|44.2       |8.0  |21.2 |1814   |\n|[resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k)|224     |80.32|95.4 |25.0       |4.3  |14.4 |2941   |\n|[resnet101s.gluon_in1k](https://huggingface.co/timm/resnet101s.gluon_in1k)|224     |80.28|95.16|44.7       |9.2  |18.6 |1851   |\n|[seresnet50.ra2_in1k](https://huggingface.co/timm/seresnet50.ra2_in1k)|224     |80.26|95.08|28.1       |4.1  |11.1 |2972   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|288     |80.24|95.24|25.6       |8.5  |19.9 |1523   |\n|[resnet50d.a2_in1k](https://huggingface.co/timm/resnet50d.a2_in1k)|224     |80.22|94.63|25.6       |4.4  |11.9 |3162   |\n|[resnet152.tv2_in1k](https://huggingface.co/timm/resnet152.tv2_in1k)|176     |80.2 |94.64|60.2       |7.2  |14.0 |2346   |\n|[seresnet50.a2_in1k](https://huggingface.co/timm/seresnet50.a2_in1k)|224     |80.08|94.74|28.1       |4.1  |11.1 |2969   |\n|[eca_resnet33ts.ra2_in1k](https://huggingface.co/timm/eca_resnet33ts.ra2_in1k)|256     |80.08|94.97|19.7       |4.8  |11.7 |3284   |\n|[gcresnet33ts.ra2_in1k](https://huggingface.co/timm/gcresnet33ts.ra2_in1k)|256     |80.06|94.99|19.9       |4.8  |11.7 |3216   |\n|[resnet50_gn.a1h_in1k](https://huggingface.co/timm/resnet50_gn.a1h_in1k)|224     |80.06|94.95|25.6       |4.1  |11.1 |1109   |\n|[seresnet50.a1_in1k](https://huggingface.co/timm/seresnet50.a1_in1k)|224     |80.02|94.71|28.1       |4.1  |11.1 |2962   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|288     |79.97|95.05|25.6       |6.8  |18.4 |2086   |\n|[resnet152c.gluon_in1k](https://huggingface.co/timm/resnet152c.gluon_in1k)|224     |79.92|94.84|60.2       |11.8 |23.4 |1455   |\n|[seresnext50_32x4d.gluon_in1k](https://huggingface.co/timm/seresnext50_32x4d.gluon_in1k)|224     |79.91|94.82|27.6       |4.3  |14.4 |2591   |\n|[resnet50.d_in1k](https://huggingface.co/timm/resnet50.d_in1k)|224     |79.91|94.67|25.6       |4.1  |11.1 |3456   |\n|[resnet101.tv2_in1k](https://huggingface.co/timm/resnet101.tv2_in1k)|176     |79.9 |94.6 |44.6       |4.9  |10.1 |3341   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|224     |79.89|94.97|35.7       |4.5  |12.1 |2774   |\n|[resnet50.c2_in1k](https://huggingface.co/timm/resnet50.c2_in1k)|224     |79.88|94.87|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|320     |79.86|95.07|16.0       |5.2  |16.4 |2168   |\n|[resnet50.a2_in1k](https://huggingface.co/timm/resnet50.a2_in1k)|224     |79.85|94.56|25.6       |4.1  |11.1 |3460   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|288     |79.83|94.97|25.6       |6.8  |18.4 |2087   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|224     |79.82|94.62|44.6       |7.8  |16.2 |2114   |\n|[resnext50_32x4d.ra_in1k](https://huggingface.co/timm/resnext50_32x4d.ra_in1k)|224     |79.76|94.6 |25.0       |4.3  |14.4 |2943   |\n|[resnet50.c1_in1k](https://huggingface.co/timm/resnet50.c1_in1k)|224     |79.74|94.95|25.6       |4.1  |11.1 |3455   |\n|[ecaresnet50d_pruned.miil_in1k](https://huggingface.co/timm/ecaresnet50d_pruned.miil_in1k)|224     |79.74|94.87|19.9       |2.5  |6.4  |3929   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|288     |79.71|94.83|19.7       |6.0  |14.8 |2710   |\n|[resnet152.gluon_in1k](https://huggingface.co/timm/resnet152.gluon_in1k)|224     |79.68|94.74|60.2       |11.6 |22.6 |1486   |\n|[resnext50d_32x4d.bt_in1k](https://huggingface.co/timm/resnext50d_32x4d.bt_in1k)|224     |79.67|94.87|25.0       |4.5  |15.2 |2729   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|288     |79.63|94.91|25.6       |6.8  |18.4 |2086   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|224     |79.56|94.72|25.6       |4.3  |11.8 |2805   |\n|[resnet101c.gluon_in1k](https://huggingface.co/timm/resnet101c.gluon_in1k)|224     |79.53|94.58|44.6       |8.1  |17.0 |2062   |\n|[resnet50.b1k_in1k](https://huggingface.co/timm/resnet50.b1k_in1k)|224     |79.52|94.61|25.6       |4.1  |11.1 |3459   |\n|[resnet50.tv2_in1k](https://huggingface.co/timm/resnet50.tv2_in1k)|176     |79.42|94.64|25.6       |2.6  |6.9  |5397   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|288     |79.4 |94.66|18.0       |5.9  |14.6 |2752   |\n|[resnet50.b2k_in1k](https://huggingface.co/timm/resnet50.b2k_in1k)|224     |79.38|94.57|25.6       |4.1  |11.1 |3459   |\n|[resnext50_32x4d.tv2_in1k](https://huggingface.co/timm/resnext50_32x4d.tv2_in1k)|176     |79.37|94.3 |25.0       |2.7  |9.0  |4577   |\n|[resnext50_32x4d.gluon_in1k](https://huggingface.co/timm/resnext50_32x4d.gluon_in1k)|224     |79.36|94.43|25.0       |4.3  |14.4 |2942   |\n|[resnext101_32x8d.tv_in1k](https://huggingface.co/timm/resnext101_32x8d.tv_in1k)|224     |79.31|94.52|88.8       |16.5 |31.2 |1100   |\n|[resnet101.gluon_in1k](https://huggingface.co/timm/resnet101.gluon_in1k)|224     |79.31|94.53|44.6       |7.8  |16.2 |2125   |\n|[resnetblur50.bt_in1k](https://huggingface.co/timm/resnetblur50.bt_in1k)|224     |79.31|94.63|25.6       |5.2  |12.0 |2524   |\n|[resnet50.a1h_in1k](https://huggingface.co/timm/resnet50.a1h_in1k)|176     |79.27|94.49|25.6       |2.6  |6.9  |5404   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|224     |79.25|94.31|25.0       |4.3  |14.4 |2931   |\n|[resnet50.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet50.fb_ssl_yfcc100m_ft_in1k)|224     |79.22|94.84|25.6       |4.1  |11.1 |3451   |\n|[resnet33ts.ra2_in1k](https://huggingface.co/timm/resnet33ts.ra2_in1k)|256     |79.21|94.56|19.7       |4.8  |11.7 |3392   |\n|[resnet50d.gluon_in1k](https://huggingface.co/timm/resnet50d.gluon_in1k)|224     |79.07|94.48|25.6       |4.4  |11.9 |3162   |\n|[resnet50.ram_in1k](https://huggingface.co/timm/resnet50.ram_in1k)|224     |79.03|94.38|25.6       |4.1  |11.1 |3453   |\n|[resnet50.am_in1k](https://huggingface.co/timm/resnet50.am_in1k)|224     |79.01|94.39|25.6       |4.1  |11.1 |3461   |\n|[resnet32ts.ra2_in1k](https://huggingface.co/timm/resnet32ts.ra2_in1k)|256     |79.01|94.37|18.0       |4.6  |11.6 |3440   |\n|[ecaresnet26t.ra2_in1k](https://huggingface.co/timm/ecaresnet26t.ra2_in1k)|256     |78.9 |94.54|16.0       |3.4  |10.5 |3421   |\n|[resnet152.a3_in1k](https://huggingface.co/timm/resnet152.a3_in1k)|160     |78.89|94.11|60.2       |5.9  |11.5 |2745   |\n|[wide_resnet101_2.tv_in1k](https://huggingface.co/timm/wide_resnet101_2.tv_in1k)|224     |78.84|94.28|126.9      |22.8 |21.2 |1079   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|288     |78.83|94.24|16.8       |4.5  |16.8 |2251   |\n|[resnet50.ra_in1k](https://huggingface.co/timm/resnet50.ra_in1k)|224     |78.81|94.32|25.6       |4.1  |11.1 |3454   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|288     |78.74|94.33|16.8       |4.5  |16.7 |2264   |\n|[resnet50s.gluon_in1k](https://huggingface.co/timm/resnet50s.gluon_in1k)|224     |78.72|94.23|25.7       |5.5  |13.5 |2796   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|224     |78.71|94.24|25.6       |4.4  |11.9 |3154   |\n|[wide_resnet50_2.tv_in1k](https://huggingface.co/timm/wide_resnet50_2.tv_in1k)|224     |78.47|94.09|68.9       |11.4 |14.4 |1934   |\n|[resnet50.bt_in1k](https://huggingface.co/timm/resnet50.bt_in1k)|224     |78.46|94.27|25.6       |4.1  |11.1 |3454   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|288     |78.43|94.35|21.8       |6.5  |7.5  |3291   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|288     |78.42|94.04|10.5       |3.1  |13.3 |3226   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|320     |78.33|94.13|16.0       |5.2  |16.4 |2391   |\n|[resnet152.tv_in1k](https://huggingface.co/timm/resnet152.tv_in1k)|224     |78.32|94.04|60.2       |11.6 |22.6 |1487   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|288     |78.28|94.1 |10.4       |3.1  |13.3 |3062   |\n|[bat_resnext26ts.ch_in1k](https://huggingface.co/timm/bat_resnext26ts.ch_in1k)|256     |78.25|94.1 |10.7       |2.5  |12.5 |3393   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|224     |78.06|93.78|25.6       |4.1  |11.1 |3450   |\n|[resnet50c.gluon_in1k](https://huggingface.co/timm/resnet50c.gluon_in1k)|224     |78.0 |93.99|25.6       |4.4  |11.9 |3286   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|288     |78.0 |93.91|10.3       |3.1  |13.3 |3297   |\n|[seresnext26t_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26t_32x4d.bt_in1k)|224     |77.98|93.75|16.8       |2.7  |10.1 |3841   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|288     |77.92|93.77|21.8       |6.1  |6.2  |3609   |\n|[resnet101.a3_in1k](https://huggingface.co/timm/resnet101.a3_in1k)|160     |77.88|93.71|44.6       |4.0  |8.3  |3926   |\n|[resnet26t.ra2_in1k](https://huggingface.co/timm/resnet26t.ra2_in1k)|256     |77.87|93.84|16.0       |3.4  |10.5 |3772   |\n|[seresnext26ts.ch_in1k](https://huggingface.co/timm/seresnext26ts.ch_in1k)|256     |77.86|93.79|10.4       |2.4  |10.5 |4263   |\n|[resnetrs50.tf_in1k](https://huggingface.co/timm/resnetrs50.tf_in1k)|160     |77.82|93.81|35.7       |2.3  |6.2  |5238   |\n|[gcresnext26ts.ch_in1k](https://huggingface.co/timm/gcresnext26ts.ch_in1k)|256     |77.81|93.82|10.5       |2.4  |10.5 |4183   |\n|[ecaresnet50t.a3_in1k](https://huggingface.co/timm/ecaresnet50t.a3_in1k)|160     |77.79|93.6 |25.6       |2.2  |6.0  |5329   |\n|[resnext50_32x4d.a3_in1k](https://huggingface.co/timm/resnext50_32x4d.a3_in1k)|160     |77.73|93.32|25.0       |2.2  |7.4  |5576   |\n|[resnext50_32x4d.tv_in1k](https://huggingface.co/timm/resnext50_32x4d.tv_in1k)|224     |77.61|93.7 |25.0       |4.3  |14.4 |2944   |\n|[seresnext26d_32x4d.bt_in1k](https://huggingface.co/timm/seresnext26d_32x4d.bt_in1k)|224     |77.59|93.61|16.8       |2.7  |10.2 |3807   |\n|[resnet50.gluon_in1k](https://huggingface.co/timm/resnet50.gluon_in1k)|224     |77.58|93.72|25.6       |4.1  |11.1 |3455   |\n|[eca_resnext26ts.ch_in1k](https://huggingface.co/timm/eca_resnext26ts.ch_in1k)|256     |77.44|93.56|10.3       |2.4  |10.5 |4284   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|288     |77.41|93.63|16.0       |4.3  |13.5 |2907   |\n|[resnet101.tv_in1k](https://huggingface.co/timm/resnet101.tv_in1k)|224     |77.38|93.54|44.6       |7.8  |16.2 |2125   |\n|[resnet50d.a3_in1k](https://huggingface.co/timm/resnet50d.a3_in1k)|160     |77.22|93.27|25.6       |2.2  |6.1  |5982   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|288     |77.17|93.47|10.3       |3.1  |13.3 |3392   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|288     |77.15|93.27|21.8       |6.1  |6.2  |3615   |\n|[resnet34d.ra2_in1k](https://huggingface.co/timm/resnet34d.ra2_in1k)|224     |77.1 |93.37|21.8       |3.9  |4.5  |5436   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|224     |77.02|93.07|28.1       |4.1  |11.1 |2952   |\n|[resnext26ts.ra2_in1k](https://huggingface.co/timm/resnext26ts.ra2_in1k)|256     |76.78|93.13|10.3       |2.4  |10.5 |4410   |\n|[resnet26d.bt_in1k](https://huggingface.co/timm/resnet26d.bt_in1k)|224     |76.7 |93.17|16.0       |2.6  |8.2  |4859   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|288     |76.5 |93.35|21.8       |6.1  |6.2  |3617   |\n|[resnet34.a1_in1k](https://huggingface.co/timm/resnet34.a1_in1k)|224     |76.42|92.87|21.8       |3.7  |3.7  |5984   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|288     |76.35|93.18|16.0       |3.9  |12.2 |3331   |\n|[resnet50.tv_in1k](https://huggingface.co/timm/resnet50.tv_in1k)|224     |76.13|92.86|25.6       |4.1  |11.1 |3457   |\n|[resnet50.a3_in1k](https://huggingface.co/timm/resnet50.a3_in1k)|160     |75.96|92.5 |25.6       |2.1  |5.7  |6490   |\n|[resnet34.a2_in1k](https://huggingface.co/timm/resnet34.a2_in1k)|224     |75.52|92.44|21.8       |3.7  |3.7  |5991   |\n|[resnet26.bt_in1k](https://huggingface.co/timm/resnet26.bt_in1k)|224     |75.3 |92.58|16.0       |2.4  |7.4  |5583   |\n|[resnet34.bt_in1k](https://huggingface.co/timm/resnet34.bt_in1k)|224     |75.16|92.18|21.8       |3.7  |3.7  |5994   |\n|[seresnet50.a3_in1k](https://huggingface.co/timm/seresnet50.a3_in1k)|160     |75.1 |92.08|28.1       |2.1  |5.7  |5513   |\n|[resnet34.gluon_in1k](https://huggingface.co/timm/resnet34.gluon_in1k)|224     |74.57|91.98|21.8       |3.7  |3.7  |5984   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|288     |73.81|91.83|11.7       |3.4  |5.4  |5196   |\n|[resnet34.tv_in1k](https://huggingface.co/timm/resnet34.tv_in1k)|224     |73.32|91.42|21.8       |3.7  |3.7  |5979   |\n|[resnet18.fb_swsl_ig1b_ft_in1k](https://huggingface.co/timm/resnet18.fb_swsl_ig1b_ft_in1k)|224     |73.28|91.73|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|288     |73.16|91.03|11.7       |3.0  |4.1  |6050   |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|224     |72.98|91.11|21.8       |3.7  |3.7  |5967   |\n|[resnet18.fb_ssl_yfcc100m_ft_in1k](https://huggingface.co/timm/resnet18.fb_ssl_yfcc100m_ft_in1k)|224     |72.6 |91.42|11.7       |1.8  |2.5  |10213  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|288     |72.37|90.59|11.7       |3.0  |4.1  |6051   |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|224     |72.26|90.31|10.1       |1.7  |5.8  |7026   |\n|[resnet18d.ra2_in1k](https://huggingface.co/timm/resnet18d.ra2_in1k)|224     |72.26|90.68|11.7       |2.1  |3.3  |8707   |\n|[resnet18.a1_in1k](https://huggingface.co/timm/resnet18.a1_in1k)|224     |71.49|90.07|11.7       |1.8  |2.5  |10187  |\n|[resnet14t.c3_in1k](https://huggingface.co/timm/resnet14t.c3_in1k)|176     |71.31|89.69|10.1       |1.1  |3.6  |10970  |\n|[resnet18.gluon_in1k](https://huggingface.co/timm/resnet18.gluon_in1k)|224     |70.84|89.76|11.7       |1.8  |2.5  |10210  |\n|[resnet18.a2_in1k](https://huggingface.co/timm/resnet18.a2_in1k)|224     |70.64|89.47|11.7       |1.8  |2.5  |10194  |\n|[resnet34.a3_in1k](https://huggingface.co/timm/resnet34.a3_in1k)|160     |70.56|89.52|21.8       |1.9  |1.9  |10737  |\n|[resnet18.tv_in1k](https://huggingface.co/timm/resnet18.tv_in1k)|224     |69.76|89.07|11.7       |1.8  |2.5  |10205  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|224     |68.34|88.03|5.4        |1.1  |2.4  |13079  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|224     |68.25|88.17|11.7       |1.8  |2.5  |10167  |\n|[resnet10t.c3_in1k](https://huggingface.co/timm/resnet10t.c3_in1k)|176     |66.71|86.96|5.4        |0.7  |1.5  |20327  |\n|[resnet18.a3_in1k](https://huggingface.co/timm/resnet18.a3_in1k)|160     |65.66|86.26|11.7       |0.9  |1.3  |18229  |\n\n## Citation\n```bibtex\n@inproceedings{wightman2021resnet,\n  title={ResNet strikes back: An improved training procedure in timm},\n  author={Wightman, Ross and Touvron, Hugo and Jegou, Herve},\n  booktitle={NeurIPS 2021 Workshop on ImageNet: Past, Present, and Future}\n}\n```\n```bibtex\n@misc{rw2019timm,\n  author = {Ross Wightman},\n  title = {PyTorch Image Models},\n  year = {2019},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  doi = {10.5281/zenodo.4414861},\n  howpublished = {\\url{https://github.com/huggingface/pytorch-image-models}}\n}\n```\n```bibtex\n@article{He2015,\n  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},\n  title = {Deep Residual Learning for Image Recognition},\n  journal = {arXiv preprint arXiv:1512.03385},\n  year = {2015}\n}\n```\n"
    },
    {
      "@id": "ark:59852/model-openai-gpt-oss-120b-xnwor1hbhdh",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "openai/gpt-oss-120b",
      "description": "<p align=\"center\">",
      "author": "openai",
      "keywords": [
        "transformers",
        "safetensors",
        "gpt_oss",
        "text-generation",
        "vllm",
        "conversational",
        "arxiv:2508.10925",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "8-bit",
        "mxfp4",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-generation",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "intendedUseCase": "Welcome to the gpt-oss series, [OpenAI\u2019s open-weight models](https://openai.com/open-models) designed for powerful reasoning, agentic tasks, and versatile developer use cases.",
      "usageInformation": "## Transformers\n\nYou can use `gpt-oss-120b` and `gpt-oss-20b` with Transformers. If you use the Transformers chat template, it will automatically apply the [harmony response format](https://github.com/openai/harmony). If you use `model.generate` directly, you need to apply the harmony format manually using the chat template or use our [openai-harmony](https://github.com/openai/harmony) package.\n\nTo get started, install the necessary dependencies to setup your environment:\n\n```\npip install -U transformers kernels torch \n```\n\nOnce, setup you can proceed to run the model by running the snippet below:\n\n```py\nfrom transformers import pipeline\nimport torch\n\nmodel_id = \"openai/gpt-oss-120b\"\n\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n]\n\noutputs = pipe(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nAlternatively, you can run the model via [`Transformers Serve`](https://huggingface.co/docs/transformers/main/serving) to spin up a OpenAI-compatible webserver:\n\n```\ntransformers serve\ntransformers chat localhost:8000 --model-name-or-path openai/gpt-oss-120b\n```\n\n[Learn more about how to use gpt-oss with Transformers.](https://cookbook.openai.com/articles/gpt-oss/run-transformers)\n\n## vLLM\n\nvLLM recommends using [uv](https://docs.astral.sh/uv/) for Python dependency management. You can use vLLM to spin up an OpenAI-compatible webserver. The following command will automatically download the model and start the server.\n\n```bash\nuv pip install --pre vllm==0.10.1+gptoss \\\n    --extra-index-url https://wheels.vllm.ai/gpt-oss/ \\\n    --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\\n    --index-strategy unsafe-best-match\n\nvllm serve openai/gpt-oss-120b\n```\n\n[Learn more about how to use gpt-oss with vLLM.](https://cookbook.openai.com/articles/gpt-oss/run-vllm)\n\n## PyTorch / Triton\n\nTo learn about how to use this model with PyTorch and Triton, check out our [reference implementations in the gpt-oss repository](https://github.com/openai/gpt-oss?tab=readme-ov-file#reference-pytorch-implementation).\n\n## Ollama\n\nIf you are trying to run gpt-oss on consumer hardware, you can use Ollama by running the following commands after [installing Ollama](https://ollama.com/download).\n\n```bash",
      "contentUrl": "https://huggingface.co/openai/gpt-oss-120b/resolve/main/model-00000-of-00014.safetensors",
      "url": "https://huggingface.co/openai/gpt-oss-120b",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n<p align=\"center\">\n  <img alt=\"gpt-oss-120b\" src=\"https://raw.githubusercontent.com/openai/gpt-oss/main/docs/gpt-oss-120b.svg\">\n</p>\n\n<p align=\"center\">\n  <a href=\"https://gpt-oss.com\"><strong>Try gpt-oss</strong></a> \u00b7\n  <a href=\"https://cookbook.openai.com/topic/gpt-oss\"><strong>Guides</strong></a> \u00b7\n  <a href=\"https://arxiv.org/abs/2508.10925\"><strong>Model card</strong></a> \u00b7\n  <a href=\"https://openai.com/index/introducing-gpt-oss/\"><strong>OpenAI blog</strong></a>\n</p>\n\n<br>\n\nWelcome to the gpt-oss series, [OpenAI\u2019s open-weight models](https://openai.com/open-models) designed for powerful reasoning, agentic tasks, and versatile developer use cases.\n\nWe\u2019re releasing two flavors of these open models:\n- `gpt-oss-120b` \u2014 for production, general purpose, high reasoning use cases that fit into a single 80GB GPU (like NVIDIA H100 or AMD MI300X) (117B parameters with 5.1B active parameters)\n- `gpt-oss-20b` \u2014 for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)\n\nBoth models were trained on our [harmony response format](https://github.com/openai/harmony) and should only be used with the harmony format as it will not work correctly otherwise.\n\n\n> [!NOTE]\n> This model card is dedicated to the larger `gpt-oss-120b` model. Check out [`gpt-oss-20b`](https://huggingface.co/openai/gpt-oss-20b) for the smaller model.\n\n# Highlights\n\n* **Permissive Apache 2.0 license:** Build freely without copyleft restrictions or patent risk\u2014ideal for experimentation, customization, and commercial deployment.  \n* **Configurable reasoning effort:** Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.  \n* **Full chain-of-thought:** Gain complete access to the model\u2019s reasoning process, facilitating easier debugging and increased trust in outputs. It\u2019s not intended to be shown to end users.  \n* **Fine-tunable:** Fully customize models to your specific use case through parameter fine-tuning.\n* **Agentic capabilities:** Use the models\u2019 native capabilities for function calling, [web browsing](https://github.com/openai/gpt-oss/tree/main?tab=readme-ov-file#browser), [Python code execution](https://github.com/openai/gpt-oss/tree/main?tab=readme-ov-file#python), and Structured Outputs.\n* **MXFP4 quantization:** The models were post-trained with MXFP4 quantization of the MoE weights, making `gpt-oss-120b` run on a single 80GB GPU (like NVIDIA H100 or AMD MI300X) and the `gpt-oss-20b` model run within 16GB of memory. All evals were performed with the same MXFP4 quantization.\n\n---\n\n# Inference examples\n\n## Transformers\n\nYou can use `gpt-oss-120b` and `gpt-oss-20b` with Transformers. If you use the Transformers chat template, it will automatically apply the [harmony response format](https://github.com/openai/harmony). If you use `model.generate` directly, you need to apply the harmony format manually using the chat template or use our [openai-harmony](https://github.com/openai/harmony) package.\n\nTo get started, install the necessary dependencies to setup your environment:\n\n```\npip install -U transformers kernels torch \n```\n\nOnce, setup you can proceed to run the model by running the snippet below:\n\n```py\nfrom transformers import pipeline\nimport torch\n\nmodel_id = \"openai/gpt-oss-120b\"\n\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n]\n\noutputs = pipe(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nAlternatively, you can run the model via [`Transformers Serve`](https://huggingface.co/docs/transformers/main/serving) to spin up a OpenAI-compatible webserver:\n\n```\ntransformers serve\ntransformers chat localhost:8000 --model-name-or-path openai/gpt-oss-120b\n```\n\n[Learn more about how to use gpt-oss with Transformers.](https://cookbook.openai.com/articles/gpt-oss/run-transformers)\n\n## vLLM\n\nvLLM recommends using [uv](https://docs.astral.sh/uv/) for Python dependency management. You can use vLLM to spin up an OpenAI-compatible webserver. The following command will automatically download the model and start the server.\n\n```bash\nuv pip install --pre vllm==0.10.1+gptoss \\\n    --extra-index-url https://wheels.vllm.ai/gpt-oss/ \\\n    --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\\n    --index-strategy unsafe-best-match\n\nvllm serve openai/gpt-oss-120b\n```\n\n[Learn more about how to use gpt-oss with vLLM.](https://cookbook.openai.com/articles/gpt-oss/run-vllm)\n\n## PyTorch / Triton\n\nTo learn about how to use this model with PyTorch and Triton, check out our [reference implementations in the gpt-oss repository](https://github.com/openai/gpt-oss?tab=readme-ov-file#reference-pytorch-implementation).\n\n## Ollama\n\nIf you are trying to run gpt-oss on consumer hardware, you can use Ollama by running the following commands after [installing Ollama](https://ollama.com/download).\n\n```bash\n# gpt-oss-120b\nollama pull gpt-oss:120b\nollama run gpt-oss:120b\n```\n\n[Learn more about how to use gpt-oss with Ollama.](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama)\n\n#### LM Studio\n\nIf you are using [LM Studio](https://lmstudio.ai/) you can use the following commands to download.\n\n```bash\n# gpt-oss-120b\nlms get openai/gpt-oss-120b\n```\n\nCheck out our [awesome list](https://github.com/openai/gpt-oss/blob/main/awesome-gpt-oss.md) for a broader collection of gpt-oss resources and inference partners.\n\n---\n\n# Download the model\n\nYou can download the model weights from the [Hugging Face Hub](https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4) directly from Hugging Face CLI:\n\n```shell\n# gpt-oss-120b\nhuggingface-cli download openai/gpt-oss-120b --include \"original/*\" --local-dir gpt-oss-120b/\npip install gpt-oss\npython -m gpt_oss.chat model/\n```\n\n# Reasoning levels\n\nYou can adjust the reasoning level that suits your task across three levels:\n\n* **Low:** Fast responses for general dialogue.  \n* **Medium:** Balanced speed and detail.  \n* **High:** Deep and detailed analysis.\n\nThe reasoning level can be set in the system prompts, e.g., \"Reasoning: high\".\n\n# Tool use\n\nThe gpt-oss models are excellent for:\n* Web browsing (using built-in browsing tools)\n* Function calling with defined schemas\n* Agentic operations like browser tasks\n\n# Fine-tuning\n\nBoth gpt-oss models can be fine-tuned for a variety of specialized use cases.\n\nThis larger model `gpt-oss-120b` can be fine-tuned on a single H100 node, whereas the smaller [`gpt-oss-20b`](https://huggingface.co/openai/gpt-oss-20b) can even be fine-tuned on consumer hardware.\n\n# Citation\n\n```bibtex\n@misc{openai2025gptoss120bgptoss20bmodel,\n      title={gpt-oss-120b & gpt-oss-20b Model Card}, \n      author={OpenAI},\n      year={2025},\n      eprint={2508.10925},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2508.10925}, \n}\n```"
    },
    {
      "@id": "ark:59852/model-openai-clip-vit-large-patch14-336-s6vypza8em",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "openai/clip-vit-large-patch14-336",
      "description": "<!-- This model card has been generated automatically according to the information Keras had access to. You should",
      "author": "openai",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "clip",
        "zero-shot-image-classification",
        "generated_from_keras_callback",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [],
      "intendedUseCase": "More information needed",
      "contentUrl": "https://huggingface.co/openai/clip-vit-large-patch14-336/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/openai/clip-vit-large-patch14-336",
      "isPartOf": [],
      "README": "\n<!-- This model card has been generated automatically according to the information Keras had access to. You should\nprobably proofread and complete it, then remove this comment. -->\n\n# clip-vit-large-patch14-336\n\nThis model was trained from scratch on an unknown dataset.\nIt achieves the following results on the evaluation set:\n\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- optimizer: None\n- training_precision: float32\n\n### Training results\n\n\n\n### Framework versions\n\n- Transformers 4.21.3\n- TensorFlow 2.8.2\n- Tokenizers 0.12.1\n"
    },
    {
      "@id": "ark:59852/model-qwen-qwen2-5-vl-7b-instruct-7jyeqs7dqf",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "Qwen/Qwen2.5-VL-7B-Instruct",
      "description": "<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">",
      "author": "Qwen",
      "keywords": [
        "transformers",
        "safetensors",
        "qwen2_5_vl",
        "image-to-text",
        "multimodal",
        "image-text-to-text",
        "conversational",
        "en",
        "arxiv:2309.00071",
        "arxiv:2409.12191",
        "arxiv:2308.12966",
        "license:apache-2.0",
        "text-generation-inference",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "usageInformation": "text = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n<details>\n<summary>Multi image inference</summary>\n\n```python",
      "contentUrl": "https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct/resolve/main/model-00001-of-00005.safetensors",
      "url": "https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Qwen2.5-VL-7B-Instruct\n<a href=\"https://chat.qwenlm.ai/\" target=\"_blank\" style=\"margin: 2px;\">\n    <img alt=\"Chat\" src=\"https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5\" style=\"display: inline-block; vertical-align: middle;\"/>\n</a>\n\n## Introduction\n\nIn the past five months since Qwen2-VL\u2019s release, numerous developers have built new models on the Qwen2-VL vision-language models, providing us with valuable feedback. During this period, we focused on building more useful vision-language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5-VL.\n\n#### Key Enhancements:\n* **Understand things visually**: Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images.\n\n* **Being agentic**: Qwen2.5-VL directly plays as a visual agent that can reason and dynamically direct tools, which is capable of computer use and phone use.\n\n* **Understanding long videos and capturing events**: Qwen2.5-VL can comprehend videos of over 1 hour, and this time it has a new ability of cpaturing event by pinpointing the relevant video segments.\n\n* **Capable of visual localization in different formats**: Qwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes.\n\n* **Generating structured outputs**: for data like scans of invoices, forms, tables, etc. Qwen2.5-VL supports structured outputs of their contents, benefiting usages in finance, commerce, etc.\n\n\n#### Model Architecture Updates:\n\n* **Dynamic Resolution and Frame Rate Training for Video Understanding**:\n\nWe extend dynamic resolution to the temporal dimension by adopting dynamic FPS sampling, enabling the model to comprehend videos at various sampling rates. Accordingly, we update mRoPE in the time dimension with IDs and absolute time alignment, enabling the model to learn temporal sequence and speed, and ultimately acquire the ability to pinpoint specific moments.\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL/qwen2.5vl_arc.jpeg\" width=\"80%\"/>\n<p>\n\n\n* **Streamlined and Efficient Vision Encoder**\n\nWe enhance both training and inference speeds by strategically implementing window attention into the ViT. The ViT architecture is further optimized with SwiGLU and RMSNorm, aligning it with the structure of the Qwen2.5 LLM.\n\n\nWe have three models with 3, 7 and 72 billion parameters. This repo contains the instruction-tuned 7B Qwen2.5-VL model. For more information, visit our [Blog](https://qwenlm.github.io/blog/qwen2.5-vl/) and [GitHub](https://github.com/QwenLM/Qwen2.5-VL).\n\n\n\n## Evaluation\n\n### Image benchmark\n\n\n| Benchmark | InternVL2.5-8B | MiniCPM-o 2.6 | GPT-4o-mini | Qwen2-VL-7B |**Qwen2.5-VL-7B** |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| MMMU<sub>val</sub>  | 56 | 50.4 | **60**| 54.1 | 58.6|\n| MMMU-Pro<sub>val</sub>  | 34.3 | - | 37.6| 30.5 | 41.0|\n| DocVQA<sub>test</sub>  | 93 | 93 | - | 94.5 | **95.7** |\n| InfoVQA<sub>test</sub>  | 77.6 | - |  - |76.5 | **82.6** |\n| ChartQA<sub>test</sub>  | 84.8 | - |- | 83.0 |**87.3** |\n| TextVQA<sub>val</sub>  | 79.1 | 80.1 | -| 84.3 | **84.9**|\n| OCRBench | 822 | 852 | 785 | 845 | **864** |\n| CC_OCR | 57.7 |  | | 61.6 | **77.8**|\n| MMStar | 62.8| | |60.7| **63.9**|\n| MMBench-V1.1-En<sub>test</sub>  | 79.4 | 78.0 | 76.0| 80.7 | **82.6** |\n| MMT-Bench<sub>test</sub> | - | - | - |**63.7** |63.6 |\n| MMStar | **61.5** | 57.5 |  54.8 | 60.7 |63.9 |\n| MMVet<sub>GPT-4-Turbo</sub>  | 54.2 | 60.0 | 66.9 | 62.0 | **67.1**|\n| HallBench<sub>avg</sub>  | 45.2 | 48.1 | 46.1| 50.6 | **52.9**|\n| MathVista<sub>testmini</sub>  | 58.3 | 60.6 | 52.4 | 58.2 | **68.2**|\n| MathVision  | - | -  | - | 16.3 | **25.07** |\n\n### Video Benchmarks\n\n| Benchmark |  Qwen2-VL-7B | **Qwen2.5-VL-7B** |\n| :--- | :---: | :---: |\n| MVBench |  67.0 | **69.6** |\n| PerceptionTest<sub>test</sub>  | 66.9 | **70.5** |\n| Video-MME<sub>wo/w subs</sub>   | 63.3/69.0 | **65.1**/**71.6** |\n| LVBench  |  | 45.3 |\n| LongVideoBench  |  | 54.7 |\n| MMBench-Video | 1.44 | 1.79 |\n| TempCompass |  | 71.7 |\n| MLVU |  | 70.2 |\n| CharadesSTA/mIoU |  43.6|\n\n### Agent benchmark\n| Benchmarks              | Qwen2.5-VL-7B |\n|-------------------------|---------------|\n| ScreenSpot              |     84.7    |\n| ScreenSpot Pro          |     29.0    |\n| AITZ_EM                 |  \t81.9    |\n| Android Control High_EM |    \t60.1    |\n| Android Control Low_EM  |  \t93.7    |\n| AndroidWorld_SR         | \t25.5  \t|\n| MobileMiniWob++_SR      | \t91.4    |\n\n## Requirements\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers accelerate\n```\nor you might encounter the following error:\n```\nKeyError: 'qwen2_5_vl'\n```\n\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen2.5-VL with \ud83e\udd16 ModelScope and \ud83e\udd17 Transformers.\n\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers accelerate\n```\nor you might encounter the following error:\n```\nKeyError: 'qwen2_5_vl'\n```\n\n\nWe offer a toolkit to help you handle various types of visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved images and videos. You can install it using the following command:\n\n```bash\n# It's highly recommanded to use `[decord]` feature for faster video loading.\npip install qwen-vl-utils[decord]==0.0.8\n```\n\nIf you are not using Linux, you might not be able to install `decord` from PyPI. In that case, you can use `pip install qwen-vl-utils` which will fall back to using torchvision for video processing. However, you can still [install decord from source](https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source) to get decord used when loading video.\n\n### Using \ud83e\udd17  Transformers to Chat\n\nHere we show a code snippet to show you how to use the chat model with `transformers` and `qwen_vl_utils`:\n\n```python\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2.5-VL-7B-Instruct\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n\n# default processer\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n\n# The default range for the number of visual tokens per image in the model is 4-16384.\n# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n<details>\n<summary>Multi image inference</summary>\n\n```python\n# Messages containing multiple images and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n            {\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n            {\"type\": \"text\", \"text\": \"Identify the similarities between these images.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n</details>\n\n<details>\n<summary>Video inference</summary>\n\n```python\n# Messages containing a images list as a video and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": [\n                    \"file:///path/to/frame1.jpg\",\n                    \"file:///path/to/frame2.jpg\",\n                    \"file:///path/to/frame3.jpg\",\n                    \"file:///path/to/frame4.jpg\",\n                ],\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n# Messages containing a local video path and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": \"file:///path/to/video1.mp4\",\n                \"max_pixels\": 360 * 420,\n                \"fps\": 1.0,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n# Messages containing a video url and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n#In Qwen 2.5 VL, frame rate information is also input into the model to align with absolute time.\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    fps=fps,\n    padding=True,\n    return_tensors=\"pt\",\n    **video_kwargs,\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\nVideo URL compatibility largely depends on the third-party library version. The details are in the table below. change the backend by `FORCE_QWENVL_VIDEO_READER=torchvision` or `FORCE_QWENVL_VIDEO_READER=decord` if you prefer not to use the default one.\n\n| Backend     | HTTP | HTTPS |\n|-------------|------|-------|\n| torchvision >= 0.19.0 | \u2705  | \u2705   |\n| torchvision < 0.19.0  | \u274c  | \u274c   |\n| decord      | \u2705  | \u274c   |\n</details>\n\n<details>\n<summary>Batch inference</summary>\n\n```python\n# Sample messages for batch inference\nmessages1 = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n            {\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n            {\"type\": \"text\", \"text\": \"What are the common elements in these pictures?\"},\n        ],\n    }\n]\nmessages2 = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n# Combine messages for batch processing\nmessages = [messages1, messages2]\n\n# Preparation for batch inference\ntexts = [\n    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n    for msg in messages\n]\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=texts,\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Batch Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_texts = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_texts)\n```\n</details>\n\n### \ud83e\udd16 ModelScope\nWe strongly advise users especially those in mainland China to use ModelScope. `snapshot_download` can help you solve issues concerning downloading checkpoints.\n\n\n### More Usage Tips\n\nFor input images, we support local files, base64, and URLs. For videos, we currently only support local files.\n\n```python\n# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.\n## Local file path\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n## Image URL\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"http://path/to/your/image.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n## Base64 encoded image\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"data:image;base64,/9j/...\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n```\n#### Image Resolution for performance boost\n\nThe model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs, such as a token count range of 256-1280, to balance speed and memory usage.\n\n```python\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n    \"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels\n)\n```\n\nBesides, We provide two methods for fine-grained control over the image size input to the model:\n\n1. Define min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels.\n   \n2. Specify exact dimensions: Directly set `resized_height` and `resized_width`. These values will be rounded to the nearest multiple of 28.\n\n```python\n# min_pixels and max_pixels\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"file:///path/to/your/image.jpg\",\n                \"resized_height\": 280,\n                \"resized_width\": 420,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n# resized_height and resized_width\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"file:///path/to/your/image.jpg\",\n                \"min_pixels\": 50176,\n                \"max_pixels\": 50176,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n\n{\n\t...,\n    \"type\": \"yarn\",\n    \"mrope_section\": [\n        16,\n        24,\n        24\n    ],\n    \"factor\": 4,\n    \"original_max_position_embeddings\": 32768\n}\n\nHowever, it should be noted that this method has a significant impact on the performance of temporal and spatial localization tasks, and is therefore not recommended for use.\n\nAt the same time, for long video inputs, since MRoPE itself is more economical with ids, the max_position_embeddings can be directly modified to a larger value, such as 64k.\n\n\n\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5-VL,\n    title = {Qwen2.5-VL},\n    url = {https://qwenlm.github.io/blog/qwen2.5-vl/},\n    author = {Qwen Team},\n    month = {January},\n    year = {2025}\n}\n\n@article{Qwen2VL,\n  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\n  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\n  journal={arXiv preprint arXiv:2409.12191},\n  year={2024}\n}\n\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```\n"
    },
    {
      "@id": "ark:59852/model-jonatasgrosman-wav2vec2-large-xlsr-53-dutch-ewfbnpbhgg",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "jonatasgrosman/wav2vec2-large-xlsr-53-dutch",
      "description": "Fine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Dutch using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice) and [CSS10](https://github.com/Kyubyong/css10).",
      "author": "jonatasgrosman",
      "keywords": [
        "transformers",
        "pytorch",
        "jax",
        "wav2vec2",
        "automatic-speech-recognition",
        "audio",
        "hf-asr-leaderboard",
        "mozilla-foundation/common_voice_6_0",
        "nl",
        "robust-speech-event",
        "speech",
        "xlsr-fine-tuning-week",
        "dataset:common_voice",
        "dataset:mozilla-foundation/common_voice_6_0",
        "doi:10.57967/hf/0203",
        "license:apache-2.0",
        "model-index",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/common_voice"
        },
        {
          "@id": "https://huggingface.co/datasets/mozilla-foundation/common_voice_6_0"
        }
      ],
      "usageInformation": "The model can be used directly (without a language model) as follows...\n\nUsing the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-dutch\")\naudio_paths = [\"/path/to/file.mp3\", \"/path/to/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"nl\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-dutch\"\nSAMPLES = 10\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)",
      "contentUrl": "https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-dutch/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-dutch",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Fine-tuned XLSR-53 large model for speech recognition in Dutch\n\nFine-tuned [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) on Dutch using the train and validation splits of [Common Voice 6.1](https://huggingface.co/datasets/common_voice) and [CSS10](https://github.com/Kyubyong/css10).\nWhen using this model, make sure that your speech input is sampled at 16kHz.\n\nThis model has been fine-tuned thanks to the GPU credits generously given by the [OVHcloud](https://www.ovhcloud.com/en/public-cloud/ai-training/) :)\n\nThe script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint\n\n## Usage\n\nThe model can be used directly (without a language model) as follows...\n\nUsing the [HuggingSound](https://github.com/jonatasgrosman/huggingsound) library:\n\n```python\nfrom huggingsound import SpeechRecognitionModel\n\nmodel = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-dutch\")\naudio_paths = [\"/path/to/file.mp3\", \"/path/to/another_file.wav\"]\n\ntranscriptions = model.transcribe(audio_paths)\n```\n\nWriting your own inference script:\n\n```python\nimport torch\nimport librosa\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n\nLANG_ID = \"nl\"\nMODEL_ID = \"jonatasgrosman/wav2vec2-large-xlsr-53-dutch\"\nSAMPLES = 10\n\ntest_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n\nprocessor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\nmodel = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n\n# Preprocessing the datasets.\n# We need to read the audio files as arrays\ndef speech_file_to_array_fn(batch):\n    speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16_000)\n    batch[\"speech\"] = speech_array\n    batch[\"sentence\"] = batch[\"sentence\"].upper()\n    return batch\n\ntest_dataset = test_dataset.map(speech_file_to_array_fn)\ninputs = processor(test_dataset[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n\nwith torch.no_grad():\n    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n\npredicted_ids = torch.argmax(logits, dim=-1)\npredicted_sentences = processor.batch_decode(predicted_ids)\n\nfor i, predicted_sentence in enumerate(predicted_sentences):\n    print(\"-\" * 100)\n    print(\"Reference:\", test_dataset[i][\"sentence\"])\n    print(\"Prediction:\", predicted_sentence)\n```\n\n| Reference  | Prediction |\n| ------------- | ------------- |\n| DE ABORIGINALS ZIJN DE OORSPRONKELIJKE BEWONERS VAN AUSTRALI\u00cb. | DE ABBORIGENALS ZIJN DE OORSPRONKELIJKE BEWONERS VAN AUSTRALI\u00cb |\n| MIJN TOETSENBORD ZIT VOL STOF. | MIJN TOETSENBORD ZIT VOL STOF |\n| ZE HAD DE BANK BESCHADIGD MET HAAR SKATEBOARD. | ZE HAD DE BANK BESCHADIGD MET HAAR SCHEETBOORD |\n| WAAR LAAT JIJ JE ONDERHOUD DOEN? | WAAR LAAT JIJ HET ONDERHOUD DOEN |\n| NA HET LEZEN VAN VELE BEOORDELINGEN HAD ZE EINDELIJK HAAR OOG LATEN VALLEN OP EEN LAPTOP MET EEN QWERTY TOETSENBORD. | NA HET LEZEN VAN VELE BEOORDELINGEN HAD ZE EINDELIJK HAAR OOG LATEN VALLEN OP EEN LAPTOP MET EEN QUERTITOETSEMBORD |\n| DE TAMPONS ZIJN OP. | DE TAPONT ZIJN OP |\n| MARIJKE KENT OLIVIER NU AL MEER DAN TWEE JAAR. | MAARRIJKEN KENT OLIEVIER NU AL MEER DAN TWEE JAAR |\n| HET VOEREN VAN BROOD AAN EENDEN IS EIGENLIJK ONGEZOND VOOR DE BEESTEN. | HET VOEREN VAN BEUROT AAN EINDEN IS EIGENLIJK ONGEZOND VOOR DE BEESTEN |\n| PARKET MOET JE STOFZUIGEN, TEGELS MOET JE DWEILEN. | PARKET MOET JE STOF ZUIGEN MAAR TEGELS MOET JE DWEILEN |\n| IN ONZE BUURT KENT IEDEREEN ELKAAR. | IN ONZE BUURT KENT IEDEREEN ELKAAR |\n\n## Evaluation\n\n1. To evaluate on `mozilla-foundation/common_voice_6_0` with split `test`\n\n```bash\npython eval.py --model_id jonatasgrosman/wav2vec2-large-xlsr-53-dutch --dataset mozilla-foundation/common_voice_6_0 --config nl --split test\n```\n\n2. To evaluate on `speech-recognition-community-v2/dev_data`\n\n```bash\npython eval.py --model_id jonatasgrosman/wav2vec2-large-xlsr-53-dutch --dataset speech-recognition-community-v2/dev_data --config nl --split validation --chunk_length_s 5.0 --stride_length_s 1.0\n```\n\n## Citation\nIf you want to cite this model you can use this:\n\n```bibtex\n@misc{grosman2021xlsr53-large-dutch,\n  title={Fine-tuned {XLSR}-53 large model for speech recognition in {D}utch},\n  author={Grosman, Jonatas},\n  howpublished={\\url{https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-dutch}},\n  year={2021}\n}\n```"
    },
    {
      "@id": "ark:59852/model-mahmoudashraf-mms-300m-1130-forced-aligner-8gxrkn6r7va",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "MahmoudAshraf/mms-300m-1130-forced-aligner",
      "description": "This Python package provides an efficient way to perform forced alignment between text and audio using Hugging Face's pretrained models. it also features an improved implementation to use much less memory than TorchAudio forced alignment API.",
      "author": "MahmoudAshraf",
      "keywords": [
        "transformers",
        "pytorch",
        "safetensors",
        "wav2vec2",
        "automatic-speech-recognition",
        "mms",
        "audio",
        "voice",
        "speech",
        "forced-alignment",
        "ab",
        "af",
        "ak",
        "am",
        "ar",
        "as",
        "av",
        "ay",
        "az",
        "ba",
        "bm",
        "be",
        "bn",
        "bi",
        "bo",
        "sh",
        "br",
        "bg",
        "ca",
        "cs",
        "ce",
        "cv",
        "ku",
        "cy",
        "da",
        "de",
        "dv",
        "dz",
        "el",
        "en",
        "eo",
        "et",
        "eu",
        "ee",
        "fo",
        "fa",
        "fj",
        "fi",
        "fr",
        "fy",
        "ff",
        "ga",
        "gl",
        "gn",
        "gu",
        "zh",
        "ht",
        "ha",
        "he",
        "hi",
        "hu",
        "hy",
        "ig",
        "ia",
        "ms",
        "is",
        "it",
        "jv",
        "ja",
        "kn",
        "ka",
        "kk",
        "kr",
        "km",
        "ki",
        "rw",
        "ky",
        "ko",
        "kv",
        "lo",
        "la",
        "lv",
        "ln",
        "lt",
        "lb",
        "lg",
        "mh",
        "ml",
        "mr",
        "mk",
        "mg",
        "mt",
        "mn",
        "mi",
        "my",
        "nl",
        "no",
        "ne",
        "ny",
        "oc",
        "om",
        "or",
        "os",
        "pa",
        "pl",
        "pt",
        "ps",
        "qu",
        "ro",
        "rn",
        "ru",
        "sg",
        "sk",
        "sl",
        "sm",
        "sn",
        "sd",
        "so",
        "es",
        "sq",
        "su",
        "sv",
        "sw",
        "ta",
        "tt",
        "te",
        "tg",
        "tl",
        "th",
        "ti",
        "ts",
        "tr",
        "uk",
        "vi",
        "wo",
        "xh",
        "yo",
        "zu",
        "za",
        "license:cc-by-nc-4.0",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "usageInformation": "```python\nimport torch\nfrom ctc_forced_aligner import (\n    load_audio,\n    load_alignment_model,\n    generate_emissions,\n    preprocess_text,\n    get_alignments,\n    get_spans,\n    postprocess_results,\n)\n\naudio_path = \"your/audio/path\"\ntext_path = \"your/text/path\"\nlanguage = \"iso\" # ISO-639-3 Language code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nbatch_size = 16\n\n\nalignment_model, alignment_tokenizer = load_alignment_model(\n    device,\n    dtype=torch.float16 if device == \"cuda\" else torch.float32,\n)\n\naudio_waveform = load_audio(audio_path, alignment_model.dtype, alignment_model.device)\n\n\nwith open(text_path, \"r\") as f:\n    lines = f.readlines()\ntext = \"\".join(line for line in lines).replace(\"\\n\", \" \").strip()\n\nemissions, stride = generate_emissions(\n    alignment_model, audio_waveform, batch_size=batch_size\n)\n\ntokens_starred, text_starred = preprocess_text(\n    text,\n    romanize=True,\n    language=language,\n)\n\nsegments, scores, blank_token = get_alignments(\n    emissions,\n    tokens_starred,\n    alignment_tokenizer,\n)\n\nspans = get_spans(tokens_starred, segments, blank_token)\n\nword_timestamps = postprocess_results(text_starred, spans, stride, scores)\n```",
      "contentUrl": "https://huggingface.co/MahmoudAshraf/mms-300m-1130-forced-aligner/resolve/main/model.safetensors",
      "url": "https://huggingface.co/MahmoudAshraf/mms-300m-1130-forced-aligner",
      "license": "cc-by-nc-4.0",
      "isPartOf": [],
      "README": "\n# Forced Alignment with Hugging Face CTC Models\nThis Python package provides an efficient way to perform forced alignment between text and audio using Hugging Face's pretrained models. it also features an improved implementation to use much less memory than TorchAudio forced alignment API.\n\nThe model checkpoint uploaded here is a conversion from torchaudio to HF Transformers for the MMS-300M checkpoint trained on forced alignment dataset\n\n## Installation\n\n```bash\npip install git+https://github.com/MahmoudAshraf97/ctc-forced-aligner.git\n```\n## Usage\n\n```python\nimport torch\nfrom ctc_forced_aligner import (\n    load_audio,\n    load_alignment_model,\n    generate_emissions,\n    preprocess_text,\n    get_alignments,\n    get_spans,\n    postprocess_results,\n)\n\naudio_path = \"your/audio/path\"\ntext_path = \"your/text/path\"\nlanguage = \"iso\" # ISO-639-3 Language code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nbatch_size = 16\n\n\nalignment_model, alignment_tokenizer = load_alignment_model(\n    device,\n    dtype=torch.float16 if device == \"cuda\" else torch.float32,\n)\n\naudio_waveform = load_audio(audio_path, alignment_model.dtype, alignment_model.device)\n\n\nwith open(text_path, \"r\") as f:\n    lines = f.readlines()\ntext = \"\".join(line for line in lines).replace(\"\\n\", \" \").strip()\n\nemissions, stride = generate_emissions(\n    alignment_model, audio_waveform, batch_size=batch_size\n)\n\ntokens_starred, text_starred = preprocess_text(\n    text,\n    romanize=True,\n    language=language,\n)\n\nsegments, scores, blank_token = get_alignments(\n    emissions,\n    tokens_starred,\n    alignment_tokenizer,\n)\n\nspans = get_spans(tokens_starred, segments, blank_token)\n\nword_timestamps = postprocess_results(text_starred, spans, stride, scores)\n```"
    },
    {
      "@id": "ark:59852/model-baai-bge-base-en-v1-5-kar49hskbb",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "BAAI/bge-base-en-v1.5",
      "description": "<h1 align=\"center\">FlagEmbedding</h1>",
      "author": "BAAI",
      "keywords": [
        "sentence-transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "bert",
        "feature-extraction",
        "sentence-similarity",
        "transformers",
        "mteb",
        "en",
        "arxiv:2401.03462",
        "arxiv:2312.15503",
        "arxiv:2311.13534",
        "arxiv:2310.07554",
        "arxiv:2309.07597",
        "license:mit",
        "model-index",
        "autotrain_compatible",
        "text-embeddings-inference",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "usageInformation": "Here are some examples for using `bge` models with \n[FlagEmbedding](#using-flagembedding), [Sentence-Transformers](#using-sentence-transformers), [Langchain](#using-langchain), or [Huggingface Transformers](#using-huggingface-transformers).\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\nIf it doesn't work for you, you can see [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md) for more methods to install FlagEmbedding.\n\n```python\nfrom FlagEmbedding import FlagModel\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = FlagModel('BAAI/bge-large-zh-v1.5', \n                  query_instruction_for_retrieval=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\",\n                  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)",
      "contentUrl": "https://huggingface.co/BAAI/bge-base-en-v1.5/resolve/main/model.safetensors",
      "url": "https://huggingface.co/BAAI/bge-base-en-v1.5",
      "license": "mit",
      "isPartOf": [],
      "README": "\n\n<h1 align=\"center\">FlagEmbedding</h1>\n\n\n<h4 align=\"center\">\n    <p>\n        <a href=#model-list>Model List</a> | \n        <a href=#frequently-asked-questions>FAQ</a> |\n        <a href=#usage>Usage</a>  |\n        <a href=\"#evaluation\">Evaluation</a> |\n        <a href=\"#train\">Train</a> |\n        <a href=\"#contact\">Contact</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\nFor more details please refer to our Github: [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding).\n\nIf you are looking for a model that supports more languages, longer texts, and other retrieval methods, you can try using [bge-m3](https://huggingface.co/BAAI/bge-m3).\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\nFlagEmbedding focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n- **Long-Context LLM**: [Activation Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon)\n- **Fine-tuning of LM** : [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail)\n- **Dense Retrieval**: [BGE-M3](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3), [LLM Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), [BGE Embedding](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding)\n- **Reranker Model**: [BGE Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n- **Benchmark**: [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)\n\n## News \n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/BGE_M3/BGE_M3.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) :fire:\n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) :fire:\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) :fire:  \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) and [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n \n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n    \n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n</details>\n\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n|              Model              | Language | | Description | query instruction for retrieval [1] |\n|:-------------------------------|:--------:| :--------:| :--------:|:--------:|\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     |    [Inference](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3#usage) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3)    | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n|  [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)  |   English | [Inference](./FlagEmbedding/llm_embedder/README.md) [Fine-tune](./FlagEmbedding/llm_embedder/README.md) | a unified embedding model to support diverse retrieval augmentation needs for LLMs | See [README](./FlagEmbedding/llm_embedder/README.md) |\n|  [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)  |   Chinese and English | [Inference](#usage-for-reranker) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) | a cross-encoder model which is more accurate but less efficient [2] |   |\n|  [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) |   Chinese and English | [Inference](#usage-for-reranker) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) | a cross-encoder model which is more accurate but less efficient [2] |   |\n|  [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution  | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5) |   Chinese |  [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | :trophy: rank **1st** in [MTEB](https://huggingface.co/spaces/mteb/leaderboard) leaderboard | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a base-scale model but with similar ability to `bge-large-en` | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) |a small-scale model but with competitive performance  | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | :trophy: rank **1st** in [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB) benchmark | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) |   Chinese |  [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a base-scale model but with similar ability to `bge-large-zh` | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a small-scale model but with competitive performance | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n\n\n[1\\]: If you need to search the relevant passages to a query, we suggest to add the instruction to the query; in other cases, no instruction is needed, just use the original query directly. In all cases, **no instruction** needs to be added to passages.\n\n[2\\]: Different from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. To balance the accuracy and time cost, cross-encoder is widely used to re-rank top-k documents retrieved by other simple models. \nFor examples, use bge embedding model to retrieve top 100 relevant documents, and then use bge reranker to re-rank the top 100 document to get the final top-3 results.\n\nAll models have been uploaded to Huggingface Hub, and you can see them at https://huggingface.co/BAAI. \nIf you cannot open the Huggingface Hub, you also can download the models at https://model.baai.ac.cn/models .\n\n\n## Frequently asked questions\n\n<details>\n  <summary>1. How to fine-tune bge embedding model?</summary>\n\n  <!-- ### How to fine-tune bge embedding model? -->\nFollowing this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) to prepare data and fine-tune your model. \nSome suggestions:\n- Mine hard negatives following this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune#hard-negatives), which can improve the retrieval performance.\n- If you pre-train bge on your data, the pre-trained model cannot be directly used to calculate similarity, and it must be fine-tuned with contrastive learning before computing similarity.\n- If the accuracy of the fine-tuned model is still not high, it is recommended to use/fine-tune the cross-encoder model (bge-reranker) to re-rank top-k results. Hard negatives also are needed to fine-tune reranker.\n\n  \n</details>\n\n<details>\n  <summary>2. The similarity score between two dissimilar sentences is higher than 0.5</summary>\n\n  <!-- ### The similarity score between two dissimilar sentences is higher than 0.5 -->\n**Suggest to use bge v1.5, which alleviates the issue of the similarity distribution.** \n\nSince we finetune the models by contrastive learning with a temperature of 0.01, \nthe similarity distribution of the current BGE model is about in the interval \\[0.6, 1\\].\nSo a similarity score greater than 0.5 does not indicate that the two sentences are similar.\n\nFor downstream tasks, such as passage retrieval or semantic similarity, \n**what matters is the relative order of the scores, not the absolute value.**\nIf you need to filter similar sentences based on a similarity threshold, \nplease select an appropriate similarity threshold based on the similarity distribution on your data (such as 0.8, 0.85, or even 0.9).\n\n</details>\n\n<details>\n  <summary>3. When does the query instruction need to be used</summary>\n\n  <!-- ### When does the query instruction need to be used -->\n\nFor the `bge-*-v1.5`, we improve its retrieval ability when not using instruction. \nNo instruction only has a slight degradation in retrieval performance compared with using instruction. \nSo you can generate embedding without instruction in all cases for convenience.\n \nFor a retrieval task that uses short queries to find long related documents, \nit is recommended to add instructions for these short queries.\n**The best method to decide whether to add instructions for queries is choosing the setting that achieves better performance on your task.**\nIn all cases, the documents/passages do not need to add the instruction. \n\n</details>\n\n\n## Usage \n\n### Usage for Embedding Model\n\nHere are some examples for using `bge` models with \n[FlagEmbedding](#using-flagembedding), [Sentence-Transformers](#using-sentence-transformers), [Langchain](#using-langchain), or [Huggingface Transformers](#using-huggingface-transformers).\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\nIf it doesn't work for you, you can see [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md) for more methods to install FlagEmbedding.\n\n```python\nfrom FlagEmbedding import FlagModel\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = FlagModel('BAAI/bge-large-zh-v1.5', \n                  query_instruction_for_retrieval=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\",\n                  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n\n# for s2p(short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\nqueries = ['query_1', 'query_2']\npassages = [\"\u6837\u4f8b\u6587\u6863-1\", \"\u6837\u4f8b\u6587\u6863-2\"]\nq_embeddings = model.encode_queries(queries)\np_embeddings = model.encode(passages)\nscores = q_embeddings @ p_embeddings.T\n```\nFor the value of the argument `query_instruction_for_retrieval`, see [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list). \n\nBy default, FlagModel will use all available GPUs when encoding. Please set `os.environ[\"CUDA_VISIBLE_DEVICES\"]` to select specific GPUs.\nYou also can set `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"` to make all GPUs unavailable.\n\n\n#### Using Sentence-Transformers\n\nYou can also use the `bge` models with [sentence-transformers](https://www.SBERT.net):\n\n```\npip install -U sentence-transformers\n```\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nembeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\nembeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\nFor s2p(short query to long passage) retrieval task, \neach short query should start with an instruction (instructions see [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list)). \nBut the instruction is not needed for passages.\n```python\nfrom sentence_transformers import SentenceTransformer\nqueries = ['query_1', 'query_2']\npassages = [\"\u6837\u4f8b\u6587\u6863-1\", \"\u6837\u4f8b\u6587\u6863-2\"]\ninstruction = \"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nq_embeddings = model.encode([instruction+q for q in queries], normalize_embeddings=True)\np_embeddings = model.encode(passages, normalize_embeddings=True)\nscores = q_embeddings @ p_embeddings.T\n```\n\n#### Using Langchain \n\nYou can use `bge` in langchain like this:\n```python\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nmodel_name = \"BAAI/bge-large-en-v1.5\"\nmodel_kwargs = {'device': 'cuda'}\nencode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\nmodel = HuggingFaceBgeEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs,\n    query_instruction=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n)\nmodel.query_instruction = \"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n```\n\n\n#### Using HuggingFace Transformers\n\nWith the transformers package, you can use the model like this: First, you pass your input through the transformer model, then you select the last hidden state of the first token (i.e., [CLS]) as the sentence embedding.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n# Sentences we want sentence embeddings for\nsentences = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel.eval()\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n    # Perform pooling. In this case, cls pooling.\n    sentence_embeddings = model_output[0][:, 0]\n# normalize embeddings\nsentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\", sentence_embeddings)\n```\n\n\n#### Usage of the ONNX files\n\n```python\nfrom optimum.onnxruntime import ORTModelForFeatureExtraction  # type: ignore\n\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-en-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-large-en-v1.5', revision=\"refs/pr/13\")\nmodel_ort = ORTModelForFeatureExtraction.from_pretrained('BAAI/bge-large-en-v1.5', revision=\"refs/pr/13\",file_name=\"onnx/model.onnx\")\n\n# Sentences we want sentence embeddings for\nsentences = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n\nmodel_output_ort = model_ort(**encoded_input)\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# model_output and model_output_ort are identical\n\n```\n\n#### Usage via infinity\nIts also possible to deploy the onnx files with the [infinity_emb](https://github.com/michaelfeil/infinity) pip package.\n```python\nimport asyncio\nfrom infinity_emb import AsyncEmbeddingEngine, EngineArgs\n\nsentences = [\"Embed this is sentence via Infinity.\", \"Paris is in France.\"]\nengine = AsyncEmbeddingEngine.from_args(\n    EngineArgs(model_name_or_path = \"BAAI/bge-large-en-v1.5\", device=\"cpu\", engine=\"optimum\" # or engine=\"torch\"\n))\n\nasync def main(): \n    async with engine:\n        embeddings, usage = await engine.embed(sentences=sentences)\nasyncio.run(main())\n```\n\n### Usage for Reranker\n\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. \nYou can get a relevance score by inputting query and passage to the reranker. \nThe reranker is optimized based cross-entropy loss, so the relevance score is not bounded to a specific range.\n\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\n\nGet relevance scores (higher scores indicate more relevance):\n```python\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\n```\n\n\n#### Using Huggingface transformers\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n```\n\n## Evaluation  \n\n`baai-general-embedding` models achieve **state-of-the-art performance on both MTEB and C-MTEB leaderboard!**\nFor more details and evaluation tools see our [scripts](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md). \n\n- **MTEB**:   \n\n| Model Name |  Dimension | Sequence Length | Average (56) | Retrieval (15) |Clustering (11) | Pair Classification (3) | Reranking (4) |  STS (10) | Summarization (1) | Classification (12) |\n|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) | 1024 | 512 |  **64.23** | **54.29** |  46.08 | 87.12 | 60.03 | 83.11 | 31.61 | 75.97 |  \n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |  768 | 512 | 63.55 | 53.25 |   45.77 | 86.55 | 58.86 | 82.4 | 31.07 | 75.53 |  \n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |  384 | 512 | 62.17 |51.68 | 43.82 |  84.92 | 58.36 | 81.59 | 30.12 | 74.14 |  \n| [bge-large-en](https://huggingface.co/BAAI/bge-large-en) |  1024 | 512 | 63.98 |  53.9 | 46.98 | 85.8 | 59.48 | 81.56 | 32.06 | 76.21 | \n| [bge-base-en](https://huggingface.co/BAAI/bge-base-en) |  768 | 512 |  63.36 | 53.0 | 46.32 | 85.86 | 58.7 | 81.84 | 29.27 | 75.27 | \n| [gte-large](https://huggingface.co/thenlper/gte-large) |  1024 | 512 | 63.13 | 52.22 | 46.84 | 85.00 | 59.13 | 83.35 | 31.66 | 73.33 |\n| [gte-base](https://huggingface.co/thenlper/gte-base) \t|  768 | 512 | 62.39 | 51.14 | 46.2 | 84.57 | 58.61 | 82.3 | 31.17 | 73.01 |\n| [e5-large-v2](https://huggingface.co/intfloat/e5-large-v2) |  1024| 512 | 62.25 | 50.56 | 44.49 | 86.03 | 56.61 | 82.05 | 30.19 | 75.24 |\n| [bge-small-en](https://huggingface.co/BAAI/bge-small-en) |  384 | 512 | 62.11 |  51.82 | 44.31 | 83.78 | 57.97 | 80.72 | 30.53 | 74.37 |  \n| [instructor-xl](https://huggingface.co/hkunlp/instructor-xl) |  768 | 512 | 61.79 | 49.26 | 44.74 | 86.62 | 57.29 | 83.06 | 32.32 | 61.79 |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2) |  768 | 512 | 61.5 | 50.29 | 43.80 | 85.73 | 55.91 | 81.05 | 30.28 | 73.84 |\n| [gte-small](https://huggingface.co/thenlper/gte-small) |  384 | 512 | 61.36 | 49.46 | 44.89 | 83.54 | 57.7 | 82.07 | 30.42 | 72.31 |\n| [text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings) | 1536 | 8192 | 60.99 | 49.25 | 45.9 | 84.89 | 56.32 | 80.97 | 30.8 | 70.93 |\n| [e5-small-v2](https://huggingface.co/intfloat/e5-base-v2) | 384 | 512 | 59.93 | 49.04 | 39.92 | 84.67 | 54.32 | 80.39 | 31.16 | 72.94 |\n| [sentence-t5-xxl](https://huggingface.co/sentence-transformers/sentence-t5-xxl) |  768 | 512 | 59.51 | 42.24 | 43.72 | 85.06 | 56.42 | 82.63 | 30.08 | 73.42 |\n| [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) \t|  768 | 514 \t| 57.78 | 43.81 | 43.69 | 83.04 | 59.36 | 80.28 | 27.49 | 65.07 |\n| [sgpt-bloom-7b1-msmarco](https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco) \t|  4096 | 2048 | 57.59 | 48.22 | 38.93 | 81.9 | 55.65 | 77.74 | 33.6 | 66.19 |\n\n\n\n- **C-MTEB**:  \nWe create the benchmark C-MTEB for Chinese text embedding which consists of 31 datasets from 6 tasks. \nPlease refer to [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md) for a detailed introduction.\n \n| Model | Embedding dimension | Avg | Retrieval | STS | PairClassification | Classification | Reranking | Clustering |\n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n| [**BAAI/bge-large-zh-v1.5**](https://huggingface.co/BAAI/bge-large-zh-v1.5) | 1024 |  **64.53** | 70.46 | 56.25 | 81.6 | 69.13 | 65.84 | 48.99 |  \n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5) | 768 |  63.13 | 69.49 | 53.72 | 79.75 | 68.07 | 65.39 | 47.53 |  \n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) | 512 | 57.82 | 61.77 | 49.11 | 70.41 | 63.96 | 60.92 | 44.18 |   \n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) | 1024 | 64.20 | 71.53 | 54.98 | 78.94 | 68.32 | 65.11 | 48.39 |\n| [bge-large-zh-noinstruct](https://huggingface.co/BAAI/bge-large-zh-noinstruct) | 1024 | 63.53 | 70.55 | 53 | 76.77 | 68.58 | 64.91 | 50.01 |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) | 768 | 62.96 | 69.53 | 54.12 | 77.5 | 67.07 | 64.91 | 47.63 |\n| [multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large) | 1024 | 58.79 | 63.66 | 48.44 | 69.89 | 67.34 | 56.00 | 48.23 |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) | 512 | 58.27 |  63.07 | 49.45 | 70.35 | 63.64 | 61.48 | 45.09 |\n| [m3e-base](https://huggingface.co/moka-ai/m3e-base) | 768 | 57.10 | 56.91 | 50.47 | 63.99 | 67.52 | 59.34 | 47.68 |\n| [m3e-large](https://huggingface.co/moka-ai/m3e-large) | 1024 |  57.05 | 54.75 | 50.42 | 64.3 | 68.2 | 59.66 | 48.88 |\n| [multilingual-e5-base](https://huggingface.co/intfloat/multilingual-e5-base) | 768 | 55.48 | 61.63 | 46.49 | 67.07 | 65.35 | 54.35 | 40.68 |\n| [multilingual-e5-small](https://huggingface.co/intfloat/multilingual-e5-small) | 384 | 55.38 | 59.95 | 45.27 | 66.45 | 65.85 | 53.86 | 45.26 |\n| [text-embedding-ada-002(OpenAI)](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings) | 1536 |  53.02 | 52.0 | 43.35 | 69.56 | 64.31 | 54.28 | 45.68 |\n| [luotuo](https://huggingface.co/silk-road/luotuo-bert-medium) | 1024 | 49.37 |  44.4 | 42.78 | 66.62 | 61 | 49.25 | 44.39 |\n| [text2vec-base](https://huggingface.co/shibing624/text2vec-base-chinese) | 768 |  47.63 | 38.79 | 43.41 | 67.41 | 62.19 | 49.45 | 37.66 |\n| [text2vec-large](https://huggingface.co/GanymedeNil/text2vec-large-chinese) | 1024 | 47.36 | 41.94 | 44.97 | 70.86 | 60.66 | 49.16 | 30.02 |\n\n\n- **Reranking**:\nSee [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/) for evaluation script.\n\n| Model | T2Reranking | T2RerankingZh2En\\* | T2RerankingEn2Zh\\* | MMarcoReranking | CMedQAv1 | CMedQAv2 | Avg |  \n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|  \n| text2vec-base-multilingual | 64.66 | 62.94 | 62.51 | 14.37 | 48.46 | 48.6 | 50.26 |  \n| multilingual-e5-small | 65.62 | 60.94 | 56.41 | 29.91 | 67.26 | 66.54 | 57.78 |  \n| multilingual-e5-large | 64.55 | 61.61 | 54.28 | 28.6 | 67.42 | 67.92 | 57.4 |  \n| multilingual-e5-base | 64.21 | 62.13 | 54.68 | 29.5 | 66.23 | 66.98 | 57.29 |  \n| m3e-base | 66.03 | 62.74 | 56.07 | 17.51 | 77.05 | 76.76 | 59.36 |  \n| m3e-large | 66.13 | 62.72 | 56.1 | 16.46 | 77.76 | 78.27 | 59.57 |  \n| bge-base-zh-v1.5 | 66.49 | 63.25 | 57.02 | 29.74 | 80.47 | 84.88 | 63.64 |  \n| bge-large-zh-v1.5 | 65.74 | 63.39 | 57.03 | 28.74 | 83.45 | 85.44 | 63.97 |  \n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) | 67.28 | 63.95 | 60.45 | 35.46 | 81.26 | 84.1 | 65.42 |  \n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) | 67.6 | 64.03 | 61.44 | 37.16 | 82.15 | 84.18 | 66.09 |  \n\n\\* : T2RerankingZh2En and T2RerankingEn2Zh are cross-language retrieval tasks\n\n## Train\n\n### BAAI Embedding \n\nWe pre-train the models using [retromae](https://github.com/staoxiao/RetroMAE) and train them on large-scale pairs data using contrastive learning. \n**You can fine-tune the embedding model on your data following our [examples](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune).**\nWe also provide a [pre-train example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/pretrain).\nNote that the goal of pre-training is to reconstruct the text, and the pre-trained model cannot be used for similarity calculation directly, it needs to be fine-tuned.\nMore training details for bge see [baai_general_embedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md).\n\n\n\n### BGE Reranker\n\nCross-encoder will perform full-attention over the input pair, \nwhich is more accurate than embedding model (i.e., bi-encoder) but more time-consuming than embedding model.\nTherefore, it can be used to re-rank the top-k documents returned by embedding model.\nWe train the cross-encoder on a multilingual pair data, \nThe data format is the same as embedding model, so you can fine-tune it easily following our [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker). \nMore details please refer to [./FlagEmbedding/reranker/README.md](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n\n\n## Contact\nIf you have any question or suggestion related to this project, feel free to open an issue or pull request.\nYou also can email Shitao Xiao(stxiao@baai.ac.cn) and Zheng Liu(liuzheng@baai.ac.cn). \n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge.\n\n"
    },
    {
      "@id": "ark:59852/model-hexgrad-kokoro-82m-iodpacdvsyr",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "hexgrad/Kokoro-82M",
      "description": "**Kokoro** is an open-weight TTS model with 82 million parameters. Despite its lightweight architecture, it delivers comparable quality to larger models while being significantly faster and more cost-efficient. With Apache-licensed weights, Kokoro can be deployed anywhere from production environments to personal projects.",
      "author": "hexgrad",
      "keywords": [
        "text-to-speech",
        "en",
        "arxiv:2306.07691",
        "arxiv:2203.02395",
        "base_model:yl4579/StyleTTS2-LJSpeech",
        "base_model:finetune:yl4579/StyleTTS2-LJSpeech",
        "doi:10.57967/hf/4329",
        "license:apache-2.0",
        "region:us"
      ],
      "version": "1.0",
      "modelFormat": "pytorch-pth",
      "trainingDataset": [],
      "intendedUseCase": "> [!NOTE]\n> As of April 2025, the market rate of Kokoro served over API is **under $1 per million characters of text input**, or under $0.06 per hour of audio output. (On average, 1000 characters of input is about 1 minute of output.) Sources: [ArtificialAnalysis/Replicate at 65 cents per M chars](https://artificialanalysis.ai/text-to-speech/model-family/kokoro#price) and [DeepInfra at 80 cents per M chars](https://deepinfra.com/hexgrad/Kokoro-82M).\n>\n> This is an Apache-licensed model, and Kokoro has been deployed in numerous projects and commercial APIs. We welcome the deployment of the model in real use cases.",
      "usageInformation": "You can run this basic cell on [Google Colab](https://colab.research.google.com/). [Listen to samples](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md). For more languages and details, see [Advanced Usage](https://github.com/hexgrad/kokoro?tab=readme-ov-file#advanced-usage).\n```py\n!pip install -q kokoro>=0.9.2 soundfile\n!apt-get -qq -y install espeak-ng > /dev/null 2>&1\nfrom kokoro import KPipeline\nfrom IPython.display import display, Audio\nimport soundfile as sf\nimport torch\npipeline = KPipeline(lang_code='a')\ntext = '''\n[Kokoro](/k\u02c8Ok\u0259\u0279O/) is an open-weight TTS model with 82 million parameters. Despite its lightweight architecture, it delivers comparable quality to larger models while being significantly faster and more cost-efficient. With Apache-licensed weights, [Kokoro](/k\u02c8Ok\u0259\u0279O/) can be deployed anywhere from production environments to personal projects.\n'''\ngenerator = pipeline(text, voice='af_heart')\nfor i, (gs, ps, audio) in enumerate(generator):\n    print(i, gs, ps)\n    display(Audio(data=audio, rate=24000, autoplay=i==0))\n    sf.write(f'{i}.wav', audio, 24000)\n```\nUnder the hood, `kokoro` uses [`misaki`](https://pypi.org/project/misaki/), a G2P library at https://github.com/hexgrad/misaki",
      "baseModel": "yl4579/StyleTTS2-LJSpeech",
      "contentUrl": "https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/kokoro-v1_0.pth",
      "url": "https://huggingface.co/hexgrad/Kokoro-82M",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "**Kokoro** is an open-weight TTS model with 82 million parameters. Despite its lightweight architecture, it delivers comparable quality to larger models while being significantly faster and more cost-efficient. With Apache-licensed weights, Kokoro can be deployed anywhere from production environments to personal projects.\n\n<audio controls><source src=\"https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/samples/HEARME.wav\" type=\"audio/wav\"></audio>\n\n\ud83d\udc08 **GitHub**: https://github.com/hexgrad/kokoro\n\n\ud83d\ude80 **Demo**: https://hf.co/spaces/hexgrad/Kokoro-TTS\n\n> [!NOTE]\n> As of April 2025, the market rate of Kokoro served over API is **under $1 per million characters of text input**, or under $0.06 per hour of audio output. (On average, 1000 characters of input is about 1 minute of output.) Sources: [ArtificialAnalysis/Replicate at 65 cents per M chars](https://artificialanalysis.ai/text-to-speech/model-family/kokoro#price) and [DeepInfra at 80 cents per M chars](https://deepinfra.com/hexgrad/Kokoro-82M).\n>\n> This is an Apache-licensed model, and Kokoro has been deployed in numerous projects and commercial APIs. We welcome the deployment of the model in real use cases.\n\n> [!CAUTION]\n> Fake websites like kokorottsai_com (snapshot: https://archive.ph/nRRnk) and kokorotts_net (snapshot: https://archive.ph/60opa) are likely scams masquerading under the banner of a popular model.\n>\n> Any website containing \"kokoro\" in its root domain (e.g. kokorottsai_com, kokorotts_net) is **NOT owned by and NOT affiliated with this model page or its author**, and attempts to imply otherwise are red flags.\n\n- [Releases](#releases)\n- [Usage](#usage)\n- [EVAL.md](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/EVAL.md) \u2197\ufe0f\n- [SAMPLES.md](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md) \u2197\ufe0f\n- [VOICES.md](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md) \u2197\ufe0f\n- [Model Facts](#model-facts)\n- [Training Details](#training-details)\n- [Creative Commons Attribution](#creative-commons-attribution)\n- [Acknowledgements](#acknowledgements)\n\n### Releases\n\n| Model | Published | Training Data | Langs & Voices | SHA256 |\n| ----- | --------- | ------------- | -------------- | ------ |\n| **v1.0** | **2025 Jan 27** | **Few hundred hrs** | [**8 & 54**](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md) | `496dba11` |\n| [v0.19](https://huggingface.co/hexgrad/kLegacy/tree/main/v0.19) | 2024 Dec 25 | <100 hrs | 1 & 10 | `3b0c392f` |\n\n| Training Costs | v0.19 | v1.0 | **Total** |\n| -------------- | ----- | ---- | ----- |\n| in A100 80GB GPU hours | 500 | 500 | **1000** |\n| average hourly rate | $0.80/h | $1.20/h | **$1/h** |\n| in USD | $400 | $600 | **$1000** |\n\n### Usage\nYou can run this basic cell on [Google Colab](https://colab.research.google.com/). [Listen to samples](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md). For more languages and details, see [Advanced Usage](https://github.com/hexgrad/kokoro?tab=readme-ov-file#advanced-usage).\n```py\n!pip install -q kokoro>=0.9.2 soundfile\n!apt-get -qq -y install espeak-ng > /dev/null 2>&1\nfrom kokoro import KPipeline\nfrom IPython.display import display, Audio\nimport soundfile as sf\nimport torch\npipeline = KPipeline(lang_code='a')\ntext = '''\n[Kokoro](/k\u02c8Ok\u0259\u0279O/) is an open-weight TTS model with 82 million parameters. Despite its lightweight architecture, it delivers comparable quality to larger models while being significantly faster and more cost-efficient. With Apache-licensed weights, [Kokoro](/k\u02c8Ok\u0259\u0279O/) can be deployed anywhere from production environments to personal projects.\n'''\ngenerator = pipeline(text, voice='af_heart')\nfor i, (gs, ps, audio) in enumerate(generator):\n    print(i, gs, ps)\n    display(Audio(data=audio, rate=24000, autoplay=i==0))\n    sf.write(f'{i}.wav', audio, 24000)\n```\nUnder the hood, `kokoro` uses [`misaki`](https://pypi.org/project/misaki/), a G2P library at https://github.com/hexgrad/misaki\n\n### Model Facts\n\n**Architecture:**\n- StyleTTS 2: https://arxiv.org/abs/2306.07691\n- ISTFTNet: https://arxiv.org/abs/2203.02395\n- Decoder only: no diffusion, no encoder release\n\n**Architected by:** Li et al @ https://github.com/yl4579/StyleTTS2\n\n**Trained by**: `@rzvzn` on Discord\n\n**Languages:** Multiple\n\n**Model SHA256 Hash:** `496dba118d1a58f5f3db2efc88dbdc216e0483fc89fe6e47ee1f2c53f18ad1e4`\n\n### Training Details\n\n**Data:** Kokoro was trained exclusively on **permissive/non-copyrighted audio data** and IPA phoneme labels. Examples of permissive/non-copyrighted audio include:\n- Public domain audio\n- Audio licensed under Apache, MIT, etc\n- Synthetic audio<sup>[1]</sup> generated by closed<sup>[2]</sup> TTS models from large providers<br/>\n[1] https://copyright.gov/ai/ai_policy_guidance.pdf<br/>\n[2] No synthetic audio from open TTS models or \"custom voice clones\"\n\n**Total Dataset Size:** A few hundred hours of audio\n\n**Total Training Cost:** About $1000 for 1000 hours of A100 80GB vRAM\n\n### Creative Commons Attribution\n\nThe following CC BY audio was part of the dataset used to train Kokoro v1.0.\n\n| Audio Data | Duration Used | License | Added to Training Set After |\n| ---------- | ------------- | ------- | --------------------------- |\n| [Koniwa](https://github.com/koniwa/koniwa) `tnc` | <1h | [CC BY 3.0](https://creativecommons.org/licenses/by/3.0/deed.ja) | v0.19 / 22 Nov 2024 |\n| [SIWIS](https://datashare.ed.ac.uk/handle/10283/2353) | <11h | [CC BY 4.0](https://datashare.ed.ac.uk/bitstream/handle/10283/2353/license_text) | v0.19 / 22 Nov 2024 |\n\n### Acknowledgements\n\n- \ud83d\udee0\ufe0f [@yl4579](https://huggingface.co/yl4579) for architecting StyleTTS 2.\n- \ud83c\udfc6 [@Pendrokar](https://huggingface.co/Pendrokar) for adding Kokoro as a contender in the TTS Spaces Arena.\n- \ud83d\udcca Thank you to everyone who contributed synthetic training data.\n- \u2764\ufe0f Special thanks to all compute sponsors.\n- \ud83d\udc7e Discord server: https://discord.gg/QuGxSWBfQy\n- \ud83e\udebd Kokoro is a Japanese word that translates to \"heart\" or \"spirit\". It is also the name of an [AI in the Terminator franchise](https://terminator.fandom.com/wiki/Kokoro).\n\n<img src=\"https://static0.gamerantimages.com/wordpress/wp-content/uploads/2024/08/terminator-zero-41-1.jpg\" width=\"400\" alt=\"kokoro\" />\n"
    },
    {
      "@id": "ark:59852/model-hustvl-vitmatte-small-composition-1k-314zxtowtad",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "hustvl/vitmatte-small-composition-1k",
      "description": "ViTMatte model trained on Composition-1k. It was introduced in the paper [ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers](https://arxiv.org/abs/2305.15272) by Yao et al. and first released in [this repository](https://github.com/hustvl/ViTMatte).",
      "author": "hustvl",
      "keywords": [
        "transformers",
        "pytorch",
        "safetensors",
        "vitmatte",
        "vision",
        "arxiv:2305.15272",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "intendedUseCase": "You can use the raw model for image matting. See the [model hub](https://huggingface.co/models?search=vitmatte) to look for other\nfine-tuned versions that may interest you.\n\n### How to use\n\nWe refer to the [docs](https://huggingface.co/docs/transformers/main/en/model_doc/vitmatte#transformers.VitMatteForImageMatting.forward.example).\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{yao2023vitmatte,\n      title={ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers}, \n      author={Jingfeng Yao and Xinggang Wang and Shusheng Yang and Baoyuan Wang},\n      year={2023},\n      eprint={2305.15272},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```",
      "usageInformation": "We refer to the [docs](https://huggingface.co/docs/transformers/main/en/model_doc/vitmatte#transformers.VitMatteForImageMatting.forward.example).",
      "contentUrl": "https://huggingface.co/hustvl/vitmatte-small-composition-1k/resolve/main/model.safetensors",
      "url": "https://huggingface.co/hustvl/vitmatte-small-composition-1k",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# ViTMatte model\n\nViTMatte model trained on Composition-1k. It was introduced in the paper [ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers](https://arxiv.org/abs/2305.15272) by Yao et al. and first released in [this repository](https://github.com/hustvl/ViTMatte). \n\nDisclaimer: The team releasing ViTMatte did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nViTMatte is a simple approach to image matting, the task of accurately estimating the foreground object in an image. The model consists of a Vision Transformer (ViT) with a lightweight head on top.\n\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vitmatte_architecture.png\"\nalt=\"drawing\" width=\"600\"/>\n\n<small> ViTMatte high-level overview. Taken from the <a href=\"https://arxiv.org/abs/2305.15272\">original paper.</a> </small>\n\n## Intended uses & limitations\n\nYou can use the raw model for image matting. See the [model hub](https://huggingface.co/models?search=vitmatte) to look for other\nfine-tuned versions that may interest you.\n\n### How to use\n\nWe refer to the [docs](https://huggingface.co/docs/transformers/main/en/model_doc/vitmatte#transformers.VitMatteForImageMatting.forward.example).\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{yao2023vitmatte,\n      title={ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers}, \n      author={Jingfeng Yao and Xinggang Wang and Shusheng Yang and Baoyuan Wang},\n      year={2023},\n      eprint={2305.15272},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```"
    },
    {
      "@id": "ark:59852/model-amazon-chronos-2-s6vypmb8o1g",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "amazon/chronos-2",
      "description": "**Chronos-2** is a 120M-parameter, encoder-only time series foundation model for zero-shot forecasting.",
      "author": "amazon",
      "keywords": [
        "chronos-forecasting",
        "safetensors",
        "t5",
        "time series",
        "forecasting",
        "foundation models",
        "pretrained models",
        "time-series-forecasting",
        "dataset:autogluon/chronos_datasets",
        "dataset:Salesforce/GiftEvalPretrain",
        "arxiv:2403.07815",
        "arxiv:2510.15821",
        "license:apache-2.0",
        "region:us"
      ],
      "version": "1.0",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/autogluon/chronos_datasets"
        },
        {
          "@id": "https://huggingface.co/datasets/Salesforce/GiftEvalPretrain"
        }
      ],
      "usageInformation": "For experimentation and local inference, you can use the [inference package](https://github.com/amazon-science/chronos-forecasting).\n\nInstall the package\n```\npip install \"chronos-forecasting>=2.0\"\n```\n\nMake zero-shot predictions using the `pandas` API\n\n```python\nimport pandas as pd  # requires: pip install 'pandas[pyarrow]'\nfrom chronos import Chronos2Pipeline\n\npipeline = Chronos2Pipeline.from_pretrained(\"amazon/chronos-2\", device_map=\"cuda\")",
      "contentUrl": "https://huggingface.co/amazon/chronos-2/resolve/main/model.safetensors",
      "url": "https://huggingface.co/amazon/chronos-2",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Chronos-2\n**Chronos-2** is a 120M-parameter, encoder-only time series foundation model for zero-shot forecasting.\nIt supports **univariate**, **multivariate**, and **covariate-informed** tasks within a single architecture.\nInspired by the T5 encoder, Chronos-2 produces multi-step-ahead quantile forecasts and uses a group attention mechanism for efficient in-context learning across related series and covariates.\nTrained on a combination of real-world and large-scale synthetic datasets, it achieves **state-of-the-art zero-shot accuracy** among public models on [**fev-bench**](https://huggingface.co/spaces/autogluon/fev-leaderboard), [**GIFT-Eval**](https://huggingface.co/spaces/Salesforce/GIFT-Eval), and [**Chronos Benchmark II**](https://arxiv.org/abs/2403.07815).\nChronos-2 is also **highly efficient**, delivering over 300 time series forecasts per second on a single A10G GPU and supporting both **GPU and CPU inference**.\n\n## Links\n- \ud83d\ude80 [Deploy Chronos-2 on Amazon SageMaker](https://github.com/amazon-science/chronos-forecasting/blob/main/notebooks/deploy-chronos-to-amazon-sagemaker.ipynb)\n- \ud83d\udcc4 [Technical report](https://arxiv.org/abs/2510.15821v1)\n- \ud83d\udcbb [GitHub](https://github.com/amazon-science/chronos-forecasting)\n- \ud83d\udcd8 [Example notebook](https://github.com/amazon-science/chronos-forecasting/blob/main/notebooks/chronos-2-quickstart.ipynb)\n- \ud83d\udcf0 [Amazon Science Blog](https://www.amazon.science/blog/introducing-chronos-2-from-univariate-to-universal-forecasting)\n\n\n## Overview\n\n| Capability | Chronos-2 | Chronos-Bolt | Chronos |\n|------------|-----------|--------------|----------|\n| Univariate Forecasting | \u2705 | \u2705 | \u2705 |\n| Cross-learning across items | \u2705 | \u274c | \u274c |\n| Multivariate Forecasting | \u2705 | \u274c | \u274c |\n| Past-only (real/categorical) covariates | \u2705 | \u274c | \u274c |\n| Known future (real/categorical) covariates | \u2705 | \ud83e\udde9 | \ud83e\udde9 |\n| Max. Context Length | 8192 | 2048 | 512 |\n| Max. Prediction Length | 1024 | 64 | 64 |\n\n\ud83e\udde9 Chronos & Chronos-Bolt do not natively support future covariates, but they can be combined with external covariate regressors (see [AutoGluon tutorial](https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-chronos.html#incorporating-the-covariates)). This only models per-timestep effects, not effects across time. In contrast, Chronos-2 supports all covariate types natively.\n\n\n## Usage\n\n### Local usage\n\nFor experimentation and local inference, you can use the [inference package](https://github.com/amazon-science/chronos-forecasting).\n\nInstall the package\n```\npip install \"chronos-forecasting>=2.0\"\n```\n\nMake zero-shot predictions using the `pandas` API\n\n```python\nimport pandas as pd  # requires: pip install 'pandas[pyarrow]'\nfrom chronos import Chronos2Pipeline\n\npipeline = Chronos2Pipeline.from_pretrained(\"amazon/chronos-2\", device_map=\"cuda\")\n\n# Load historical target values and past values of covariates\ncontext_df = pd.read_parquet(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/electricity_price/train.parquet\")\n\n# (Optional) Load future values of covariates\ntest_df = pd.read_parquet(\"https://autogluon.s3.amazonaws.com/datasets/timeseries/electricity_price/test.parquet\")\nfuture_df = test_df.drop(columns=\"target\")\n\n# Generate predictions with covariates\npred_df = pipeline.predict_df(\n    context_df,\n    future_df=future_df,\n    prediction_length=24,  # Number of steps to forecast\n    quantile_levels=[0.1, 0.5, 0.9],  # Quantiles for probabilistic forecast\n    id_column=\"id\",  # Column identifying different time series\n    timestamp_column=\"timestamp\",  # Column with datetime information\n    target=\"target\",  # Column(s) with time series values to predict\n)\n```\n\n### Deploying a Chronos-2 endpoint to SageMaker\n\nFor production use, we recommend deploying Chronos-2 endpoints to Amazon SageMaker.\n\nFirst, update the SageMaker SDK to make sure that all the latest models are available.\n\n```\npip install -U sagemaker\n```\n\nDeploy an inference endpoint to SageMaker.\n\n```python\nfrom sagemaker.jumpstart.model import JumpStartModel\n\nmodel = JumpStartModel(\n    model_id=\"pytorch-forecasting-chronos-2\",\n    instance_type=\"ml.g5.2xlarge\",\n)\npredictor = model.deploy()\n```\n\nNow you can send time series data to the endpoint in JSON format.\n\n```python\nimport pandas as pd\ndf = pd.read_csv(\"https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\")\n\npayload = {\n    \"inputs\": [\n        {\"target\": df[\"#Passengers\"].tolist()}\n    ],\n    \"parameters\": {\n        \"prediction_length\": 12,\n    }\n}\nforecast = predictor.predict(payload)[\"predictions\"]\n```\n\nFor more details about the endpoint API, check out the [example notebook](https://github.com/amazon-science/chronos-forecasting/blob/main/notebooks/deploy-chronos-to-amazon-sagemaker.ipynb)\n\n\n## Training data\nMore details about the training data are available in the [technical report](https://arxiv.org/abs/2510.15821).\n\n- Subset of [Chronos Datasets](https://huggingface.co/datasets/autogluon/chronos_datasets) (excluding test portion of datasets that overlap with GIFT-Eval)\n- Subset of [GIFT-Eval Pretrain](https://huggingface.co/datasets/Salesforce/GiftEvalPretrain)\n- Synthetic univariate and multivariate data\n\n\n## Citation\n\nIf you find Chronos-2 useful for your research, please consider citing the associated paper:\n\n```\n@article{ansari2025chronos2,\n  title        = {Chronos-2: From Univariate to Universal Forecasting},\n  author       = {Abdul Fatir Ansari and Oleksandr Shchur and Jaris K\u00fcken and Andreas Auer and Boran Han and Pedro Mercado and Syama Sundar Rangapuram and Huibin Shen and Lorenzo Stella and Xiyuan Zhang and Mononito Goswami and Shubham Kapoor and Danielle C. Maddix and Pablo Guerron and Tony Hu and Junming Yin and Nick Erickson and Prateek Mutalik Desai and Hao Wang and Huzefa Rangwala and George Karypis and Yuyang Wang and Michael Bohlke-Schneider},\n  year         = {2025},\n  url          = {https://arxiv.org/abs/2510.15821}\n}\n```\n"
    },
    {
      "@id": "ark:59852/model-baai-bge-small-en-v1-5-jflnu55wd1",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "BAAI/bge-small-en-v1.5",
      "description": "<h1 align=\"center\">FlagEmbedding</h1>",
      "author": "BAAI",
      "keywords": [
        "sentence-transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "bert",
        "feature-extraction",
        "sentence-similarity",
        "transformers",
        "mteb",
        "en",
        "arxiv:2401.03462",
        "arxiv:2312.15503",
        "arxiv:2311.13534",
        "arxiv:2310.07554",
        "arxiv:2309.07597",
        "license:mit",
        "model-index",
        "autotrain_compatible",
        "text-embeddings-inference",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "usageInformation": "Here are some examples for using `bge` models with \n[FlagEmbedding](#using-flagembedding), [Sentence-Transformers](#using-sentence-transformers), [Langchain](#using-langchain), or [Huggingface Transformers](#using-huggingface-transformers).\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\nIf it doesn't work for you, you can see [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md) for more methods to install FlagEmbedding.\n\n```python\nfrom FlagEmbedding import FlagModel\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = FlagModel('BAAI/bge-large-zh-v1.5', \n                  query_instruction_for_retrieval=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\",\n                  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)",
      "contentUrl": "https://huggingface.co/BAAI/bge-small-en-v1.5/resolve/main/model.safetensors",
      "url": "https://huggingface.co/BAAI/bge-small-en-v1.5",
      "license": "mit",
      "isPartOf": [],
      "README": "\n\n<h1 align=\"center\">FlagEmbedding</h1>\n\n\n<h4 align=\"center\">\n    <p>\n        <a href=#model-list>Model List</a> | \n        <a href=#frequently-asked-questions>FAQ</a> |\n        <a href=#usage>Usage</a>  |\n        <a href=\"#evaluation\">Evaluation</a> |\n        <a href=\"#train\">Train</a> |\n        <a href=\"#contact\">Contact</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\nMore details please refer to our Github: [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding).\n\nIf you are looking for a model that supports more languages, longer texts, and other retrieval methods, you can try using [bge-m3](https://huggingface.co/BAAI/bge-m3).\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\nFlagEmbedding focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n- **Long-Context LLM**: [Activation Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon)\n- **Fine-tuning of LM** : [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail)\n- **Dense Retrieval**: [BGE-M3](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3), [LLM Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), [BGE Embedding](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/baai_general_embedding)\n- **Reranker Model**: [BGE Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n- **Benchmark**: [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)\n\n## News \n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/BGE_M3/BGE_M3.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) :fire:\n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) :fire:\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) :fire:  \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n \n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n    \n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n</details>\n\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n|              Model              | Language | | Description | query instruction for retrieval [1] |\n|:-------------------------------|:--------:| :--------:| :--------:|:--------:|\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     |    [Inference](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3#usage) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3)    | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n|  [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)  |   English | [Inference](./FlagEmbedding/llm_embedder/README.md) [Fine-tune](./FlagEmbedding/llm_embedder/README.md) | a unified embedding model to support diverse retrieval augmentation needs for LLMs | See [README](./FlagEmbedding/llm_embedder/README.md) |\n|  [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)  |   Chinese and English | [Inference](#usage-for-reranker) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) | a cross-encoder model which is more accurate but less efficient [2] |   |\n|  [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) |   Chinese and English | [Inference](#usage-for-reranker) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) | a cross-encoder model which is more accurate but less efficient [2] |   |\n|  [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution  | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5) |   Chinese |  [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | version 1.5 with more reasonable similarity distribution | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | :trophy: rank **1st** in [MTEB](https://huggingface.co/spaces/mteb/leaderboard) leaderboard | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a base-scale model but with similar ability to `bge-large-en` | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en) |   English | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) |a small-scale model but with competitive performance  | `Represent this sentence for searching relevant passages: `  |\n|  [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | :trophy: rank **1st** in [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB) benchmark | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) |   Chinese |  [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a base-scale model but with similar ability to `bge-large-zh` | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n|  [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) |   Chinese | [Inference](#usage-for-embedding-model) [Fine-tune](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) | a small-scale model but with competitive performance | `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`  |\n\n[1\\]: If you need to search the relevant passages to a query, we suggest to add the instruction to the query; in other cases, no instruction is needed, just use the original query directly. In all cases, **no instruction** needs to be added to passages.\n\n[2\\]: Different from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. To balance the accuracy and time cost, cross-encoder is widely used to re-rank top-k documents retrieved by other simple models. \nFor examples, use bge embedding model to retrieve top 100 relevant documents, and then use bge reranker to re-rank the top 100 document to get the final top-3 results.\n\nAll models have been uploaded to Huggingface Hub, and you can see them at https://huggingface.co/BAAI. \nIf you cannot open the Huggingface Hub, you also can download the models at https://model.baai.ac.cn/models .\n\n\n## Frequently asked questions\n\n<details>\n  <summary>1. How to fine-tune bge embedding model?</summary>\n\n  <!-- ### How to fine-tune bge embedding model? -->\nFollowing this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune) to prepare data and fine-tune your model. \nSome suggestions:\n- Mine hard negatives following this [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune#hard-negatives), which can improve the retrieval performance.\n- If you pre-train bge on your data, the pre-trained model cannot be directly used to calculate similarity, and it must be fine-tuned with contrastive learning before computing similarity.\n- If the accuracy of the fine-tuned model is still not high, it is recommended to use/fine-tune the cross-encoder model (bge-reranker) to re-rank top-k results. Hard negatives also are needed to fine-tune reranker.\n\n  \n</details>\n\n<details>\n  <summary>2. The similarity score between two dissimilar sentences is higher than 0.5</summary>\n\n  <!-- ### The similarity score between two dissimilar sentences is higher than 0.5 -->\n**Suggest to use bge v1.5, which alleviates the issue of the similarity distribution.** \n\nSince we finetune the models by contrastive learning with a temperature of 0.01, \nthe similarity distribution of the current BGE model is about in the interval \\[0.6, 1\\].\nSo a similarity score greater than 0.5 does not indicate that the two sentences are similar.\n\nFor downstream tasks, such as passage retrieval or semantic similarity, \n**what matters is the relative order of the scores, not the absolute value.**\nIf you need to filter similar sentences based on a similarity threshold, \nplease select an appropriate similarity threshold based on the similarity distribution on your data (such as 0.8, 0.85, or even 0.9).\n\n</details>\n\n<details>\n  <summary>3. When does the query instruction need to be used</summary>\n\n  <!-- ### When does the query instruction need to be used -->\n\nFor the `bge-*-v1.5`, we improve its retrieval ability when not using instruction. \nNo instruction only has a slight degradation in retrieval performance compared with using instruction. \nSo you can generate embedding without instruction in all cases for convenience.\n \nFor a retrieval task that uses short queries to find long related documents, \nit is recommended to add instructions for these short queries.\n**The best method to decide whether to add instructions for queries is choosing the setting that achieves better performance on your task.**\nIn all cases, the documents/passages do not need to add the instruction. \n\n</details>\n\n\n## Usage \n\n### Usage for Embedding Model\n\nHere are some examples for using `bge` models with \n[FlagEmbedding](#using-flagembedding), [Sentence-Transformers](#using-sentence-transformers), [Langchain](#using-langchain), or [Huggingface Transformers](#using-huggingface-transformers).\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\nIf it doesn't work for you, you can see [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md) for more methods to install FlagEmbedding.\n\n```python\nfrom FlagEmbedding import FlagModel\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = FlagModel('BAAI/bge-large-zh-v1.5', \n                  query_instruction_for_retrieval=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\",\n                  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n\n# for s2p(short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\nqueries = ['query_1', 'query_2']\npassages = [\"\u6837\u4f8b\u6587\u6863-1\", \"\u6837\u4f8b\u6587\u6863-2\"]\nq_embeddings = model.encode_queries(queries)\np_embeddings = model.encode(passages)\nscores = q_embeddings @ p_embeddings.T\n```\nFor the value of the argument `query_instruction_for_retrieval`, see [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list). \n\nBy default, FlagModel will use all available GPUs when encoding. Please set `os.environ[\"CUDA_VISIBLE_DEVICES\"]` to select specific GPUs.\nYou also can set `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"` to make all GPUs unavailable.\n\n\n#### Using Sentence-Transformers\n\nYou can also use the `bge` models with [sentence-transformers](https://www.SBERT.net):\n\n```\npip install -U sentence-transformers\n```\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences_1 = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\nsentences_2 = [\"\u6837\u4f8b\u6570\u636e-3\", \"\u6837\u4f8b\u6570\u636e-4\"]\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nembeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\nembeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\nFor s2p(short query to long passage) retrieval task, \neach short query should start with an instruction (instructions see [Model List](https://github.com/FlagOpen/FlagEmbedding/tree/master#model-list)). \nBut the instruction is not needed for passages.\n```python\nfrom sentence_transformers import SentenceTransformer\nqueries = ['query_1', 'query_2']\npassages = [\"\u6837\u4f8b\u6587\u6863-1\", \"\u6837\u4f8b\u6587\u6863-2\"]\ninstruction = \"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n\nmodel = SentenceTransformer('BAAI/bge-large-zh-v1.5')\nq_embeddings = model.encode([instruction+q for q in queries], normalize_embeddings=True)\np_embeddings = model.encode(passages, normalize_embeddings=True)\nscores = q_embeddings @ p_embeddings.T\n```\n\n#### Using Langchain \n\nYou can use `bge` in langchain like this:\n```python\nfrom langchain.embeddings import HuggingFaceBgeEmbeddings\nmodel_name = \"BAAI/bge-large-en-v1.5\"\nmodel_kwargs = {'device': 'cuda'}\nencode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\nmodel = HuggingFaceBgeEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs,\n    query_instruction=\"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n)\nmodel.query_instruction = \"\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a\"\n```\n\n\n#### Using HuggingFace Transformers\n\nWith the transformers package, you can use the model like this: First, you pass your input through the transformer model, then you select the last hidden state of the first token (i.e., [CLS]) as the sentence embedding.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n# Sentences we want sentence embeddings for\nsentences = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-large-zh-v1.5')\nmodel.eval()\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n    # Perform pooling. In this case, cls pooling.\n    sentence_embeddings = model_output[0][:, 0]\n# normalize embeddings\nsentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\nprint(\"Sentence embeddings:\", sentence_embeddings)\n```\n\n### Usage for Reranker\n\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. \nYou can get a relevance score by inputting query and passage to the reranker. \nThe reranker is optimized based cross-entropy loss, so the relevance score is not bounded to a specific range.\n\n\n#### Using FlagEmbedding\n```\npip install -U FlagEmbedding\n```\n\nGet relevance scores (higher scores indicate more relevance):\n```python\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nscore = reranker.compute_score(['query', 'passage'])\nprint(score)\n\nscores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])\nprint(scores)\n```\n\n\n#### Using Huggingface transformers\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\nmodel = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\nmodel.eval()\n\npairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n```\n\n#### Usage of the ONNX files\n\n```python\nfrom optimum.onnxruntime import ORTModelForFeatureExtraction  # type: ignore\n\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('BAAI/bge-small-en-v1.5')\nmodel = AutoModel.from_pretrained('BAAI/bge-small-en-v1.5')\nmodel_ort = ORTModelForFeatureExtraction.from_pretrained('BAAI/bge-small-en-v1.5', file_name=\"onnx/model.onnx\")\n\n# Sentences we want sentence embeddings for\nsentences = [\"\u6837\u4f8b\u6570\u636e-1\", \"\u6837\u4f8b\u6570\u636e-2\"]\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n# for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n# encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n\nmodel_output_ort = model_ort(**encoded_input)\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# model_output and model_output_ort are identical\n\n```\n\n#### Usage via infinity\nIts also possible to deploy the onnx files with the [infinity_emb](https://github.com/michaelfeil/infinity) pip package.\nRecommended is `device=\"cuda\", engine=\"torch\"` with flash attention on gpu, and `device=\"cpu\", engine=\"optimum\"` for onnx inference.\n\n```python\nimport asyncio\nfrom infinity_emb import AsyncEmbeddingEngine, EngineArgs\n\nsentences = [\"Embed this is sentence via Infinity.\", \"Paris is in France.\"]\nengine = AsyncEmbeddingEngine.from_args(\n    EngineArgs(model_name_or_path = \"BAAI/bge-small-en-v1.5\", device=\"cpu\", engine=\"optimum\" # or engine=\"torch\"\n))\n\nasync def main(): \n    async with engine:\n        embeddings, usage = await engine.embed(sentences=sentences)\nasyncio.run(main())\n```\n\n\n## Evaluation  \n\n`baai-general-embedding` models achieve **state-of-the-art performance on both MTEB and C-MTEB leaderboard!**\nFor more details and evaluation tools see our [scripts](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md). \n\n- **MTEB**:   \n\n| Model Name |  Dimension | Sequence Length | Average (56) | Retrieval (15) |Clustering (11) | Pair Classification (3) | Reranking (4) |  STS (10) | Summarization (1) | Classification (12) |\n|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5) | 1024 | 512 |  **64.23** | **54.29** |  46.08 | 87.12 | 60.03 | 83.11 | 31.61 | 75.97 |  \n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) |  768 | 512 | 63.55 | 53.25 |   45.77 | 86.55 | 58.86 | 82.4 | 31.07 | 75.53 |  \n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) |  384 | 512 | 62.17 |51.68 | 43.82 |  84.92 | 58.36 | 81.59 | 30.12 | 74.14 |  \n| [bge-large-en](https://huggingface.co/BAAI/bge-large-en) |  1024 | 512 | 63.98 |  53.9 | 46.98 | 85.8 | 59.48 | 81.56 | 32.06 | 76.21 | \n| [bge-base-en](https://huggingface.co/BAAI/bge-base-en) |  768 | 512 |  63.36 | 53.0 | 46.32 | 85.86 | 58.7 | 81.84 | 29.27 | 75.27 | \n| [gte-large](https://huggingface.co/thenlper/gte-large) |  1024 | 512 | 63.13 | 52.22 | 46.84 | 85.00 | 59.13 | 83.35 | 31.66 | 73.33 |\n| [gte-base](https://huggingface.co/thenlper/gte-base) \t|  768 | 512 | 62.39 | 51.14 | 46.2 | 84.57 | 58.61 | 82.3 | 31.17 | 73.01 |\n| [e5-large-v2](https://huggingface.co/intfloat/e5-large-v2) |  1024| 512 | 62.25 | 50.56 | 44.49 | 86.03 | 56.61 | 82.05 | 30.19 | 75.24 |\n| [bge-small-en](https://huggingface.co/BAAI/bge-small-en) |  384 | 512 | 62.11 |  51.82 | 44.31 | 83.78 | 57.97 | 80.72 | 30.53 | 74.37 |  \n| [instructor-xl](https://huggingface.co/hkunlp/instructor-xl) |  768 | 512 | 61.79 | 49.26 | 44.74 | 86.62 | 57.29 | 83.06 | 32.32 | 61.79 |\n| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2) |  768 | 512 | 61.5 | 50.29 | 43.80 | 85.73 | 55.91 | 81.05 | 30.28 | 73.84 |\n| [gte-small](https://huggingface.co/thenlper/gte-small) |  384 | 512 | 61.36 | 49.46 | 44.89 | 83.54 | 57.7 | 82.07 | 30.42 | 72.31 |\n| [text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings) | 1536 | 8192 | 60.99 | 49.25 | 45.9 | 84.89 | 56.32 | 80.97 | 30.8 | 70.93 |\n| [e5-small-v2](https://huggingface.co/intfloat/e5-base-v2) | 384 | 512 | 59.93 | 49.04 | 39.92 | 84.67 | 54.32 | 80.39 | 31.16 | 72.94 |\n| [sentence-t5-xxl](https://huggingface.co/sentence-transformers/sentence-t5-xxl) |  768 | 512 | 59.51 | 42.24 | 43.72 | 85.06 | 56.42 | 82.63 | 30.08 | 73.42 |\n| [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) \t|  768 | 514 \t| 57.78 | 43.81 | 43.69 | 83.04 | 59.36 | 80.28 | 27.49 | 65.07 |\n| [sgpt-bloom-7b1-msmarco](https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco) \t|  4096 | 2048 | 57.59 | 48.22 | 38.93 | 81.9 | 55.65 | 77.74 | 33.6 | 66.19 |\n\n\n\n- **C-MTEB**:  \nWe create the benchmark C-MTEB for Chinese text embedding which consists of 31 datasets from 6 tasks. \nPlease refer to [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/README.md) for a detailed introduction.\n \n| Model | Embedding dimension | Avg | Retrieval | STS | PairClassification | Classification | Reranking | Clustering |\n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n| [**BAAI/bge-large-zh-v1.5**](https://huggingface.co/BAAI/bge-large-zh-v1.5) | 1024 |  **64.53** | 70.46 | 56.25 | 81.6 | 69.13 | 65.84 | 48.99 |  \n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5) | 768 |  63.13 | 69.49 | 53.72 | 79.75 | 68.07 | 65.39 | 47.53 |  \n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5) | 512 | 57.82 | 61.77 | 49.11 | 70.41 | 63.96 | 60.92 | 44.18 |   \n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) | 1024 | 64.20 | 71.53 | 54.98 | 78.94 | 68.32 | 65.11 | 48.39 |\n| [bge-large-zh-noinstruct](https://huggingface.co/BAAI/bge-large-zh-noinstruct) | 1024 | 63.53 | 70.55 | 53 | 76.77 | 68.58 | 64.91 | 50.01 |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) | 768 | 62.96 | 69.53 | 54.12 | 77.5 | 67.07 | 64.91 | 47.63 |\n| [multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large) | 1024 | 58.79 | 63.66 | 48.44 | 69.89 | 67.34 | 56.00 | 48.23 |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) | 512 | 58.27 |  63.07 | 49.45 | 70.35 | 63.64 | 61.48 | 45.09 |\n| [m3e-base](https://huggingface.co/moka-ai/m3e-base) | 768 | 57.10 | 56.91 | 50.47 | 63.99 | 67.52 | 59.34 | 47.68 |\n| [m3e-large](https://huggingface.co/moka-ai/m3e-large) | 1024 |  57.05 | 54.75 | 50.42 | 64.3 | 68.2 | 59.66 | 48.88 |\n| [multilingual-e5-base](https://huggingface.co/intfloat/multilingual-e5-base) | 768 | 55.48 | 61.63 | 46.49 | 67.07 | 65.35 | 54.35 | 40.68 |\n| [multilingual-e5-small](https://huggingface.co/intfloat/multilingual-e5-small) | 384 | 55.38 | 59.95 | 45.27 | 66.45 | 65.85 | 53.86 | 45.26 |\n| [text-embedding-ada-002(OpenAI)](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings) | 1536 |  53.02 | 52.0 | 43.35 | 69.56 | 64.31 | 54.28 | 45.68 |\n| [luotuo](https://huggingface.co/silk-road/luotuo-bert-medium) | 1024 | 49.37 |  44.4 | 42.78 | 66.62 | 61 | 49.25 | 44.39 |\n| [text2vec-base](https://huggingface.co/shibing624/text2vec-base-chinese) | 768 |  47.63 | 38.79 | 43.41 | 67.41 | 62.19 | 49.45 | 37.66 |\n| [text2vec-large](https://huggingface.co/GanymedeNil/text2vec-large-chinese) | 1024 | 47.36 | 41.94 | 44.97 | 70.86 | 60.66 | 49.16 | 30.02 |\n\n\n- **Reranking**:\nSee [C_MTEB](https://github.com/FlagOpen/FlagEmbedding/blob/master/C_MTEB/) for evaluation script.\n\n| Model | T2Reranking | T2RerankingZh2En\\* | T2RerankingEn2Zh\\* | MMarcoReranking | CMedQAv1 | CMedQAv2 | Avg |  \n|:-------------------------------|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|  \n| text2vec-base-multilingual | 64.66 | 62.94 | 62.51 | 14.37 | 48.46 | 48.6 | 50.26 |  \n| multilingual-e5-small | 65.62 | 60.94 | 56.41 | 29.91 | 67.26 | 66.54 | 57.78 |  \n| multilingual-e5-large | 64.55 | 61.61 | 54.28 | 28.6 | 67.42 | 67.92 | 57.4 |  \n| multilingual-e5-base | 64.21 | 62.13 | 54.68 | 29.5 | 66.23 | 66.98 | 57.29 |  \n| m3e-base | 66.03 | 62.74 | 56.07 | 17.51 | 77.05 | 76.76 | 59.36 |  \n| m3e-large | 66.13 | 62.72 | 56.1 | 16.46 | 77.76 | 78.27 | 59.57 |  \n| bge-base-zh-v1.5 | 66.49 | 63.25 | 57.02 | 29.74 | 80.47 | 84.88 | 63.64 |  \n| bge-large-zh-v1.5 | 65.74 | 63.39 | 57.03 | 28.74 | 83.45 | 85.44 | 63.97 |  \n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) | 67.28 | 63.95 | 60.45 | 35.46 | 81.26 | 84.1 | 65.42 |  \n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) | 67.6 | 64.03 | 61.44 | 37.16 | 82.15 | 84.18 | 66.09 |  \n\n\\* : T2RerankingZh2En and T2RerankingEn2Zh are cross-language retrieval tasks\n\n## Train\n\n### BAAI Embedding \n\nWe pre-train the models using [retromae](https://github.com/staoxiao/RetroMAE) and train them on large-scale pairs data using contrastive learning. \n**You can fine-tune the embedding model on your data following our [examples](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune).**\nWe also provide a [pre-train example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/pretrain).\nNote that the goal of pre-training is to reconstruct the text, and the pre-trained model cannot be used for similarity calculation directly, it needs to be fine-tuned.\nMore training details for bge see [baai_general_embedding](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/baai_general_embedding/README.md).\n\n\n\n### BGE Reranker\n\nCross-encoder will perform full-attention over the input pair, \nwhich is more accurate than embedding model (i.e., bi-encoder) but more time-consuming than embedding model.\nTherefore, it can be used to re-rank the top-k documents returned by embedding model.\nWe train the cross-encoder on a multilingual pair data, \nThe data format is the same as embedding model, so you can fine-tune it easily following our [example](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker). \nMore details please refer to [./FlagEmbedding/reranker/README.md](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/reranker)\n\n\n## Contact\nIf you have any question or suggestion related to this project, feel free to open an issue or pull request.\nYou also can email Shitao Xiao(stxiao@baai.ac.cn) and Zheng Liu(liuzheng@baai.ac.cn). \n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). The released models can be used for commercial purposes free of charge.\n\n"
    },
    {
      "@id": "ark:59852/model-gensyn-qwen2-5-0-5b-instruct-7jyeqivdgmw",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "Gensyn/Qwen2.5-0.5B-Instruct",
      "description": "This model is intended for use in the [Gensyn RL Swarm](https://www.gensyn.ai/articles/rl-swarm), to finetune locally using peer-to-peer reinforcement learning post-training.",
      "author": "Gensyn",
      "keywords": [
        "transformers",
        "safetensors",
        "qwen2",
        "text-generation",
        "chat",
        "rl-swarm",
        "gensyn",
        "conversational",
        "en",
        "base_model:Qwen/Qwen2.5-0.5B",
        "base_model:finetune:Qwen/Qwen2.5-0.5B",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-generation",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "hasBias": "This repo contains an **unmodified version** of the instruction-tuned 0.5B Qwen2.5 model, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\n- Number of Parameters: 0.49B\n- Number of Paramaters (Non-Embedding): 0.36B\n- Number of Layers: 24\n- Number of Attention Heads (GQA): 14 for Q and 2 for KV\n- Context Length: Full 32,768 tokens and generation 8192 tokens",
      "baseModel": "Qwen/Qwen2.5-0.5B",
      "contentUrl": "https://huggingface.co/Gensyn/Qwen2.5-0.5B-Instruct/resolve/main/model.safetensors",
      "url": "https://huggingface.co/Gensyn/Qwen2.5-0.5B-Instruct",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Qwen2.5-0.5B-Instruct\n\n## Introduction\nThis model is intended for use in the [Gensyn RL Swarm](https://www.gensyn.ai/articles/rl-swarm), to finetune locally using peer-to-peer reinforcement learning post-training.\n\nOnce finetuned, the model can be used as normal in any workflow, for details on how to do this please refer to the [original model documentation](https://qwen.readthedocs.io/en/latest/).\n\nFor more details on the original model, please refer to the original repository [here](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct).\n\nThis repo contains an **unmodified version** of the instruction-tuned 0.5B Qwen2.5 model, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\n- Number of Parameters: 0.49B\n- Number of Paramaters (Non-Embedding): 0.36B\n- Number of Layers: 24\n- Number of Attention Heads (GQA): 14 for Q and 2 for KV\n- Context Length: Full 32,768 tokens and generation 8192 tokens\n\n## Requirements\n\nThis model is intended for use in the [Gensyn RL Swarm](https://www.gensyn.ai/articles/rl-swarm) system, for details on model requirements when using outside of a swarm, refer to the original Qwen repo [here](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct).\n\n## Quickstart\n\nTo deploy this model into a swarm and/or participate in the Gensyn Testnet, follow the instructions in the [RL Swarm repository](https://github.com/gensyn-ai/rl-swarm), read about the [testnet](https://www.gensyn.ai/testnet), read the [RL Swarm overview](https://www.gensyn.ai/articles/rl-swarm), and/or read the [RL Swarm technical report](https://github.com/gensyn-ai/paper-rl-swarm/blob/main/latest.pdf).\n"
    },
    {
      "@id": "ark:59852/model-alibaba-nlp-gte-large-en-v1-5-lnuashjixr",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "Alibaba-NLP/gte-large-en-v1.5",
      "description": "<!-- **English** | [\u4e2d\u6587](./README_zh.md) -->",
      "author": "Alibaba-NLP",
      "keywords": [
        "transformers",
        "onnx",
        "safetensors",
        "new",
        "feature-extraction",
        "sentence-transformers",
        "gte",
        "mteb",
        "transformers.js",
        "sentence-similarity",
        "custom_code",
        "en",
        "dataset:allenai/c4",
        "arxiv:2407.19669",
        "arxiv:2308.03281",
        "license:apache-2.0",
        "model-index",
        "text-embeddings-inference",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "onnx",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/allenai/c4"
        }
      ],
      "contentUrl": "https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5/resolve/main/model.safetensors",
      "url": "https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n<!-- **English** | [\u4e2d\u6587](./README_zh.md) -->\n\n# gte-large-en-v1.5\n\nWe introduce `gte-v1.5` series, upgraded `gte` embeddings that support the context length of up to **8192**, while further enhancing model performance.\nThe models are built upon the `transformer++` encoder [backbone](https://huggingface.co/Alibaba-NLP/new-impl) (BERT + RoPE + GLU).\n\nThe `gte-v1.5` series achieve state-of-the-art scores on the MTEB benchmark within the same model size category and prodvide competitive on the LoCo long-context retrieval tests (refer to [Evaluation](#evaluation)).\n\nWe also present the [`gte-Qwen1.5-7B-instruct`](https://huggingface.co/Alibaba-NLP/gte-Qwen1.5-7B-instruct),\na SOTA instruction-tuned multi-lingual embedding model that ranked 2nd in MTEB and 1st in C-MTEB.\n\n<!-- Provide a longer summary of what this model is. -->\n\n- **Developed by:** Institute for Intelligent Computing, Alibaba Group\n- **Model type:** Text Embeddings\n- **Paper:** [mGTE: Generalized Long-Context Text Representation and Reranking\nModels for Multilingual Text Retrieval](https://arxiv.org/pdf/2407.19669)\n\n<!-- - **Demo [optional]:** [More Information Needed] -->\n\n### Model list\n\n| Models | Language | Model Size | Max Seq. Length | Dimension | MTEB-en | LoCo |\n|:-----: | :-----: |:-----: |:-----: |:-----: | :-----: | :-----: |\n|[`gte-Qwen1.5-7B-instruct`](https://huggingface.co/Alibaba-NLP/gte-Qwen1.5-7B-instruct)| Multiple | 7720 | 32768 | 4096 | 67.34 | 87.57 |\n|[`gte-large-en-v1.5`](https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5) | English | 434 | 8192 | 1024 | 65.39 | 86.71 |\n|[`gte-base-en-v1.5`](https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5) | English | 137 | 8192 | 768 | 64.11 | 87.44 |\n\n\n## How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n```python\n# Requires transformers>=4.36.0\n\nimport torch.nn.functional as F\nfrom transformers import AutoModel, AutoTokenizer\n\ninput_texts = [\n    \"what is the capital of China?\",\n    \"how to implement quick sort in python?\",\n    \"Beijing\",\n    \"sorting algorithms\"\n]\n\nmodel_path = 'Alibaba-NLP/gte-large-en-v1.5'\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n\n# Tokenize the input texts\nbatch_dict = tokenizer(input_texts, max_length=8192, padding=True, truncation=True, return_tensors='pt')\n\noutputs = model(**batch_dict)\nembeddings = outputs.last_hidden_state[:, 0]\n \n# (Optionally) normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:1] @ embeddings[1:].T) * 100\nprint(scores.tolist())\n```\n\n**It is recommended to install xformers and enable unpadding for acceleration, refer to [enable-unpadding-and-xformers](https://huggingface.co/Alibaba-NLP/new-impl#recommendation-enable-unpadding-and-acceleration-with-xformers).**\n\n\nUse with sentence-transformers:\n\n```python\n# Requires sentence_transformers>=2.7.0\n\nfrom sentence_transformers import SentenceTransformer\nfrom sentence_transformers.util import cos_sim\n\nsentences = ['That is a happy person', 'That is a very happy person']\n\nmodel = SentenceTransformer('Alibaba-NLP/gte-large-en-v1.5', trust_remote_code=True)\nembeddings = model.encode(sentences)\nprint(cos_sim(embeddings[0], embeddings[1]))\n```\n\nUse with `transformers.js`:\n\n```js\n// npm i @xenova/transformers\nimport { pipeline, dot } from '@xenova/transformers';\n\n// Create feature extraction pipeline\nconst extractor = await pipeline('feature-extraction', 'Alibaba-NLP/gte-large-en-v1.5', {\n    quantized: false, // Comment out this line to use the quantized version\n});\n\n// Generate sentence embeddings\nconst sentences = [\n    \"what is the capital of China?\",\n    \"how to implement quick sort in python?\",\n    \"Beijing\",\n    \"sorting algorithms\"\n]\nconst output = await extractor(sentences, { normalize: true, pooling: 'cls' });\n\n// Compute similarity scores\nconst [source_embeddings, ...document_embeddings ] = output.tolist();\nconst similarities = document_embeddings.map(x => 100 * dot(source_embeddings, x));\nconsole.log(similarities); // [41.86354093370361, 77.07076371259589, 37.02981979677899]\n```\n\n## Training Details\n\n### Training Data\n\n- Masked language modeling (MLM): `c4-en`\n- Weak-supervised contrastive pre-training (CPT): [GTE](https://arxiv.org/pdf/2308.03281.pdf) pre-training data\n- Supervised contrastive fine-tuning: [GTE](https://arxiv.org/pdf/2308.03281.pdf) fine-tuning data\n\n### Training Procedure \n\nTo enable the backbone model to support a context length of 8192, we adopted a multi-stage training strategy.\nThe model first undergoes preliminary MLM pre-training on shorter lengths.\nAnd then, we resample the data, reducing the proportion of short texts, and continue the MLM pre-training.\n\nThe entire training process is as follows:\n- MLM-512: lr 2e-4, mlm_probability 0.3, batch_size 4096, num_steps 300000, rope_base 10000\n- MLM-2048: lr 5e-5, mlm_probability 0.3, batch_size 4096, num_steps 30000, rope_base 10000\n- [MLM-8192](https://huggingface.co/Alibaba-NLP/gte-en-mlm-large): lr 5e-5, mlm_probability 0.3, batch_size 1024, num_steps 30000, rope_base 160000\n- CPT: max_len 512, lr 5e-5, batch_size 28672, num_steps 100000\n- Fine-tuning: TODO\n\n\n## Evaluation\n\n\n### MTEB\n\nThe results of other models are retrieved from [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n\nThe gte evaluation setting: `mteb==1.2.0, fp16 auto mix precision, max_length=8192`, and set ntk scaling factor to 2 (equivalent to rope_base * 2).\n\n| Model Name | Param Size (M) | Dimension | Sequence Length | Average (56) | Class. (12) | Clust. (11) | Pair Class. (3) | Reran. (4) | Retr. (15) | STS (10) | Summ. (1) |\n|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [**gte-large-en-v1.5**](https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5) | 409 | 1024 | 8192 | **65.39** | 77.75 | 47.95 | 84.63 | 58.50 | 57.91 | 81.43 | 30.91 |\n| [mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1) | 335 | 1024 | 512 | 64.68 | 75.64 | 46.71 | 87.2 | 60.11 | 54.39 | 85 | 32.71 |\n| [multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct) | 560 | 1024 | 514 | 64.41 | 77.56 | 47.1 | 86.19 | 58.58 | 52.47 | 84.78 | 30.39 |\n| [bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)| 335 | 1024 | 512 | 64.23 | 75.97 | 46.08 | 87.12 | 60.03 | 54.29 | 83.11 | 31.61 |\n| [**gte-base-en-v1.5**](https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5) | 137 | 768 | 8192 | **64.11** | 77.17 | 46.82 | 85.33 | 57.66 | 54.09 | 81.97 | 31.17 |\n| [bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)| 109 | 768 | 512 | 63.55 | 75.53 | 45.77 | 86.55 | 58.86 | 53.25 | 82.4 | 31.07 |\n\n\n### LoCo\n\n| Model Name |  Dimension | Sequence Length | Average (5) | QsmsumRetrieval | SummScreenRetrieval | QasperAbastractRetrieval | QasperTitleRetrieval |  GovReportRetrieval |\n|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [gte-qwen1.5-7b](https://huggingface.co/Alibaba-NLP/gte-qwen1.5-7b) | 4096 | 32768 |  87.57 | 49.37 | 93.10 | 99.67 | 97.54 | 98.21 | \n| [gte-large-v1.5](https://huggingface.co/Alibaba-NLP/gte-large-v1.5) |1024 | 8192 | 86.71 | 44.55 | 92.61 | 99.82 | 97.81 | 98.74 |\n| [gte-base-v1.5](https://huggingface.co/Alibaba-NLP/gte-base-v1.5) | 768 | 8192 | 87.44 | 49.91  | 91.78 | 99.82 | 97.13 | 98.58 |\n\n\n\n## Citation\n\nIf you find our paper or models helpful, please consider citing them as follows:\n\n```\n@article{zhang2024mgte,\n  title={mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval},\n  author={Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Wen and Dai, Ziqi and Tang, Jialong and Lin, Huan and Yang, Baosong and Xie, Pengjun and Huang, Fei and others},\n  journal={arXiv preprint arXiv:2407.19669},\n  year={2024}\n}\n\n@article{li2023towards,\n  title={Towards general text embeddings with multi-stage contrastive learning},\n  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\n  journal={arXiv preprint arXiv:2308.03281},\n  year={2023}\n}\n```"
    },
    {
      "@id": "ark:59852/model-meta-llama-llama-3-2-1b-instruct-jflnu5swde",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "meta-llama/Llama-3.2-1B-Instruct",
      "description": "The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.",
      "author": "meta-llama",
      "keywords": [
        "transformers",
        "safetensors",
        "llama",
        "text-generation",
        "facebook",
        "meta",
        "pytorch",
        "llama-3",
        "conversational",
        "en",
        "de",
        "fr",
        "it",
        "pt",
        "hi",
        "es",
        "th",
        "arxiv:2204.05149",
        "arxiv:2405.16406",
        "license:llama3.2",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-generation",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "hasBias": "As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama\n2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm\n3. Provide protections for the community to help prevent the misuse of our models\n\n### Responsible Deployment\n\n**Approach:** Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta\u2019s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver\u2019s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/).\n\n#### Llama 3.2 Instruct\n\n**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).\n\n**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We\u2019ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n#### Llama 3.2 Systems\n\n**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n\n### New Capabilities and Use Cases\n\n**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.\n\n**Constrained Environments:** Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.\n\n### Evaluations\n\n**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.\n\n**Red Teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n### Critical Risks\n\nIn addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:\n\n**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.\n\n**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the model\u2019s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n\n**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2\u2019s 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.\n\n### Community\n\n**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n\n**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta\u2019s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).\n\n**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.",
      "intendedUseCase": "**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.\n\n**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.",
      "usageInformation": "This repository contains two versions of Llama-3.2-1B-Instruct, for use with transformers and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with `transformers >= 4.43.0` onward, you can run conversational inference using the Transformers `pipeline` abstraction or by leveraging the Auto classes with the `generate()` function.\n\nMake sure to update your transformers installation via `pip install --upgrade transformers`.\n\n```python\nimport torch\nfrom transformers import pipeline\n\nmodel_id = \"meta-llama/Llama-3.2-1B-Instruct\"\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\noutputs = pipe(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nNote: You can also find detailed recipes on how to use the model locally, with `torch.compile()`, assisted generations, quantised and more at [`huggingface-llama-recipes`](https://github.com/huggingface/huggingface-llama-recipes)\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama)\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Llama-3.2-1B-Instruct --include \"original/*\" --local-dir Llama-3.2-1B-Instruct\n```",
      "contentUrl": "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/model.safetensors",
      "url": "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct",
      "license": "llama3.2",
      "isPartOf": [],
      "README": "\n## Model Information\n\nThe Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model Developer:** Meta\n\n**Model Architecture:** Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n|  | Training Data | Params | Input modalities | Output modalities | Context Length | GQA | Shared Embeddings | Token count | Knowledge cutoff |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| Llama 3.2 (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 128k | Yes | Yes | Up to 9T tokens | December 2023 |\n|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code  |  |  |  |  |  |\n| Llama 3.2 Quantized (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 8k | Yes | Yes | Up to 9T tokens | December 2023 |\n|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code |  |  |  |  |  |\n\n**Supported Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\n\n**Llama 3.2 Model Family:** Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date:** Sept 25, 2024\n\n**Status:** This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\n\n**License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).\n\n**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n\n## Intended Use\n\n**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.\n\n**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.\n\n## How to use\n\nThis repository contains two versions of Llama-3.2-1B-Instruct, for use with transformers and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with `transformers >= 4.43.0` onward, you can run conversational inference using the Transformers `pipeline` abstraction or by leveraging the Auto classes with the `generate()` function.\n\nMake sure to update your transformers installation via `pip install --upgrade transformers`.\n\n```python\nimport torch\nfrom transformers import pipeline\n\nmodel_id = \"meta-llama/Llama-3.2-1B-Instruct\"\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n)\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\noutputs = pipe(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nNote: You can also find detailed recipes on how to use the model locally, with `torch.compile()`, assisted generations, quantised and more at [`huggingface-llama-recipes`](https://github.com/huggingface/huggingface-llama-recipes)\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama)\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Llama-3.2-1B-Instruct --include \"original/*\" --local-dir Llama-3.2-1B-Instruct\n```\n\n## Hardware and Software\n\n**Training Factors:** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use:** Training utilized a cumulative of **916k** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\n\n**Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **240** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n|  | Training Time (GPU hours) | Logit Generation Time (GPU Hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n| :---- | :---: | ----- | :---: | :---: | :---: |\n| Llama 3.2 1B | 370k | \\- | 700 | 107 | 0 |\n| Llama 3.2 3B | 460k | \\- | 700 | 133 | 0 |\n| Llama 3.2 1B SpinQuant | 1.7 | 0 | 700 | *Negligible*\\*\\* | 0 |\n| Llama 3.2 3B SpinQuant | 2.4 | 0 | 700 | *Negligible*\\*\\* | 0 |\n| Llama 3.2 1B QLora | 1.3k | 0 | 700 | 0.381 | 0 |\n| Llama 3.2 3B QLora | 1.6k | 0 | 700 | 0.461 | 0 |\n| Total | 833k |         86k |  | 240 | 0 |\n\n\\*\\* The location-based CO2e emissions of Llama 3.2 1B SpinQuant and Llama 3.2 3B SpinQuant are less than 0.001 metric tonnes each. This is due to the minimal training GPU hours that are required.\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149). Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\n\n## Training Data\n\n**Overview:** Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).\n\n**Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n\n## Quantization\n\n### Quantization Scheme\n\nWe designed the current quantization scheme with the [PyTorch\u2019s ExecuTorch](https://github.com/pytorch/executorch) inference framework and Arm CPU backend in mind, taking into account metrics including model quality, prefill/decoding speed, and memory footprint. Our quantization scheme involves three parts:\n- All linear layers in all transformer blocks are quantized to a 4-bit groupwise scheme (with a group size of 32) for weights and 8-bit per-token dynamic quantization for activations.\n- The classification layer is quantized to 8-bit per-channel for weight and 8-bit per token dynamic quantization for activation.\n- Similar to classification layer, an 8-bit per channel quantization is used for embedding layer.\n\n\n### Quantization-Aware Training and LoRA\n\nThe quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).\n\n### SpinQuant\n\n[SpinQuant](https://arxiv.org/abs/2405.16406) was applied, together with generative post-training quantization (GPTQ). For the SpinQuant rotation matrix fine-tuning, we optimized for 100 iterations, using 800 samples with sequence-length 2048 from the WikiText 2 dataset. For GPTQ, we used 128 samples from the same dataset with the same sequence-length.\n\n## Benchmarks \\- English Text\n\nIn this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.\n\n### Base Pretrained Models\n\n| Category | Benchmark | \\# Shots | Metric | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |\n| ----- | ----- | :---: | :---: | :---: | :---: | :---: |\n| General | MMLU | 5 | macro\\_avg/acc\\_char | 32.2 | 58 | 66.7 |\n|  | AGIEval English | 3-5 | average/acc\\_char | 23.3 | 39.2 | 47.8 |\n|  | ARC-Challenge | 25 | acc\\_char | 32.8 | 69.1 | 79.7 |\n| Reading comprehension | SQuAD | 1 | em | 49.2 | 67.7 | 77 |\n|  | QuAC (F1) | 1 | f1 | 37.9 | 42.9 | 44.9 |\n|  | DROP (F1) | 3 | f1 | 28.0 | 45.2 | 59.5 |\n| Long Context | Needle in Haystack | 0 | em | 96.8 | 1 | 1 |\n\n### Instruction Tuned Models\n\n| Capability |  | Benchmark | \\# Shots | Metric | Llama 3.2 1B bf16 | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B bf16 | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |\n| :---: | ----- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| General |  | MMLU | 5 | macro\\_avg/acc | 49.3 | 43.3 | 47.3 | 49.0 | 63.4 | 60.5 | 62 | 62.4 | 69.4 |\n| Re-writing |  | Open-rewrite eval | 0 | micro\\_avg/rougeL | 41.6 | 39.2 | 40.9 | 41.2 | 40.1 | 40.3 | 40.8 | 40.7 | 40.9 |\n| Summarization |  | TLDR9+ (test) | 1 | rougeL | 16.8 | 14.9 | 16.7 | 16.8 | 19.0 | 19.1 | 19.2 | 19.1 | 17.2 |\n| Instruction following |  | IFEval | 0 | Avg(Prompt/Instruction acc Loose/Strict) | 59.5 | 51.5 | 58.4 | 55.6 | 77.4 | 73.9 | 73.5 | 75.9 | 80.4 |\n| Math |  | GSM8K (CoT) | 8 | em\\_maj1@1 | 44.4 | 33.1 | 40.6 | 46.5 | 77.7 | 72.9 | 75.7 | 77.9 | 84.5 |\n|  |  | MATH (CoT) | 0 | final\\_em | 30.6 | 20.5 | 25.3 | 31.0 | 48.0 | 44.2 | 45.3 | 49.2 | 51.9 |\n| Reasoning |  | ARC-C | 0 | acc | 59.4 | 54.3 | 57 | 60.7 | 78.6 | 75.6 | 77.6 | 77.6 | 83.4 |\n|  |  | GPQA | 0 | acc | 27.2 | 25.9 | 26.3 | 25.9 | 32.8 | 32.8 | 31.7 | 33.9 | 32.8 |\n|  |  | Hellaswag | 0 | acc | 41.2 | 38.1 | 41.3 | 41.5 | 69.8 | 66.3 | 68 | 66.3 | 78.7 |\n| Tool Use |  | BFCL V2 | 0 | acc | 25.7 | 14.3 | 15.9 | 23.7 | 67.0 | 53.4 | 60.1 | 63.5 | 67.1 |\n|  |  | Nexus | 0 | macro\\_avg/acc | 13.5 | 5.2 | 9.6 | 12.5 | 34.3 | 32.4 | 31.5 | 30.1 | 38.5 |\n| Long Context |  | InfiniteBench/En.QA | 0 | longbook\\_qa/f1 | 20.3 | N/A | N/A | N/A | 19.8 | N/A | N/A | N/A | 27.3 |\n|  |  | InfiniteBench/En.MC | 0 | longbook\\_choice/acc | 38.0 | N/A | N/A | N/A | 63.3 | N/A | N/A | N/A | 72.2 |\n|  |  | NIH/Multi-needle | 0 | recall | 75.0 | N/A | N/A | N/A | 84.7 | N/A | N/A | N/A | 98.8 |\n| Multilingual |  | MGSM (CoT) | 0 | em | 24.5 | 13.7 | 18.2 | 24.4 | 58.2 | 48.9 | 54.3 | 56.8 | 68.9 |\n\n\\*\\*for comparison purposes only. Model not released.\n\n### Multilingual Benchmarks\n\n| Category | Benchmark | Language | Llama 3.2 1B | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| General | MMLU (5-shot, macro_avg/acc) | Portuguese | 39.8 | 34.9 | 38.9 | 40.2 | 54.5 | 50.9 | 53.3 | 53.4 | 62.1 |\n| | | Spanish | 41.5 | 36.0 | 39.8 | 41.8 | 55.1 | 51.9 | 53.6 | 53.6 | 62.5 |\n| | | Italian | 39.8 | 34.9 | 38.1 | 40.6 | 53.8 | 49.9 | 52.1 | 51.7 | 61.6 |\n| | | German | 39.2 | 34.9 | 37.5 | 39.6 | 53.3 | 50.0 | 52.2 | 51.3 | 60.6 |\n| | | French | 40.5 | 34.8 | 39.2 | 40.8 | 54.6 | 51.2 | 53.3 | 53.3 | 62.3 |\n| | | Hindi | 33.5 | 30.0 | 32.1 | 34.0 | 43.3 | 40.4 | 42.0 | 42.1 | 50.9 |\n| | | Thai | 34.7 | 31.2 | 32.4 | 34.9 | 44.5 | 41.3 | 44.0 | 42.2 | 50.3 |\n\n\\*\\*for comparison purposes only. Model not released.\n\n## Inference time\n\nIn the below table, we compare the performance metrics of different quantization methods (SpinQuant and QAT \\+ LoRA) with the BF16 baseline. The evaluation was done using the [ExecuTorch](https://github.com/pytorch/executorch) framework as the inference engine, with the ARM CPU as a backend using Android OnePlus 12 device.\n\n| Category | Decode (tokens/sec)  | Time-to-first-token (sec) | Prefill (tokens/sec) | Model size (PTE file size in MB) | Memory size (RSS in MB) |\n| :---- | ----- | ----- | ----- | ----- | ----- |\n| 1B BF16 (baseline) | 19.2 | 1.0 | 60.3 | 2358 | 3,185 |\n| 1B SpinQuant | 50.2 (2.6x) | 0.3 (-76.9%) | 260.5 (4.3x) | 1083 (-54.1%) | 1,921 (-39.7%) |\n| 1B QLoRA | 45.8 (2.4x) | 0.3 (-76.0%) | 252.0 (4.2x) | 1127 (-52.2%) | 2,255 (-29.2%) |\n| 3B BF16 (baseline) | 7.6 | 3.0 | 21.2 | 6129 | 7,419 |\n| 3B SpinQuant | 19.7 (2.6x) | 0.7 (-76.4%) | 89.7 (4.2x) | 2435 (-60.3%) | 3,726 (-49.8%) |\n| 3B QLoRA | 18.5 (2.4x) | 0.7 (-76.1%) | 88.8 (4.2x) | 2529 (-58.7%) | 4,060 (-45.3%) |\n\n(\\*) The performance measurement is done using an adb binary-based approach.\n(\\*\\*) It is measured on an Android OnePlus 12 device.\n(\\*\\*\\*) Time-to-first-token (TTFT)  is measured with prompt length=64\n\n*Footnote:*\n\n- *Decode (tokens/second) is for how quickly it keeps generating. Higher is better.*\n- *Time-to-first-token (TTFT for shorthand) is for how fast it generates the first token for a given prompt. Lower is better.*\n- *Prefill is the inverse of TTFT (aka 1/TTFT)  in tokens/second. Higher is better*\n- *Model size \\- how big is the model, measured by, PTE file, a binary file format for ExecuTorch*\n- *RSS size \\- Memory usage in resident set size (RSS)*\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama\n2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm\n3. Provide protections for the community to help prevent the misuse of our models\n\n### Responsible Deployment\n\n**Approach:** Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta\u2019s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver\u2019s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/).\n\n#### Llama 3.2 Instruct\n\n**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).\n\n**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We\u2019ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n#### Llama 3.2 Systems\n\n**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n\n### New Capabilities and Use Cases\n\n**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.\n\n**Constrained Environments:** Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.\n\n### Evaluations\n\n**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.\n\n**Red Teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n### Critical Risks\n\nIn addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:\n\n**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.\n\n**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the model\u2019s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n\n**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2\u2019s 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.\n\n### Community\n\n**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n\n**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta\u2019s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).\n\n**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n## Ethical Considerations and Limitations\n\n**Values:** The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\n\n**Testing:** Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2\u2019s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.\n"
    },
    {
      "@id": "ark:59852/model-mistralai-mistral-7b-instruct-v0-2-1otynso79jy",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "mistralai/Mistral-7B-Instruct-v0.2",
      "description": "```py",
      "author": "mistralai",
      "keywords": [
        "transformers",
        "pytorch",
        "safetensors",
        "mistral",
        "text-generation",
        "finetuned",
        "mistral-common",
        "conversational",
        "arxiv:2310.06825",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-generation",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "intendedUseCase": "The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. \nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.",
      "usageInformation": "```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n \nmodel = Transformer.from_folder(mistral_models_path)\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n\nresult = tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n \n```py\nfrom transformers import AutoModelForCausalLM\n \nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\nmodel.to(\"cuda\")\n \ngenerated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)",
      "contentUrl": "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/resolve/main/model-00001-of-00003.safetensors",
      "url": "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Model Card for Mistral-7B-Instruct-v0.2\n\n\n## Encode and Decode with `mistral_common`\n            \n```py\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n \nmistral_models_path = \"MISTRAL_MODELS_PATH\"\n \ntokenizer = MistralTokenizer.v1()\n \ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=\"Explain Machine Learning to me in a nutshell.\")])\n \ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n```\n \n## Inference with `mistral_inference`\n \n ```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n \nmodel = Transformer.from_folder(mistral_models_path)\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n\nresult = tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n## Inference with hugging face `transformers`\n \n```py\nfrom transformers import AutoModelForCausalLM\n \nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\nmodel.to(\"cuda\")\n \ngenerated_ids = model.generate(tokens, max_new_tokens=1000, do_sample=True)\n\n# decode with mistral tokenizer\nresult = tokenizer.decode(generated_ids[0].tolist())\nprint(result)\n```\n\n> [!TIP]\n> PRs to correct the `transformers` tokenizer so that it gives 1-to-1 the same results as the `mistral_common` reference implementation are very welcome!\n            \n---\n\nThe Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.2.\n\nMistral-7B-v0.2 has the following changes compared to Mistral-7B-v0.1\n- 32k context window (vs 8k context in v0.1)\n- Rope-theta = 1e6\n- No Sliding-Window Attention\n\nFor full details of this model please read our [paper](https://arxiv.org/abs/2310.06825) and [release blog post](https://mistral.ai/news/la-plateforme/).\n\n## Instruction format\n\nIn order to leverage instruction fine-tuning, your prompt should be surrounded by `[INST]` and `[/INST]` tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id.\n\nE.g.\n```\ntext = \"<s>[INST] What is your favourite condiment? [/INST]\"\n\"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> \"\n\"[INST] Do you have mayonnaise recipes? [/INST]\"\n```\n\nThis format is available as a [chat template](https://huggingface.co/docs/transformers/main/chat_templating) via the `apply_chat_template()` method:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\n```\n\n## Troubleshooting\n- If you see the following error:\n```\nTraceback (most recent call last):\nFile \"\", line 1, in\nFile \"/transformers/models/auto/auto_factory.py\", line 482, in from_pretrained\nconfig, kwargs = AutoConfig.from_pretrained(\nFile \"/transformers/models/auto/configuration_auto.py\", line 1022, in from_pretrained\nconfig_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\nFile \"/transformers/models/auto/configuration_auto.py\", line 723, in getitem\nraise KeyError(key)\nKeyError: 'mistral'\n```\n\nInstalling transformers from source should solve the issue\npip install git+https://github.com/huggingface/transformers\n\nThis should not be required after transformers-v4.33.4.\n\n## Limitations\n\nThe Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. \nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, William El Sayed."
    },
    {
      "@id": "ark:59852/model-nvidia-parakeet-tdt-0-6b-v2-iz2sibgd7uf",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "nvidia/parakeet-tdt-0.6b-v2",
      "description": "<style>",
      "author": "nvidia",
      "keywords": [
        "nemo",
        "automatic-speech-recognition",
        "speech",
        "audio",
        "Transducer",
        "TDT",
        "FastConformer",
        "Conformer",
        "pytorch",
        "NeMo",
        "hf-asr-leaderboard",
        "en",
        "dataset:nvidia/Granary",
        "dataset:nvidia/nemo-asr-set-3.0",
        "arxiv:2305.05084",
        "arxiv:2304.06795",
        "arxiv:2406.00899",
        "arxiv:2410.01036",
        "arxiv:2505.13404",
        "license:cc-by-4.0",
        "model-index",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/nvidia/Granary"
        },
        {
          "@id": "https://huggingface.co/datasets/nvidia/nemo-asr-set-3.0"
        }
      ],
      "hasBias": "Field                                                                                               |  Response\n---------------------------------------------------------------------------------------------------|---------------\nParticipation considerations from adversely impacted groups [protected classes](https://www.senate.ca.gov/content/protected-classes) in model design and testing  |  None\nMeasures taken to mitigate against unwanted bias    | None",
      "intendedUseCase": "This model serves developers, researchers, academics, and industries building applications that require speech-to-text capabilities, including but not limited to: conversational AI, voice assistants, transcription services, subtitle generation, and voice analytics platforms.",
      "usageInformation": "To train, fine-tune or play with the model you will need to install [NVIDIA NeMo](https://github.com/NVIDIA/NeMo). We recommend you install it after you've installed latest PyTorch version.\n```bash\npip install -U nemo_toolkit[\"asr\"]\n``` \nThe model is available for use in the NeMo toolkit [3], and can be used as a pre-trained checkpoint for inference or for fine-tuning on another dataset.\n\n#### Automatically instantiate the model\n\n```python\nimport nemo.collections.asr as nemo_asr\nasr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-0.6b-v2\")\n```\n\n#### Transcribing using Python\nFirst, let's get a sample\n```bash\nwget https://dldata-public.s3.us-east-2.amazonaws.com/2086-149220-0033.wav\n```\nThen simply do:\n```python\noutput = asr_model.transcribe(['2086-149220-0033.wav'])\nprint(output[0].text)\n```\n\n#### Transcribing with timestamps\n\nTo transcribe with timestamps:\n```python\noutput = asr_model.transcribe(['2086-149220-0033.wav'], timestamps=True)",
      "url": "https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2",
      "license": "cc-by-4.0",
      "isPartOf": [],
      "README": "\n# **\ud83e\udd9c Parakeet TDT 0.6B V2 (En)**\n\n<style>\nimg {\n display: inline;\n}\n</style>\n\n[![Model architecture](https://img.shields.io/badge/Model_Arch-FastConformer--TDT-blue#model-badge)](#model-architecture)\n| [![Model size](https://img.shields.io/badge/Params-0.6B-green#model-badge)](#model-architecture)\n| [![Language](https://img.shields.io/badge/Language-en-orange#model-badge)](#datasets)\n\n> **\ud83c\udf89 NEW: Multilingual Parakeet TDT 0.6B V3 is now available!**  \n> \ud83c\udf0d **25 European Languages** | \ud83d\ude80 **Enhanced Performance** | \ud83d\udd17 **[Try it here: nvidia/parakeet-tdt-0.6b-v3](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v3)**\n\n## <span style=\"color:#466f00;\">Description:</span>\n\n`parakeet-tdt-0.6b-v2` is a 600-million-parameter automatic speech recognition (ASR) model designed for high-quality English transcription, featuring support for punctuation, capitalization, and accurate timestamp prediction. Try Demo here: https://huggingface.co/spaces/nvidia/parakeet-tdt-0.6b-v2 \n\nThis XL variant of the FastConformer [1] architecture integrates the TDT [2] decoder and is trained with full attention, enabling efficient transcription of audio segments up to 24 minutes in a single pass. The model achieves an RTFx of 3380 on the HF-Open-ASR leaderboard with a batch size of 128. Note: *RTFx Performance may vary depending on dataset audio duration and batch size.*  \n\n\n\n**Key Features**\n- Accurate word-level timestamp predictions  \n- Automatic punctuation and capitalization  \n- Robust performance on spoken numbers, and song lyrics transcription \n\nFor more information, refer to the [Model Architecture](#model-architecture) section and the [NeMo documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer).\n\nThis model is ready for commercial/non-commercial use.\n\n\n## <span style=\"color:#466f00;\">License/Terms of Use:</span>\n\nGOVERNING TERMS: Use of this model is governed by the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode.en) license.\n\n\n### <span style=\"color:#466f00;\">Deployment Geography:</span>\nGlobal\n\n\n### <span style=\"color:#466f00;\">Use Case:</span>\n\nThis model serves developers, researchers, academics, and industries building applications that require speech-to-text capabilities, including but not limited to: conversational AI, voice assistants, transcription services, subtitle generation, and voice analytics platforms.\n\n\n### <span style=\"color:#466f00;\">Release Date:</span>\n\n05/01/2025\n\n### <span style=\"color:#466f00;\">Model Architecture:</span>\n\n**Architecture Type**: \n\nFastConformer-TDT\n\n**Network Architecture**:\n\n* This model was developed based on [FastConformer encoder](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer) architecture[1] and TDT decoder[2]\n* This model has 600 million model parameters.\n\n### <span style=\"color:#466f00;\">Input:</span>\n- **Input Type(s):** 16kHz Audio\n- **Input Format(s):** `.wav` and `.flac` audio formats\n- **Input Parameters:** 1D (audio signal)\n- **Other Properties Related to Input:**  Monochannel audio\n\n### <span style=\"color:#466f00;\">Output:</span>\n- **Output Type(s):**  Text\n- **Output Format:**  String\n- **Output Parameters:**  1D (text)\n- **Other Properties Related to Output:** Punctuations and Capitalizations included.\n\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions. \n\n## <span style=\"color:#466f00;\">How to Use this Model:</span>\n\nTo train, fine-tune or play with the model you will need to install [NVIDIA NeMo](https://github.com/NVIDIA/NeMo). We recommend you install it after you've installed latest PyTorch version.\n```bash\npip install -U nemo_toolkit[\"asr\"]\n``` \nThe model is available for use in the NeMo toolkit [3], and can be used as a pre-trained checkpoint for inference or for fine-tuning on another dataset.\n\n#### Automatically instantiate the model\n\n```python\nimport nemo.collections.asr as nemo_asr\nasr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-0.6b-v2\")\n```\n\n#### Transcribing using Python\nFirst, let's get a sample\n```bash\nwget https://dldata-public.s3.us-east-2.amazonaws.com/2086-149220-0033.wav\n```\nThen simply do:\n```python\noutput = asr_model.transcribe(['2086-149220-0033.wav'])\nprint(output[0].text)\n```\n\n#### Transcribing with timestamps\n\nTo transcribe with timestamps:\n```python\noutput = asr_model.transcribe(['2086-149220-0033.wav'], timestamps=True)\n# by default, timestamps are enabled for char, word and segment level\nword_timestamps = output[0].timestamp['word'] # word level timestamps for first sample\nsegment_timestamps = output[0].timestamp['segment'] # segment level timestamps\nchar_timestamps = output[0].timestamp['char'] # char level timestamps\n\nfor stamp in segment_timestamps:\n    print(f\"{stamp['start']}s - {stamp['end']}s : {stamp['segment']}\")\n```\n\n\n## <span style=\"color:#466f00;\">Software Integration:</span>\n\n**Runtime Engine(s):**\n* NeMo 2.2  \n\n\n**Supported Hardware Microarchitecture Compatibility:** \n* NVIDIA Ampere\n* NVIDIA Blackwell  \n* NVIDIA Hopper\n* NVIDIA Volta\n\n**[Preferred/Supported] Operating System(s):**\n\n- Linux\n\n**Hardware Specific Requirements:**\n\nAtleast 2GB RAM for model to load. The bigger the RAM, the larger audio input it supports.\n\n#### Model Version\n\nCurrent version: parakeet-tdt-0.6b-v2. Previous versions can be [accessed](https://huggingface.co/collections/nvidia/parakeet-659711f49d1469e51546e021) here. \n\n## <span style=\"color:#466f00;\">Training and Evaluation Datasets:</span>\n\n### <span style=\"color:#466f00;\">Training</span>\n\nThis model was trained using the NeMo toolkit [3], following the strategies below:\n\n- Initialized from a FastConformer SSL checkpoint that was pretrained with a wav2vec method on the LibriLight dataset[7].  \n- Trained for 150,000 steps on 64 A100 GPUs. \n- Dataset corpora were balanced using a temperature sampling value of 0.5.  \n- Stage 2 fine-tuning was performed for 2,500 steps on 4 A100 GPUs using approximately 500 hours of high-quality, human-transcribed data of NeMo ASR Set 3.0.  \n\nTraining was conducted using this [example script](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/asr_transducer/speech_to_text_rnnt_bpe.py) and [TDT configuration](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/conf/fastconformer/hybrid_transducer_ctc/fastconformer_hybrid_tdt_ctc_bpe.yaml).\n\nThe tokenizer was constructed from the training set transcripts using this [script](https://github.com/NVIDIA/NeMo/blob/main/scripts/tokenizers/process_asr_text_tokenizer.py).\n\n### <span style=\"color:#466f00;\">Training Dataset</span>\nThe model was trained on the Granary dataset[8], consisting of approximately 120,000 hours of English speech data:\n\n- 10,000 hours from human-transcribed NeMo ASR Set 3.0, including:\n  - LibriSpeech (960 hours)\n  - Fisher Corpus\n  - National Speech Corpus Part 1 \n  - VCTK\n  - VoxPopuli (English)\n  - Europarl-ASR (English)\n  - Multilingual LibriSpeech (MLS English) \u2013 2,000-hour subset\n  - Mozilla Common Voice (v7.0)\n  - AMI\n\n- 110,000 hours of pseudo-labeled data from:\n  - YTC (YouTube-Commons) dataset[4]\n  - YODAS dataset [5]\n  - Librilight [7]\n\nAll transcriptions preserve punctuation and capitalization. The Granary dataset[8] will be made publicly available after presentation at Interspeech 2025.\n\n**Data Collection Method by dataset**\n\n* Hybrid: Automated, Human\n\n**Labeling Method by dataset**\n\n* Hybrid: Synthetic, Human \n\n**Properties:**\n\n* Noise robust data from various sources\n* Single channel, 16kHz sampled data\n\n#### Evaluation Dataset\n\nHuggingface Open ASR Leaderboard datasets are used to evaluate the performance of this model. \n\n**Data Collection Method by dataset**\n* Human\n\n**Labeling Method by dataset**\n* Human\n\n**Properties:**\n\n* All are commonly used for benchmarking English ASR systems.\n* Audio data is typically processed into a 16kHz mono channel format for ASR evaluation, consistent with benchmarks like the [Open ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard).\n\n## <span style=\"color:#466f00;\">Performance</span>\n\n#### Huggingface Open-ASR-Leaderboard Performance\nThe performance of Automatic Speech Recognition (ASR) models is measured using Word Error Rate (WER). Given that this model is trained on a large and diverse dataset spanning multiple domains, it is generally more robust and accurate across various types of audio.\n\n### Base Performance\nThe table below summarizes the WER (%) using a Transducer decoder with greedy decoding (without an external language model):\n\n| **Model** | **Avg WER** | **AMI** | **Earnings-22** | **GigaSpeech** | **LS test-clean** | **LS test-other** | **SPGI Speech** | **TEDLIUM-v3** | **VoxPopuli** |\n|:-------------|:-------------:|:---------:|:------------------:|:----------------:|:-----------------:|:-----------------:|:------------------:|:----------------:|:---------------:|\n| parakeet-tdt-0.6b-v2 | 6.05 | 11.16 | 11.15 | 9.74 | 1.69 | 3.19 | 2.17 | 3.38 | 5.95 | - |\n\n### Noise Robustness\nPerformance across different Signal-to-Noise Ratios (SNR) using MUSAN music and noise samples:\n\n| **SNR Level** | **Avg WER** | **AMI** | **Earnings** | **GigaSpeech** | **LS test-clean** | **LS test-other** | **SPGI** | **Tedlium** | **VoxPopuli** | **Relative Change** |\n|:---------------|:-------------:|:----------:|:------------:|:----------------:|:-----------------:|:-----------------:|:-----------:|:-------------:|:---------------:|:-----------------:|\n| Clean | 6.05 | 11.16 | 11.15 | 9.74 | 1.69 | 3.19 | 2.17 | 3.38 | 5.95 | - |\n| SNR 10 | 6.95 | 14.38 | 12.04 | 10.24 | 1.92 | 4.13 | 2.84 | 3.63 | 6.38 | -14.75% |\n| SNR 5 | 8.23 | 18.07 | 13.82 | 11.18 | 2.33 | 5.58 | 3.81 | 4.24 | 6.81 | -35.97% |\n| SNR 0 | 11.88 | 25.43 | 18.59 | 14.32 | 4.40 | 10.07 | 7.27 | 6.42 | 8.54 | -96.28% |\n| SNR -5 | 20.26 | 36.57 | 28.06 | 22.27 | 11.82 | 19.91 | 16.14 | 13.07 | 14.23 | -234.66% |\n\n### Telephony Audio Performance \nPerformance comparison between standard 16kHz audio and telephony-style audio (using \u03bc-law encoding with 16kHz\u21928kHz\u219216kHz conversion):\n\n| **Audio Format** | **Avg WER** | **AMI** | **Earnings** | **GigaSpeech** | **LS test-clean** | **LS test-other** | **SPGI** | **Tedlium** | **VoxPopuli** | **Relative Change** |\n|:-----------------|:-------------:|:----------:|:------------:|:----------------:|:-----------------:|:-----------------:|:-----------:|:-------------:|:---------------:|:-----------------:|\n| Standard 16kHz | 6.05 | 11.16 | 11.15 | 9.74 | 1.69 | 3.19 | 2.17 | 3.38 | 5.95 | - |\n| \u03bc-law 8kHz | 6.32 | 11.98 | 11.16 | 10.02 | 1.78 | 3.52 | 2.20 | 3.38 | 6.52 | -4.10% |\n\nThese WER scores were obtained using greedy decoding without an external language model. Additional evaluation details are available on the [Hugging Face ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard).[6]\n\n\n\n## <span style=\"color:#466f00;\">References</span>\n\n[1] [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://arxiv.org/abs/2305.05084)\n\n[2] [Efficient Sequence Transduction by Jointly Predicting Tokens and Durations](https://arxiv.org/abs/2304.06795)\n\n[3] [NVIDIA NeMo Toolkit](https://github.com/NVIDIA/NeMo)\n\n[4] [Youtube-commons: A massive open corpus for conversational and multimodal data](https://huggingface.co/blog/Pclanglais/youtube-commons)\n\n[5] [Yodas: Youtube-oriented dataset for audio and speech](https://arxiv.org/abs/2406.00899)\n\n[6] [HuggingFace ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)\n\n[7] [MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages](https://arxiv.org/abs/2410.01036) \n\n[8] [Granary: Speech Recognition and Translation Dataset in 25 European Languages](https://arxiv.org/pdf/2505.13404)\n\n## <span style=\"color:#466f00;\">Inference:</span>\n\n**Engine**: \n* NVIDIA NeMo\n\n**Test Hardware**:\n* NVIDIA A10\n* NVIDIA A100\n* NVIDIA A30\n* NVIDIA H100\n* NVIDIA L4\n* NVIDIA L40\n* NVIDIA Turing T4\n* NVIDIA Volta V100\n\n## <span style=\"color:#466f00;\">Ethical Considerations:</span>\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.\n\nFor more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards [here](https://developer.nvidia.com/blog/enhancing-ai-transparency-and-ethical-considerations-with-model-card/).\n\nPlease report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\n## <span style=\"color:#466f00;\">Bias:</span>\n\nField                                                                                               |  Response\n---------------------------------------------------------------------------------------------------|---------------\nParticipation considerations from adversely impacted groups [protected classes](https://www.senate.ca.gov/content/protected-classes) in model design and testing  |  None\nMeasures taken to mitigate against unwanted bias    | None\n\n## <span style=\"color:#466f00;\">Explainability:</span>\n\nField                                                                                                  |  Response\n------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------\nIntended Domain                                                                   |  Speech to Text Transcription\nModel Type                                                                                            |  FastConformer\nIntended Users                                                                                        |  This model is intended for developers, researchers, academics, and industries building conversational based applications. \nOutput                                                                                                |  Text \nDescribe how the model works                                                                          |  Speech input is encoded into embeddings and passed into conformer-based model and output a text response.\nName the adversely impacted groups this has been tested to deliver comparable outcomes regardless of  |  Not Applicable\nTechnical Limitations & Mitigation                                                                    |  Transcripts may be not 100% accurate. Accuracy varies based on language and characteristics of input audio (Domain, Use Case, Accent, Noise, Speech Type, Context of speech, etc.)\nVerified to have met prescribed NVIDIA quality standards  |  Yes\nPerformance Metrics                                                                                   | Word Error Rate\nPotential Known Risks                                                                                 |  If a word is not trained in the language model and not presented in vocabulary, the word is not likely to be recognized. Not recommended for word-for-word/incomplete sentences as accuracy varies based on the context of input text\nLicensing                                                                                             |  GOVERNING TERMS: Use of this model is governed by the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode.en) license.\n\n## <span style=\"color:#466f00;\">Privacy:</span>\n\nField                                                                                                                              |  Response\n----------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------\nGeneratable or reverse engineerable personal data?                                                     |  None\nPersonal data used to create this model?                                                                                       |  None\nIs there provenance for all datasets used in training?                                                                                |  Yes\nDoes data labeling (annotation, metadata) comply with privacy laws?                                                                |  Yes\nIs data compliant with data subject requests for data correction or removal, if such a request was made?                           |  No, not possible with externally-sourced data.\nApplicable Privacy Policy        | https://www.nvidia.com/en-us/about-nvidia/privacy-policy/ \n\n## <span style=\"color:#466f00;\">Safety:</span>\n\nField                                               |  Response\n---------------------------------------------------|----------------------------------\nModel Application(s)                               |  Speech to Text Transcription\nDescribe the life critical impact   |  None\nUse Case Restrictions                              | Abide by [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/legalcode.en) License\nModel and dataset restrictions            |  The Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training, and dataset license constraints adhered to.\n"
    },
    {
      "@id": "ark:59852/model-google-bert-bert-base-cased-hfgzzkrb826",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "google-bert/bert-base-cased",
      "description": "Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in",
      "author": "google-bert",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "safetensors",
        "bert",
        "fill-mask",
        "exbert",
        "en",
        "dataset:bookcorpus",
        "dataset:wikipedia",
        "arxiv:1810.04805",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/bookcorpus"
        },
        {
          "@id": "https://huggingface.co/datasets/wikipedia"
        }
      ],
      "hasBias": "Even if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-cased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] The man worked as a lawyer. [SEP]',\n  'score': 0.04804691672325134,\n  'token': 4545,\n  'token_str': 'lawyer'},\n {'sequence': '[CLS] The man worked as a waiter. [SEP]',\n  'score': 0.037494491785764694,\n  'token': 17989,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] The man worked as a cop. [SEP]',\n  'score': 0.035512614995241165,\n  'token': 9947,\n  'token_str': 'cop'},\n {'sequence': '[CLS] The man worked as a detective. [SEP]',\n  'score': 0.031271643936634064,\n  'token': 9140,\n  'token_str': 'detective'},\n {'sequence': '[CLS] The man worked as a doctor. [SEP]',\n  'score': 0.027423162013292313,\n  'token': 3995,\n  'token_str': 'doctor'}]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] The woman worked as a nurse. [SEP]',\n  'score': 0.16927455365657806,\n  'token': 7439,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] The woman worked as a waitress. [SEP]',\n  'score': 0.1501094549894333,\n  'token': 15098,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] The woman worked as a maid. [SEP]',\n  'score': 0.05600163713097572,\n  'token': 13487,\n  'token_str': 'maid'},\n {'sequence': '[CLS] The woman worked as a housekeeper. [SEP]',\n  'score': 0.04838843643665314,\n  'token': 26458,\n  'token_str': 'housekeeper'},\n {'sequence': '[CLS] The woman worked as a cook. [SEP]',\n  'score': 0.029980547726154327,\n  'token': 9834,\n  'token_str': 'cook'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.",
      "intendedUseCase": "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-cased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] Hello I'm a fashion model. [SEP]\",\n  'score': 0.09019174426794052,\n  'token': 4633,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] Hello I'm a new model. [SEP]\",\n  'score': 0.06349995732307434,\n  'token': 1207,\n  'token_str': 'new'},\n {'sequence': \"[CLS] Hello I'm a male model. [SEP]\",\n  'score': 0.06228214129805565,\n  'token': 2581,\n  'token_str': 'male'},\n {'sequence': \"[CLS] Hello I'm a professional model. [SEP]\",\n  'score': 0.0441727414727211,\n  'token': 1848,\n  'token_str': 'professional'},\n {'sequence': \"[CLS] Hello I'm a super model. [SEP]\",\n  'score': 0.03326151892542839,\n  'token': 7688,\n  'token_str': 'super'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nmodel = BertModel.from_pretrained(\"bert-base-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nmodel = TFBertModel.from_pretrained(\"bert-base-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-cased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] The man worked as a lawyer. [SEP]',\n  'score': 0.04804691672325134,\n  'token': 4545,\n  'token_str': 'lawyer'},\n {'sequence': '[CLS] The man worked as a waiter. [SEP]',\n  'score': 0.037494491785764694,\n  'token': 17989,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] The man worked as a cop. [SEP]',\n  'score': 0.035512614995241165,\n  'token': 9947,\n  'token_str': 'cop'},\n {'sequence': '[CLS] The man worked as a detective. [SEP]',\n  'score': 0.031271643936634064,\n  'token': 9140,\n  'token_str': 'detective'},\n {'sequence': '[CLS] The man worked as a doctor. [SEP]',\n  'score': 0.027423162013292313,\n  'token': 3995,\n  'token_str': 'doctor'}]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] The woman worked as a nurse. [SEP]',\n  'score': 0.16927455365657806,\n  'token': 7439,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] The woman worked as a waitress. [SEP]',\n  'score': 0.1501094549894333,\n  'token': 15098,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] The woman worked as a maid. [SEP]',\n  'score': 0.05600163713097572,\n  'token': 13487,\n  'token_str': 'maid'},\n {'sequence': '[CLS] The woman worked as a housekeeper. [SEP]',\n  'score': 0.04838843643665314,\n  'token': 26458,\n  'token_str': 'housekeeper'},\n {'sequence': '[CLS] The woman worked as a cook. [SEP]',\n  'score': 0.029980547726154327,\n  'token': 9834,\n  'token_str': 'cook'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.",
      "usageInformation": "You can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-cased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] Hello I'm a fashion model. [SEP]\",\n  'score': 0.09019174426794052,\n  'token': 4633,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] Hello I'm a new model. [SEP]\",\n  'score': 0.06349995732307434,\n  'token': 1207,\n  'token_str': 'new'},\n {'sequence': \"[CLS] Hello I'm a male model. [SEP]\",\n  'score': 0.06228214129805565,\n  'token': 2581,\n  'token_str': 'male'},\n {'sequence': \"[CLS] Hello I'm a professional model. [SEP]\",\n  'score': 0.0441727414727211,\n  'token': 1848,\n  'token_str': 'professional'},\n {'sequence': \"[CLS] Hello I'm a super model. [SEP]\",\n  'score': 0.03326151892542839,\n  'token': 7688,\n  'token_str': 'super'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nmodel = BertModel.from_pretrained(\"bert-base-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nmodel = TFBertModel.from_pretrained(\"bert-base-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```",
      "contentUrl": "https://huggingface.co/google-bert/bert-base-cased/resolve/main/model.safetensors",
      "url": "https://huggingface.co/google-bert/bert-base-cased",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# BERT base model (cased)\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1810.04805) and first released in\n[this repository](https://github.com/google-research/bert). This model is case-sensitive: it makes a difference between\nenglish and English.\n\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\nwas pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-cased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] Hello I'm a fashion model. [SEP]\",\n  'score': 0.09019174426794052,\n  'token': 4633,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] Hello I'm a new model. [SEP]\",\n  'score': 0.06349995732307434,\n  'token': 1207,\n  'token_str': 'new'},\n {'sequence': \"[CLS] Hello I'm a male model. [SEP]\",\n  'score': 0.06228214129805565,\n  'token': 2581,\n  'token_str': 'male'},\n {'sequence': \"[CLS] Hello I'm a professional model. [SEP]\",\n  'score': 0.0441727414727211,\n  'token': 1848,\n  'token_str': 'professional'},\n {'sequence': \"[CLS] Hello I'm a super model. [SEP]\",\n  'score': 0.03326151892542839,\n  'token': 7688,\n  'token_str': 'super'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nmodel = BertModel.from_pretrained(\"bert-base-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nmodel = TFBertModel.from_pretrained(\"bert-base-cased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-cased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] The man worked as a lawyer. [SEP]',\n  'score': 0.04804691672325134,\n  'token': 4545,\n  'token_str': 'lawyer'},\n {'sequence': '[CLS] The man worked as a waiter. [SEP]',\n  'score': 0.037494491785764694,\n  'token': 17989,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] The man worked as a cop. [SEP]',\n  'score': 0.035512614995241165,\n  'token': 9947,\n  'token_str': 'cop'},\n {'sequence': '[CLS] The man worked as a detective. [SEP]',\n  'score': 0.031271643936634064,\n  'token': 9140,\n  'token_str': 'detective'},\n {'sequence': '[CLS] The man worked as a doctor. [SEP]',\n  'score': 0.027423162013292313,\n  'token': 3995,\n  'token_str': 'doctor'}]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] The woman worked as a nurse. [SEP]',\n  'score': 0.16927455365657806,\n  'token': 7439,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] The woman worked as a waitress. [SEP]',\n  'score': 0.1501094549894333,\n  'token': 15098,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] The woman worked as a maid. [SEP]',\n  'score': 0.05600163713097572,\n  'token': 13487,\n  'token_str': 'maid'},\n {'sequence': '[CLS] The woman worked as a housekeeper. [SEP]',\n  'score': 0.04838843643665314,\n  'token': 26458,\n  'token_str': 'housekeeper'},\n {'sequence': '[CLS] The woman worked as a cook. [SEP]',\n  'score': 0.029980547726154327,\n  'token': 9834,\n  'token_str': 'cook'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038\nunpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and\nheaders).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are then of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\nused is Adam with a learning rate of 1e-4, \\\\(\\beta_{1} = 0.9\\\\) and \\\\(\\beta_{2} = 0.999\\\\), a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI-(m/mm) | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  | Average |\n|:----:|:-----------:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|:-------:|\n|      | 84.6/83.4   | 71.2 | 90.5 | 93.5  | 52.1 | 85.8  | 88.9 | 66.4 | 79.6    |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=bert-base-cased\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n"
    },
    {
      "@id": "ark:59852/model-facebook-bart-large-mnli-ewfbnprhsh",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "facebook/bart-large-mnli",
      "description": "This is the checkpoint for [bart-large](https://huggingface.co/facebook/bart-large) after being trained on the [MultiNLI (MNLI)](https://huggingface.co/datasets/multi_nli) dataset.",
      "author": "facebook",
      "keywords": [
        "transformers",
        "pytorch",
        "jax",
        "rust",
        "safetensors",
        "bart",
        "text-classification",
        "zero-shot-classification",
        "dataset:multi_nli",
        "arxiv:1910.13461",
        "arxiv:1909.00161",
        "license:mit",
        "autotrain_compatible",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-classification",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/multi_nli"
        }
      ],
      "usageInformation": "This method is surprisingly effective in many cases, particularly when used with larger pre-trained models like BART and Roberta. See [this blog post](https://joeddav.github.io/blog/2020/05/29/ZSL.html) for a more expansive introduction to this and other zero shot methods, and see the code snippets below for examples of using this model for zero-shot classification both with Hugging Face's built-in pipeline and with native Transformers/PyTorch code.",
      "contentUrl": "https://huggingface.co/facebook/bart-large-mnli/resolve/main/model.safetensors",
      "url": "https://huggingface.co/facebook/bart-large-mnli",
      "license": "mit",
      "isPartOf": [],
      "README": "\n# bart-large-mnli\n\nThis is the checkpoint for [bart-large](https://huggingface.co/facebook/bart-large) after being trained on the [MultiNLI (MNLI)](https://huggingface.co/datasets/multi_nli) dataset.\n\nAdditional information about this model:\n- The [bart-large](https://huggingface.co/facebook/bart-large) model page\n- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n](https://arxiv.org/abs/1910.13461)\n- [BART fairseq implementation](https://github.com/pytorch/fairseq/tree/master/fairseq/models/bart)\n\n## NLI-based Zero Shot Text Classification\n\n[Yin et al.](https://arxiv.org/abs/1909.00161) proposed a method for using pre-trained NLI models as a ready-made zero-shot sequence classifiers. The method works by posing the sequence to be classified as the NLI premise and to construct a hypothesis from each candidate label. For example, if we want to evaluate whether a sequence belongs to the class \"politics\", we could construct a hypothesis of `This text is about politics.`. The probabilities for entailment and contradiction are then converted to label probabilities.\n\nThis method is surprisingly effective in many cases, particularly when used with larger pre-trained models like BART and Roberta. See [this blog post](https://joeddav.github.io/blog/2020/05/29/ZSL.html) for a more expansive introduction to this and other zero shot methods, and see the code snippets below for examples of using this model for zero-shot classification both with Hugging Face's built-in pipeline and with native Transformers/PyTorch code.\n\n#### With the zero-shot classification pipeline\n\nThe model can be loaded with the `zero-shot-classification` pipeline like so:\n\n```python\nfrom transformers import pipeline\nclassifier = pipeline(\"zero-shot-classification\",\n                      model=\"facebook/bart-large-mnli\")\n```\n\nYou can then use this pipeline to classify sequences into any of the class names you specify.\n\n```python\nsequence_to_classify = \"one day I will see the world\"\ncandidate_labels = ['travel', 'cooking', 'dancing']\nclassifier(sequence_to_classify, candidate_labels)\n#{'labels': ['travel', 'dancing', 'cooking'],\n# 'scores': [0.9938651323318481, 0.0032737774308770895, 0.002861034357920289],\n# 'sequence': 'one day I will see the world'}\n```\n\nIf more than one candidate label can be correct, pass `multi_label=True` to calculate each class independently:\n\n```python\ncandidate_labels = ['travel', 'cooking', 'dancing', 'exploration']\nclassifier(sequence_to_classify, candidate_labels, multi_label=True)\n#{'labels': ['travel', 'exploration', 'dancing', 'cooking'],\n# 'scores': [0.9945111274719238,\n#  0.9383890628814697,\n#  0.0057061901316046715,\n#  0.0018193122232332826],\n# 'sequence': 'one day I will see the world'}\n```\n\n\n#### With manual PyTorch\n\n```python\n# pose sequence as a NLI premise and label as a hypothesis\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nnli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\ntokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n\npremise = sequence\nhypothesis = f'This example is {label}.'\n\n# run through model pre-trained on MNLI\nx = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n                     truncation_strategy='only_first')\nlogits = nli_model(x.to(device))[0]\n\n# we throw away \"neutral\" (dim 1) and take the probability of\n# \"entailment\" (2) as the probability of the label being true \nentail_contradiction_logits = logits[:,[0,2]]\nprobs = entail_contradiction_logits.softmax(dim=1)\nprob_label_is_true = probs[:,1]\n```\n"
    },
    {
      "@id": "ark:59852/model-trl-internal-testing-tiny-qwen2forcausallm-2-5-acvf1lytsgx",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "trl-internal-testing/tiny-Qwen2ForCausalLM-2.5",
      "description": "This is a minimal model built for unit tests in the [TRL](https://github.com/huggingface/trl) library.",
      "author": "trl-internal-testing",
      "keywords": [
        "transformers",
        "safetensors",
        "qwen2",
        "text-generation",
        "trl",
        "conversational",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-generation",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "contentUrl": "https://huggingface.co/trl-internal-testing/tiny-Qwen2ForCausalLM-2.5/resolve/main/model.safetensors",
      "url": "https://huggingface.co/trl-internal-testing/tiny-Qwen2ForCausalLM-2.5",
      "isPartOf": [],
      "README": "\n# Tiny Qwen2ForCausalLM\n\nThis is a minimal model built for unit tests in the [TRL](https://github.com/huggingface/trl) library.\n"
    },
    {
      "@id": "ark:59852/model-google-vit-base-patch16-224-s6vypmo8gkm",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "google/vit-base-patch16-224",
      "description": "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer). However, the weights were converted from the [timm repository](https://github.com/rwightman/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.",
      "author": "google",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "safetensors",
        "vit",
        "image-classification",
        "vision",
        "dataset:imagenet-1k",
        "dataset:imagenet-21k",
        "arxiv:2010.11929",
        "arxiv:2006.03677",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "image-classification",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/imagenet-1k"
        },
        {
          "@id": "https://huggingface.co/datasets/imagenet-21k"
        }
      ],
      "intendedUseCase": "You can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=google/vit) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits",
      "usageInformation": "Here is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits",
      "contentUrl": "https://huggingface.co/google/vit-base-patch16-224/resolve/main/model.safetensors",
      "url": "https://huggingface.co/google/vit-base-patch16-224",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Vision Transformer (base-sized model) \n\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer). However, the weights were converted from the [timm repository](https://github.com/rwightman/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. \n\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=google/vit) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n\ninputs = processor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n```\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/transformers/model_doc/vit.html#).\n\n## Training data\n\nThe ViT model was pretrained on [ImageNet-21k](http://www.image-net.org/), a dataset consisting of 14 million images and 21k classes, and fine-tuned on [ImageNet](http://www.image-net.org/challenges/LSVRC/2012/), a dataset consisting of 1 million images and 1k classes. \n\n## Training procedure\n\n### Preprocessing\n\nThe exact details of preprocessing of images during training/validation can be found [here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py). \n\nImages are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\n\n### Pretraining\n\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Training resolution is 224.\n\n## Evaluation results\n\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{wu2020visual,\n      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, \n      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n      year={2020},\n      eprint={2006.03677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n```bibtex\n@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}\n```"
    },
    {
      "@id": "ark:59852/model-ggml-org-models-moved-iodpacgvcqt",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "ggml-org/models-moved",
      "description": "Various models to be used in llama.cpp CI workflow.",
      "author": "ggml-org",
      "keywords": [
        "gguf",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "trainingDataset": [],
      "url": "https://huggingface.co/ggml-org/models-moved",
      "isPartOf": [],
      "README": "Various models to be used in llama.cpp CI workflow.\n\nDo not use it in production."
    },
    {
      "@id": "ark:59852/model-isotonic-distilbertfinetunedai4privacyv2-n2hexf8ztgd",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "Isotonic/distilbert_finetuned_ai4privacy_v2",
      "description": "<!-- This model card has been generated automatically according to the information the Trainer had access to. You",
      "author": "Isotonic",
      "keywords": [
        "transformers",
        "onnx",
        "safetensors",
        "distilbert",
        "token-classification",
        "generated_from_trainer",
        "en",
        "dataset:ai4privacy/pii-masking-200k",
        "dataset:Isotonic/pii-masking-200k",
        "base_model:distilbert/distilbert-base-uncased",
        "base_model:quantized:distilbert/distilbert-base-uncased",
        "doi:10.57967/hf/6999",
        "license:cc-by-nc-4.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "onnx",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/ai4privacy/pii-masking-200k"
        },
        {
          "@id": "https://huggingface.co/datasets/Isotonic/pii-masking-200k"
        }
      ],
      "intendedUseCase": "More information needed",
      "usageInformation": "GitHub Implementation: [Ai4Privacy](https://github.com/Sripaad/ai4privacy)",
      "baseModel": "distilbert-base-uncased",
      "contentUrl": "https://huggingface.co/Isotonic/distilbert_finetuned_ai4privacy_v2/resolve/main/model.safetensors",
      "url": "https://huggingface.co/Isotonic/distilbert_finetuned_ai4privacy_v2",
      "license": "cc-by-nc-4.0",
      "isPartOf": [],
      "README": "\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n\ud83c\udf1f Buying me coffee is a direct way to show support for this project. \n<a href=\"https://www.buymeacoffee.com/isotonic\"><img src=\"https://www.buymeacoffee.com/assets/img/guidelines/download-assets-sm-1.svg\" alt=\"\"></a>\n\n\n# distilbert_finetuned_ai4privacy_v2\n\nThis model is a fine-tuned version of [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) on the English Subset of [ai4privacy/pii-masking-200k](https://huggingface.co/datasets/ai4privacy/pii-masking-200k) dataset.\n\n## Useage\nGitHub Implementation: [Ai4Privacy](https://github.com/Sripaad/ai4privacy)\n\n## Model description\n\nThis model has been finetuned on the World's largest open source privacy dataset.\n\nThe purpose of the trained models is to remove personally identifiable information (PII) from text, especially in the context of AI assistants and LLMs.\n\nThe example texts have 54 PII classes (types of sensitive data), targeting 229 discussion subjects / use cases split across business, education, psychology and legal fields, and 5 interactions styles (e.g. casual conversation, formal document, emails etc...).\n\nTake a look at the Github implementation for specific reasearch.\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training hyperparameters\nThe following hyperparameters were used during training:\n- learning_rate: 5e-05\n- train_batch_size: 8\n- eval_batch_size: 8\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine_with_restarts\n- lr_scheduler_warmup_ratio: 0.2\n- num_epochs: 5\n\n## Class wise metrics\nIt achieves the following results on the evaluation set:\n- Loss: 0.0451\n- Overall Precision: 0.9438\n- Overall Recall: 0.9663\n- Overall F1: 0.9549\n- Overall Accuracy: 0.9838\n  \n- Accountname F1: 0.9946\n- Accountnumber F1: 0.9940\n- Age F1: 0.9624\n- Amount F1: 0.9643\n- Bic F1: 0.9929\n- Bitcoinaddress F1: 0.9948\n- Buildingnumber F1: 0.9845\n- City F1: 0.9955\n- Companyname F1: 0.9962\n- County F1: 0.9877\n- Creditcardcvv F1: 0.9643\n- Creditcardissuer F1: 0.9953\n- Creditcardnumber F1: 0.9793\n- Currency F1: 0.7811\n- Currencycode F1: 0.8850\n- Currencyname F1: 0.2281\n- Currencysymbol F1: 0.9562\n- Date F1: 0.9061\n- Dob F1: 0.7914\n- Email F1: 1.0\n- Ethereumaddress F1: 1.0\n- Eyecolor F1: 0.9837\n- Firstname F1: 0.9846\n- Gender F1: 0.9971\n- Height F1: 0.9910\n- Iban F1: 0.9906\n- Ip F1: 0.4349\n- Ipv4 F1: 0.8126\n- Ipv6 F1: 0.7679\n- Jobarea F1: 0.9880\n- Jobtitle F1: 0.9991\n- Jobtype F1: 0.9777\n- Lastname F1: 0.9684\n- Litecoinaddress F1: 0.9721\n- Mac F1: 1.0\n- Maskednumber F1: 0.9635\n- Middlename F1: 0.9330\n- Nearbygpscoordinate F1: 1.0\n- Ordinaldirection F1: 0.9910\n- Password F1: 1.0\n- Phoneimei F1: 0.9918\n- Phonenumber F1: 0.9962\n- Pin F1: 0.9477\n- Prefix F1: 0.9546\n- Secondaryaddress F1: 0.9892\n- Sex F1: 0.9876\n- Ssn F1: 0.9976\n- State F1: 0.9893\n- Street F1: 0.9873\n- Time F1: 0.9889\n- Url F1: 1.0\n- Useragent F1: 0.9953\n- Username F1: 0.9975\n- Vehiclevin F1: 1.0\n- Vehiclevrm F1: 1.0\n- Zipcode F1: 0.9873\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Overall Precision | Overall Recall | Overall F1 | Overall Accuracy | Accountname F1 | Accountnumber F1 | Age F1 | Amount F1 | Bic F1 | Bitcoinaddress F1 | Buildingnumber F1 | City F1 | Companyname F1 | County F1 | Creditcardcvv F1 | Creditcardissuer F1 | Creditcardnumber F1 | Currency F1 | Currencycode F1 | Currencyname F1 | Currencysymbol F1 | Date F1 | Dob F1 | Email F1 | Ethereumaddress F1 | Eyecolor F1 | Firstname F1 | Gender F1 | Height F1 | Iban F1 | Ip F1  | Ipv4 F1 | Ipv6 F1 | Jobarea F1 | Jobtitle F1 | Jobtype F1 | Lastname F1 | Litecoinaddress F1 | Mac F1 | Maskednumber F1 | Middlename F1 | Nearbygpscoordinate F1 | Ordinaldirection F1 | Password F1 | Phoneimei F1 | Phonenumber F1 | Pin F1 | Prefix F1 | Secondaryaddress F1 | Sex F1 | Ssn F1 | State F1 | Street F1 | Time F1 | Url F1 | Useragent F1 | Username F1 | Vehiclevin F1 | Vehiclevrm F1 | Zipcode F1 |\n|:-------------:|:-----:|:----:|:---------------:|:-----------------:|:--------------:|:----------:|:----------------:|:--------------:|:----------------:|:------:|:---------:|:------:|:-----------------:|:-----------------:|:-------:|:--------------:|:---------:|:----------------:|:-------------------:|:-------------------:|:-----------:|:---------------:|:---------------:|:-----------------:|:-------:|:------:|:--------:|:------------------:|:-----------:|:------------:|:---------:|:---------:|:-------:|:------:|:-------:|:-------:|:----------:|:-----------:|:----------:|:-----------:|:------------------:|:------:|:---------------:|:-------------:|:----------------------:|:-------------------:|:-----------:|:------------:|:--------------:|:------:|:---------:|:-------------------:|:------:|:------:|:--------:|:---------:|:-------:|:------:|:------------:|:-----------:|:-------------:|:-------------:|:----------:|\n| 0.6445        | 1.0   | 1088 | 0.3322          | 0.6449            | 0.7003         | 0.6714     | 0.8900           | 0.7607         | 0.8733           | 0.6576 | 0.1766    | 0.25   | 0.6783            | 0.3621            | 0.6005  | 0.6909         | 0.5586    | 0.0              | 0.2449              | 0.7095              | 0.2889      | 0.0             | 0.0             | 0.3902            | 0.7720  | 0.0    | 0.9862   | 0.8011             | 0.5088      | 0.7740       | 0.7118    | 0.5434    | 0.8088  | 0.0    | 0.8303  | 0.7562  | 0.5318     | 0.7294      | 0.4681     | 0.6779      | 0.0                | 0.8909 | 0.0             | 0.0107        | 0.9985                 | 0.4000              | 0.7307      | 0.9057       | 0.8618         | 0.0    | 0.9127    | 0.8235              | 0.9211 | 0.8026 | 0.4656   | 0.6390    | 0.9383  | 0.9775 | 0.8868       | 0.8201      | 0.4526        | 0.0550        | 0.5368     |\n| 0.222         | 2.0   | 2176 | 0.1259          | 0.8170            | 0.8747         | 0.8449     | 0.9478           | 0.9708         | 0.9813           | 0.7638 | 0.7427    | 0.7837 | 0.8908            | 0.8833            | 0.8747  | 0.9814         | 0.8749    | 0.7601           | 0.9777              | 0.8834              | 0.5372      | 0.4828          | 0.0056          | 0.7785            | 0.8149  | 0.3140 | 0.9956   | 0.9935             | 0.9101      | 0.9270       | 0.9450    | 0.9853    | 0.9253  | 0.0650 | 0.0084  | 0.7962  | 0.9013     | 0.9446      | 0.9203     | 0.8555      | 0.6885             | 1.0    | 0.7152          | 0.6442        | 1.0                    | 0.9623              | 0.9349      | 0.9905       | 0.9782         | 0.7656 | 0.9324    | 0.9903              | 0.9736 | 0.9274 | 0.8520   | 0.9138    | 0.9678  | 0.9922 | 0.9893       | 0.9804      | 0.9646        | 0.8556        | 0.8385     |\n| 0.1331        | 3.0   | 3264 | 0.0773          | 0.9133            | 0.9371         | 0.9250     | 0.9654           | 0.9822         | 0.9815           | 0.9196 | 0.8852    | 0.9718 | 0.9785            | 0.9215            | 0.9757  | 0.9935         | 0.9651    | 0.8742           | 0.9921              | 0.9438              | 0.7568      | 0.7710          | 0.0             | 0.8998            | 0.7895  | 0.6578 | 0.9994   | 1.0                | 0.9554      | 0.9525       | 0.9823    | 0.9910    | 0.9866  | 0.0435 | 0.8293  | 0.7824  | 0.9671     | 0.9794      | 0.9571     | 0.9447      | 0.9141             | 1.0    | 0.8825          | 0.7988        | 1.0                    | 0.9797              | 0.9921      | 0.9932       | 0.9943         | 0.8726 | 0.9401    | 0.9860              | 0.9792 | 0.9928 | 0.9740   | 0.9604    | 0.9730  | 0.9983 | 0.9964       | 0.9959      | 0.9890        | 0.9774        | 0.9247     |\n| 0.0847        | 4.0   | 4352 | 0.0503          | 0.9368            | 0.9614         | 0.9489     | 0.9789           | 0.9955         | 0.9949           | 0.9573 | 0.9480    | 0.9929 | 0.9846            | 0.9808            | 0.9927  | 0.9962         | 0.9811    | 0.9436           | 0.9953              | 0.9695              | 0.7826      | 0.8713          | 0.1653          | 0.9458            | 0.8782  | 0.7996 | 1.0      | 1.0                | 0.9809      | 0.9816       | 0.9941    | 0.9910    | 0.9906  | 0.3389 | 0.8364  | 0.7066  | 0.9862     | 1.0         | 0.9795     | 0.9637      | 0.9429             | 1.0    | 0.9438          | 0.9165        | 1.0                    | 0.9864              | 1.0         | 0.9932       | 0.9962         | 0.9352 | 0.9483    | 0.9860              | 0.9866 | 0.9976 | 0.9884   | 0.9827    | 0.9881  | 1.0    | 0.9953       | 0.9975      | 0.9945        | 0.9915        | 0.9841     |\n| 0.0557        | 5.0   | 5440 | 0.0451          | 0.9438            | 0.9663         | 0.9549     | 0.9838           | 0.9946         | 0.9940           | 0.9624 | 0.9643    | 0.9929 | 0.9948            | 0.9845            | 0.9955  | 0.9962         | 0.9877    | 0.9643           | 0.9953              | 0.9793              | 0.7811      | 0.8850          | 0.2281          | 0.9562            | 0.9061  | 0.7914 | 1.0      | 1.0                | 0.9837      | 0.9846       | 0.9971    | 0.9910    | 0.9906  | 0.4349 | 0.8126  | 0.7679  | 0.9880     | 0.9991      | 0.9777     | 0.9684      | 0.9721             | 1.0    | 0.9635          | 0.9330        | 1.0                    | 0.9910              | 1.0         | 0.9918       | 0.9962         | 0.9477 | 0.9546    | 0.9892              | 0.9876 | 0.9976 | 0.9893   | 0.9873    | 0.9889  | 1.0    | 0.9953       | 0.9975      | 1.0           | 1.0           | 0.9873     |\n\n\n### Framework versions\n\n- Transformers 4.35.0\n- Pytorch 2.0.0\n- Datasets 2.1.0\n- Tokenizers 0.14.1"
    },
    {
      "@id": "ark:59852/model-sentence-transformers-all-minilm-l12-v2-1otyns27ay",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "sentence-transformers/all-MiniLM-L12-v2",
      "description": "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
      "author": "sentence-transformers",
      "keywords": [
        "sentence-transformers",
        "pytorch",
        "rust",
        "onnx",
        "safetensors",
        "openvino",
        "bert",
        "feature-extraction",
        "sentence-similarity",
        "transformers",
        "en",
        "dataset:s2orc",
        "dataset:flax-sentence-embeddings/stackexchange_xml",
        "dataset:ms_marco",
        "dataset:gooaq",
        "dataset:yahoo_answers_topics",
        "dataset:code_search_net",
        "dataset:search_qa",
        "dataset:eli5",
        "dataset:snli",
        "dataset:multi_nli",
        "dataset:wikihow",
        "dataset:natural_questions",
        "dataset:trivia_qa",
        "dataset:embedding-data/sentence-compression",
        "dataset:embedding-data/flickr30k-captions",
        "dataset:embedding-data/altlex",
        "dataset:embedding-data/simple-wiki",
        "dataset:embedding-data/QQP",
        "dataset:embedding-data/SPECTER",
        "dataset:embedding-data/PAQ_pairs",
        "dataset:embedding-data/WikiAnswers",
        "arxiv:1904.06472",
        "arxiv:2102.07033",
        "arxiv:2104.08727",
        "arxiv:1704.05179",
        "arxiv:1810.09305",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-embeddings-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/s2orc"
        },
        {
          "@id": "https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml"
        },
        {
          "@id": "https://huggingface.co/datasets/ms_marco"
        },
        {
          "@id": "https://huggingface.co/datasets/gooaq"
        },
        {
          "@id": "https://huggingface.co/datasets/yahoo_answers_topics"
        },
        {
          "@id": "https://huggingface.co/datasets/code_search_net"
        },
        {
          "@id": "https://huggingface.co/datasets/search_qa"
        },
        {
          "@id": "https://huggingface.co/datasets/eli5"
        },
        {
          "@id": "https://huggingface.co/datasets/snli"
        },
        {
          "@id": "https://huggingface.co/datasets/multi_nli"
        },
        {
          "@id": "https://huggingface.co/datasets/wikihow"
        },
        {
          "@id": "https://huggingface.co/datasets/natural_questions"
        },
        {
          "@id": "https://huggingface.co/datasets/trivia_qa"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/sentence-compression"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/flickr30k-captions"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/altlex"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/simple-wiki"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/QQP"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/SPECTER"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/PAQ_pairs"
        },
        {
          "@id": "https://huggingface.co/datasets/embedding-data/WikiAnswers"
        }
      ],
      "intendedUseCase": "Our model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated.",
      "usageInformation": "Using this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F",
      "contentUrl": "https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2/resolve/main/model.safetensors",
      "url": "https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n\n# all-MiniLM-L12-v2\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n## Usage (Sentence-Transformers)\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L12-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\n# Normalize embeddings\nsentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n------\n\n## Background\n\nThe project aims to train sentence embedding models on very large sentence level datasets using a self-supervised \ncontrastive learning objective. We used the pretrained [`microsoft/MiniLM-L12-H384-uncased`](https://huggingface.co/microsoft/MiniLM-L12-H384-uncased) model and fine-tuned in on a \n1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.\n\nWe developped this model during the \n[Community week using JAX/Flax for NLP & CV](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104), \norganized by Hugging Face. We developped this model as part of the project:\n[Train the Best Sentence Embedding Model Ever with 1B Training Pairs](https://discuss.huggingface.co/t/train-the-best-sentence-embedding-model-ever-with-1b-training-pairs/7354). We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.\n\n## Intended uses\n\nOur model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`microsoft/MiniLM-L12-H384-uncased`](https://huggingface.co/microsoft/MiniLM-L12-H384-uncased) model. Please refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each possible sentence pairs from the batch.\nWe then apply the cross entropy loss by comparing with true pairs.\n\n#### Hyper parameters\n\nWe trained ou model on a TPU v3-8. We train the model during 100k steps using a batch size of 1024 (128 per TPU core).\nWe use a learning rate warm up of 500. The sequence length was limited to 128 tokens. We used the AdamW optimizer with\na 2e-5 learning rate. The full training script is accessible in this current repository: `train_script.py`.\n\n#### Training data\n\nWe use the concatenation from multiple datasets to fine-tune our model. The total number of sentence pairs is above 1 billion sentences.\nWe sampled each dataset given a weighted probability which configuration is detailed in the `data_config.json` file.\n\n\n| Dataset                                                  | Paper                                    | Number of training tuples  |\n|--------------------------------------------------------|:----------------------------------------:|:--------------------------:|\n| [Reddit comments (2015-2018)](https://github.com/PolyAI-LDN/conversational-datasets/tree/master/reddit) | [paper](https://arxiv.org/abs/1904.06472) | 726,484,430 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Abstracts) | [paper](https://aclanthology.org/2020.acl-main.447/) | 116,288,806 |\n| [WikiAnswers](https://github.com/afader/oqa#wikianswers-corpus) Duplicate question pairs | [paper](https://doi.org/10.1145/2623330.2623677) | 77,427,422 |\n| [PAQ](https://github.com/facebookresearch/PAQ) (Question, Answer) pairs | [paper](https://arxiv.org/abs/2102.07033) | 64,371,441 |\n| [S2ORC](https://github.com/allenai/s2orc) Citation pairs (Titles) | [paper](https://aclanthology.org/2020.acl-main.447/) | 52,603,982 |\n| [S2ORC](https://github.com/allenai/s2orc) (Title, Abstract) | [paper](https://aclanthology.org/2020.acl-main.447/) | 41,769,185 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Body) pairs  | - | 25,316,456 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title+Body, Answer) pairs  | - | 21,396,559 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) (Title, Answer) pairs  | - | 21,396,559 |\n| [MS MARCO](https://microsoft.github.io/msmarco/) triplets | [paper](https://doi.org/10.1145/3404835.3462804) | 9,144,553 |\n| [GOOAQ: Open Question Answering with Diverse Answer Types](https://github.com/allenai/gooaq) | [paper](https://arxiv.org/pdf/2104.08727.pdf) | 3,012,496 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 1,198,260 |\n| [Code Search](https://huggingface.co/datasets/code_search_net) | - | 1,151,414 |\n| [COCO](https://cocodataset.org/#home) Image captions | [paper](https://link.springer.com/chapter/10.1007%2F978-3-319-10602-1_48) | 828,395|\n| [SPECTER](https://github.com/allenai/specter) citation triplets | [paper](https://doi.org/10.18653/v1/2020.acl-main.207) | 684,100 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Question, Answer) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 681,164 |\n| [Yahoo Answers](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset) (Title, Question) | [paper](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html) | 659,896 |\n| [SearchQA](https://huggingface.co/datasets/search_qa) | [paper](https://arxiv.org/abs/1704.05179) | 582,261 |\n| [Eli5](https://huggingface.co/datasets/eli5) | [paper](https://doi.org/10.18653/v1/p19-1346) | 325,475 |\n| [Flickr 30k](https://shannon.cs.illinois.edu/DenotationGraph/) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/229/33) | 317,695 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles) | | 304,525 |\n| AllNLI ([SNLI](https://nlp.stanford.edu/projects/snli/) and [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) | [paper SNLI](https://doi.org/10.18653/v1/d15-1075), [paper MultiNLI](https://doi.org/10.18653/v1/n18-1101) | 277,230 | \n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (bodies) | | 250,519 |\n| [Stack Exchange](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_xml) Duplicate questions (titles+bodies) | | 250,460 |\n| [Sentence Compression](https://github.com/google-research-datasets/sentence-compression) | [paper](https://www.aclweb.org/anthology/D13-1155/) | 180,000 |\n| [Wikihow](https://github.com/pvl/wikihow_pairs_dataset) | [paper](https://arxiv.org/abs/1810.09305) | 128,542 |\n| [Altlex](https://github.com/chridey/altlex/) | [paper](https://aclanthology.org/P16-1135.pdf) | 112,696 |\n| [Quora Question Triplets](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) | - | 103,663 |\n| [Simple Wikipedia](https://cs.pomona.edu/~dkauchak/simplification/) | [paper](https://www.aclweb.org/anthology/P11-2117/) | 102,225 |\n| [Natural Questions (NQ)](https://ai.google.com/research/NaturalQuestions) | [paper](https://transacl.org/ojs/index.php/tacl/article/view/1455) | 100,231 |\n| [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) | [paper](https://aclanthology.org/P18-2124.pdf) | 87,599 |\n| [TriviaQA](https://huggingface.co/datasets/trivia_qa) | - | 73,346 |\n| **Total** | | **1,170,060,424** |"
    },
    {
      "@id": "ark:59852/model-meta-llama-llama-3-2-1b-vlpktukmnyk",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "meta-llama/Llama-3.2-1B",
      "description": "The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.",
      "author": "meta-llama",
      "keywords": [
        "transformers",
        "safetensors",
        "llama",
        "text-generation",
        "facebook",
        "meta",
        "pytorch",
        "llama-3",
        "en",
        "de",
        "fr",
        "it",
        "pt",
        "hi",
        "es",
        "th",
        "arxiv:2204.05149",
        "arxiv:2405.16406",
        "license:llama3.2",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "modelType": "text-generation",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "hasBias": "As part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama\n2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm\n3. Provide protections for the community to help prevent the misuse of our models\n\n### Responsible Deployment\n\n**Approach:** Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta\u2019s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver\u2019s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/).\n\n#### Llama 3.2 Instruct\n\n**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).\n\n**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We\u2019ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n#### Llama 3.2 Systems\n\n**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n\n### New Capabilities and Use Cases\n\n**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.\n\n**Constrained Environments:** Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.\n\n### Evaluations\n\n**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.\n\n**Red Teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n### Critical Risks\n\nIn addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:\n\n**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.\n\n**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the model\u2019s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n\n**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2\u2019s 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.\n\n### Community\n\n**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n\n**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta\u2019s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).\n\n**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.",
      "intendedUseCase": "**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.\n\n**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.",
      "usageInformation": "This repository contains two versions of Llama-3.2-1B, for use with transformers and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with transformers >= 4.43.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\n\nMake sure to update your transformers installation via pip install --upgrade transformers.\n\n```python\nimport torch\nfrom transformers import pipeline\n\nmodel_id = \"meta-llama/Llama-3.2-1B\"\n\npipe = pipeline(\n    \"text-generation\", \n    model=model_id, \n    torch_dtype=torch.bfloat16, \n    device_map=\"auto\"\n)\n\npipe(\"The key to life is\")\n```\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama).\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Llama-3.2-1B --include \"original/*\" --local-dir Llama-3.2-1B\n```",
      "contentUrl": "https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/model.safetensors",
      "url": "https://huggingface.co/meta-llama/Llama-3.2-1B",
      "license": "llama3.2",
      "isPartOf": [],
      "README": "\n## Model Information\n\nThe Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n\n**Model Developer:** Meta\n\n**Model Architecture:** Llama 3.2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\n|  | Training Data | Params | Input modalities | Output modalities | Context Length | GQA | Shared Embeddings | Token count | Knowledge cutoff |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| Llama 3.2 (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 128k | Yes | Yes | Up to 9T tokens | December 2023 |\n|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code  |  |  |  |  |  |\n| Llama 3.2 Quantized (text only)  | A new mix of publicly available online data. | 1B (1.23B) | Multilingual Text | Multilingual Text and code  | 8k | Yes | Yes | Up to 9T tokens | December 2023 |\n|  |  | 3B (3.21B) | Multilingual Text | Multilingual Text and code |  |  |  |  |  |\n\n**Supported Languages:** English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Developers may fine-tune Llama 3.2 models for languages beyond these supported languages, provided they comply with the Llama 3.2 Community License and the Acceptable Use Policy. Developers are always expected to ensure that their deployments, including those that involve additional languages, are completed safely and responsibly.\n\n**Llama 3.2 Model Family:** Token counts refer to pretraining data only. All model versions use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Release Date:** Sept 25, 2024\n\n**Status:** This is a static model trained on an offline dataset. Future versions may be released that improve model capabilities and safety.\n\n**License:** Use of Llama 3.2 is governed by the [Llama 3.2 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) (a custom, commercial license agreement).\n\n**Feedback:** Instructions on how to provide feedback or comments on the model can be found in the Llama Models [README](https://github.com/meta-llama/llama-models/blob/main/README.md). For more technical information about generation parameters and recipes for how to use Llama 3.2 in applications, please go [here](https://github.com/meta-llama/llama-recipes).\n\n## Intended Use\n\n**Intended Use Cases:** Llama 3.2 is intended for commercial and research use in multiple languages. Instruction tuned text only models are intended for assistant-like chat and agentic applications like knowledge retrieval and summarization, mobile AI powered writing assistants and query and prompt rewriting. Pretrained models can be adapted for a variety of additional natural language generation tasks. Similarly, quantized models can be adapted for a variety of on-device use-cases with limited compute resources.\n\n**Out of Scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in any other way that is prohibited by the Acceptable Use Policy and Llama 3.2 Community License. Use in languages beyond those explicitly referenced as supported in this model card.\n\n## How to use\n\nThis repository contains two versions of Llama-3.2-1B, for use with transformers and with the original `llama` codebase.\n\n### Use with transformers\n\nStarting with transformers >= 4.43.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\n\nMake sure to update your transformers installation via pip install --upgrade transformers.\n\n```python\nimport torch\nfrom transformers import pipeline\n\nmodel_id = \"meta-llama/Llama-3.2-1B\"\n\npipe = pipeline(\n    \"text-generation\", \n    model=model_id, \n    torch_dtype=torch.bfloat16, \n    device_map=\"auto\"\n)\n\npipe(\"The key to life is\")\n```\n\n### Use with `llama`\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama).\n\nTo download Original checkpoints, see the example command below leveraging `huggingface-cli`:\n\n```\nhuggingface-cli download meta-llama/Llama-3.2-1B --include \"original/*\" --local-dir Llama-3.2-1B\n```\n\n## Hardware and Software\n\n**Training Factors:** We used custom training libraries, Meta's custom built GPU cluster, and production infrastructure for pretraining. Fine-tuning, quantization, annotation, and evaluation were also performed on production infrastructure.\n\n**Training Energy Use:** Training utilized a cumulative of **916k** GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.\n\n**Training Greenhouse Gas Emissions:** Estimated total location-based greenhouse gas emissions were **240** tons CO2eq for training. Since 2020, Meta has maintained net zero greenhouse gas emissions in its global operations and matched 100% of its electricity use with renewable energy; therefore, the total market-based greenhouse gas emissions for training were 0 tons CO2eq.\n\n|  | Training Time (GPU hours) | Logit Generation Time (GPU Hours) | Training Power Consumption (W) | Training Location-Based Greenhouse Gas Emissions (tons CO2eq) | Training Market-Based Greenhouse Gas Emissions (tons CO2eq) |\n| :---- | :---: | ----- | :---: | :---: | :---: |\n| Llama 3.2 1B | 370k | \\- | 700 | 107 | 0 |\n| Llama 3.2 3B | 460k | \\- | 700 | 133 | 0 |\n| Llama 3.2 1B SpinQuant | 1.7 | 0 | 700 | *Negligible*\\*\\* | 0 |\n| Llama 3.2 3B SpinQuant | 2.4 | 0 | 700 | *Negligible*\\*\\* | 0 |\n| Llama 3.2 1B QLora | 1.3k | 0 | 700 | 0.381 | 0 |\n| Llama 3.2 3B QLora | 1.6k | 0 | 700 | 0.461 | 0 |\n| Total | 833k |         86k |  | 240 | 0 |\n\n\\*\\* The location-based CO2e emissions of Llama 3.2 1B SpinQuant and Llama 3.2 3B SpinQuant are less than 0.001 metric tonnes each. This is due to the minimal training GPU hours that are required.\n\nThe methodology used to determine training energy use and greenhouse gas emissions can be found [here](https://arxiv.org/pdf/2204.05149). Since Meta is openly releasing these models, the training energy use and greenhouse gas emissions will not be incurred by others.\n\n## Training Data\n\n**Overview:** Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources. For the 1B and 3B Llama 3.2 models, we incorporated logits from the Llama 3.1 8B and 70B models into the pretraining stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance. In post-training we used a similar recipe as Llama 3.1 and produced final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involved Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).\n\n**Data Freshness:** The pretraining data has a cutoff of December 2023\\.\n\n## Quantization\n\n### Quantization Scheme\n\nWe designed the current quantization scheme with the [PyTorch\u2019s ExecuTorch](https://github.com/pytorch/executorch) inference framework and Arm CPU backend in mind, taking into account metrics including model quality, prefill/decoding speed, and memory footprint. Our quantization scheme involves three parts:\n- All linear layers in all transformer blocks are quantized to a 4-bit groupwise scheme (with a group size of 32) for weights and 8-bit per-token dynamic quantization for activations.\n- The classification layer is quantized to 8-bit per-channel for weight and 8-bit per token dynamic quantization for activation.\n- Similar to classification layer, an 8-bit per channel quantization is used for embedding layer.\n\n\n### Quantization-Aware Training and LoRA\n\nThe quantization-aware training (QAT) with low-rank adaptation (LoRA) models went through only post-training stages, using the same data as the full precision models. To initialize QAT, we utilize BF16 Llama 3.2 model checkpoints obtained after supervised fine-tuning (SFT) and perform an additional full round of SFT training with QAT. We then freeze the backbone of the QAT model and perform another round of SFT with LoRA adaptors applied to all layers within the transformer block. Meanwhile, the LoRA adaptors' weights and activations are maintained in BF16. Because our approach is similar to QLoRA of Dettmers et al., (2023) (i.e., quantization followed by LoRA adapters), we refer this method as QLoRA. Finally, we fine-tune the resulting model (both backbone and LoRA adaptors) using direct preference optimization (DPO).\n\n### SpinQuant\n\n[SpinQuant](https://arxiv.org/abs/2405.16406) was applied, together with generative post-training quantization (GPTQ). For the SpinQuant rotation matrix fine-tuning, we optimized for 100 iterations, using 800 samples with sequence-length 2048 from the WikiText 2 dataset. For GPTQ, we used 128 samples from the same dataset with the same sequence-length.\n\n## Benchmarks \\- English Text\n\nIn this section, we report the results for Llama 3.2 models on standard automatic benchmarks. For all these evaluations, we used our internal evaluations library.\n\n### Base Pretrained Models\n\n| Category | Benchmark | \\# Shots | Metric | Llama 3.2 1B | Llama 3.2 3B | Llama 3.1 8B |\n| ----- | ----- | :---: | :---: | :---: | :---: | :---: |\n| General | MMLU | 5 | macro\\_avg/acc\\_char | 32.2 | 58 | 66.7 |\n|  | AGIEval English | 3-5 | average/acc\\_char | 23.3 | 39.2 | 47.8 |\n|  | ARC-Challenge | 25 | acc\\_char | 32.8 | 69.1 | 79.7 |\n| Reading comprehension | SQuAD | 1 | em | 49.2 | 67.7 | 77 |\n|  | QuAC (F1) | 1 | f1 | 37.9 | 42.9 | 44.9 |\n|  | DROP (F1) | 3 | f1 | 28.0 | 45.2 | 59.5 |\n| Long Context | Needle in Haystack | 0 | em | 96.8 | 1 | 1 |\n\n### Instruction Tuned Models\n\n| Capability |  | Benchmark | \\# Shots | Metric | Llama 3.2 1B bf16 | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B bf16 | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |\n| :---: | ----- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| General |  | MMLU | 5 | macro\\_avg/acc | 49.3 | 43.3 | 47.3 | 49.0 | 63.4 | 60.5 | 62 | 62.4 | 69.4 |\n| Re-writing |  | Open-rewrite eval | 0 | micro\\_avg/rougeL | 41.6 | 39.2 | 40.9 | 41.2 | 40.1 | 40.3 | 40.8 | 40.7 | 40.9 |\n| Summarization |  | TLDR9+ (test) | 1 | rougeL | 16.8 | 14.9 | 16.7 | 16.8 | 19.0 | 19.1 | 19.2 | 19.1 | 17.2 |\n| Instruction following |  | IFEval | 0 | Avg(Prompt/Instruction acc Loose/Strict) | 59.5 | 51.5 | 58.4 | 55.6 | 77.4 | 73.9 | 73.5 | 75.9 | 80.4 |\n| Math |  | GSM8K (CoT) | 8 | em\\_maj1@1 | 44.4 | 33.1 | 40.6 | 46.5 | 77.7 | 72.9 | 75.7 | 77.9 | 84.5 |\n|  |  | MATH (CoT) | 0 | final\\_em | 30.6 | 20.5 | 25.3 | 31.0 | 48.0 | 44.2 | 45.3 | 49.2 | 51.9 |\n| Reasoning |  | ARC-C | 0 | acc | 59.4 | 54.3 | 57 | 60.7 | 78.6 | 75.6 | 77.6 | 77.6 | 83.4 |\n|  |  | GPQA | 0 | acc | 27.2 | 25.9 | 26.3 | 25.9 | 32.8 | 32.8 | 31.7 | 33.9 | 32.8 |\n|  |  | Hellaswag | 0 | acc | 41.2 | 38.1 | 41.3 | 41.5 | 69.8 | 66.3 | 68 | 66.3 | 78.7 |\n| Tool Use |  | BFCL V2 | 0 | acc | 25.7 | 14.3 | 15.9 | 23.7 | 67.0 | 53.4 | 60.1 | 63.5 | 67.1 |\n|  |  | Nexus | 0 | macro\\_avg/acc | 13.5 | 5.2 | 9.6 | 12.5 | 34.3 | 32.4 | 31.5 | 30.1 | 38.5 |\n| Long Context |  | InfiniteBench/En.QA | 0 | longbook\\_qa/f1 | 20.3 | N/A | N/A | N/A | 19.8 | N/A | N/A | N/A | 27.3 |\n|  |  | InfiniteBench/En.MC | 0 | longbook\\_choice/acc | 38.0 | N/A | N/A | N/A | 63.3 | N/A | N/A | N/A | 72.2 |\n|  |  | NIH/Multi-needle | 0 | recall | 75.0 | N/A | N/A | N/A | 84.7 | N/A | N/A | N/A | 98.8 |\n| Multilingual |  | MGSM (CoT) | 0 | em | 24.5 | 13.7 | 18.2 | 24.4 | 58.2 | 48.9 | 54.3 | 56.8 | 68.9 |\n\n\\*\\*for comparison purposes only. Model not released.\n\n### Multilingual Benchmarks\n\n| Category | Benchmark | Language | Llama 3.2 1B | Llama 3.2 1B Vanilla PTQ\\*\\* | Llama 3.2 1B Spin Quant | Llama 3.2 1B QLoRA | Llama 3.2 3B | Llama 3.2 3B Vanilla PTQ\\*\\* | Llama 3.2 3B Spin Quant | Llama 3.2 3B QLoRA | Llama 3.1 8B |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| General | MMLU (5-shot, macro_avg/acc) | Portuguese | 39.8 | 34.9 | 38.9 | 40.2 | 54.5 | 50.9 | 53.3 | 53.4 | 62.1 |\n| | | Spanish | 41.5 | 36.0 | 39.8 | 41.8 | 55.1 | 51.9 | 53.6 | 53.6 | 62.5 |\n| | | Italian | 39.8 | 34.9 | 38.1 | 40.6 | 53.8 | 49.9 | 52.1 | 51.7 | 61.6 |\n| | | German | 39.2 | 34.9 | 37.5 | 39.6 | 53.3 | 50.0 | 52.2 | 51.3 | 60.6 |\n| | | French | 40.5 | 34.8 | 39.2 | 40.8 | 54.6 | 51.2 | 53.3 | 53.3 | 62.3 |\n| | | Hindi | 33.5 | 30.0 | 32.1 | 34.0 | 43.3 | 40.4 | 42.0 | 42.1 | 50.9 |\n| | | Thai | 34.7 | 31.2 | 32.4 | 34.9 | 44.5 | 41.3 | 44.0 | 42.2 | 50.3 |\n\n\\*\\*for comparison purposes only. Model not released.\n\n## Inference time\n\nIn the below table, we compare the performance metrics of different quantization methods (SpinQuant and QAT \\+ LoRA) with the BF16 baseline. The evaluation was done using the [ExecuTorch](https://github.com/pytorch/executorch) framework as the inference engine, with the ARM CPU as a backend using Android OnePlus 12 device.\n\n| Category | Decode (tokens/sec)  | Time-to-first-token (sec) | Prefill (tokens/sec) | Model size (PTE file size in MB) | Memory size (RSS in MB) |\n| :---- | ----- | ----- | ----- | ----- | ----- |\n| 1B BF16 (baseline) | 19.2 | 1.0 | 60.3 | 2358 | 3,185 |\n| 1B SpinQuant | 50.2 (2.6x) | 0.3 (-76.9%) | 260.5 (4.3x) | 1083 (-54.1%) | 1,921 (-39.7%) |\n| 1B QLoRA | 45.8 (2.4x) | 0.3 (-76.0%) | 252.0 (4.2x) | 1127 (-52.2%) | 2,255 (-29.2%) |\n| 3B BF16 (baseline) | 7.6 | 3.0 | 21.2 | 6129 | 7,419 |\n| 3B SpinQuant | 19.7 (2.6x) | 0.7 (-76.4%) | 89.7 (4.2x) | 2435 (-60.3%) | 3,726 (-49.8%) |\n| 3B QLoRA | 18.5 (2.4x) | 0.7 (-76.1%) | 88.8 (4.2x) | 2529 (-58.7%) | 4,060 (-45.3%) |\n\n(\\*) The performance measurement is done using an adb binary-based approach.\n(\\*\\*) It is measured on an Android OnePlus 12 device.\n(\\*\\*\\*) Time-to-first-token (TTFT)  is measured with prompt length=64\n\n*Footnote:*\n\n- *Decode (tokens/second) is for how quickly it keeps generating. Higher is better.*\n- *Time-to-first-token (TTFT for shorthand) is for how fast it generates the first token for a given prompt. Lower is better.*\n- *Prefill is the inverse of TTFT (aka 1/TTFT)  in tokens/second. Higher is better*\n- *Model size \\- how big is the model, measured by, PTE file, a binary file format for ExecuTorch*\n- *RSS size \\- Memory usage in resident set size (RSS)*\n\n## Responsibility & Safety\n\nAs part of our Responsible release approach, we followed a three-pronged strategy to managing trust & safety risks:\n\n1. Enable developers to deploy helpful, safe and flexible experiences for their target audience and for the use cases supported by Llama\n2. Protect developers against adversarial users aiming to exploit Llama capabilities to potentially cause harm\n3. Provide protections for the community to help prevent the misuse of our models\n\n### Responsible Deployment\n\n**Approach:** Llama is a foundational technology designed to be used in a variety of use cases. Examples on how Meta\u2019s Llama models have been responsibly deployed can be found in our [Community Stories webpage](https://llama.meta.com/community-stories/). Our approach is to build the most helpful models, enabling the world to benefit from the technology power, by aligning our model safety for generic use cases and addressing a standard set of harms. Developers are then in the driver\u2019s seat to tailor safety for their use cases, defining their own policies and deploying the models with the necessary safeguards in their Llama systems. Llama 3.2 was developed following the best practices outlined in our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide/).\n\n#### Llama 3.2 Instruct\n\n**Objective:** Our main objectives for conducting safety fine-tuning are to provide the research community with a valuable resource for studying the robustness of safety fine-tuning, as well as to offer developers a readily available, safe, and powerful model for various applications to reduce the developer workload to deploy safe AI systems. We implemented the same set of safety mitigations as in Llama 3, and you can learn more about these in the Llama 3 [paper](https://ai.meta.com/research/publications/the-llama-3-herd-of-models/).\n\n**Fine-Tuning Data:** We employ a multi-faceted approach to data collection, combining human-generated data from our vendors with synthetic data to mitigate potential safety risks. We\u2019ve developed many large language model (LLM)-based classifiers that enable us to thoughtfully select high-quality prompts and responses, enhancing data quality control.\n\n**Refusals and Tone:** Building on the work we started with Llama 3, we put a great emphasis on model refusals to benign prompts as well as refusal tone. We included both borderline and adversarial prompts in our safety data strategy, and modified our safety data responses to follow tone guidelines.\n\n#### Llama 3.2 Systems\n\n**Safety as a System:** Large language models, including Llama 3.2, **are not designed to be deployed in isolation** but instead should be deployed as part of an overall AI system with additional safety guardrails as required. Developers are expected to deploy system safeguards when building agentic systems. Safeguards are key to achieve the right helpfulness-safety alignment as well as mitigating safety and security risks inherent to the system and any integration of the model or system with external tools. As part of our responsible release approach, we provide the community with [safeguards](https://llama.meta.com/trust-and-safety/) that developers should deploy with Llama models or other LLMs, including Llama Guard, Prompt Guard and Code Shield. All our [reference implementations](https://github.com/meta-llama/llama-agentic-system) demos contain these safeguards by default so developers can benefit from system-level safety out-of-the-box.\n\n### New Capabilities and Use Cases\n\n**Technological Advancement:** Llama releases usually introduce new capabilities that require specific considerations in addition to the best practices that generally apply across all Generative AI use cases. For prior release capabilities also supported by Llama 3.2, see [Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md), as the same considerations apply here as well.\n\n**Constrained Environments:** Llama 3.2 1B and 3B models are expected to be deployed in highly constrained environments, such as mobile devices. LLM Systems using smaller models will have a different alignment profile and safety/helpfulness tradeoff than more complex, larger systems. Developers should ensure the safety of their system meets the requirements of their use case. We recommend using lighter system safeguards for such use cases, like Llama Guard 3-1B or its mobile-optimized version.\n\n### Evaluations\n\n**Scaled Evaluations:** We built dedicated, adversarial evaluation datasets and evaluated systems composed of Llama models and Purple Llama safeguards to filter input prompt and output response. It is important to evaluate applications in context, and we recommend building dedicated evaluation dataset for your use case.\n\n**Red Teaming:** We conducted recurring red teaming exercises with the goal of discovering risks via adversarial prompting and we used the learnings to improve our benchmarks and safety tuning datasets. We partnered early with subject-matter experts in critical risk areas to understand the nature of these real-world harms and how such models may lead to unintended harm for society. Based on these conversations, we derived a set of adversarial goals for the red team to attempt to achieve, such as extracting harmful information or reprogramming the model to act in a potentially harmful capacity. The red team consisted of experts in cybersecurity, adversarial machine learning, responsible AI, and integrity in addition to multilingual content specialists with background in integrity issues in specific geographic markets.\n\n### Critical Risks\n\nIn addition to our safety work above, we took extra care on measuring and/or mitigating the following critical risk areas:\n\n**1\\. CBRNE (Chemical, Biological, Radiological, Nuclear, and Explosive Weapons):** Llama 3.2 1B and 3B models are smaller and less capable derivatives of Llama 3.1. For Llama 3.1 70B and 405B, to assess risks related to proliferation of chemical and biological weapons, we performed uplift testing designed to assess whether use of Llama 3.1 models could meaningfully increase the capabilities of malicious actors to plan or carry out attacks using these types of weapons and have determined that such testing also applies to the smaller 1B and 3B models.\n\n**2\\. Child Safety:** Child Safety risk assessments were conducted using a team of experts, to assess the model\u2019s capability to produce outputs that could result in Child Safety risks and inform on any necessary and appropriate risk mitigations via fine tuning. We leveraged those expert red teaming sessions to expand the coverage of our evaluation benchmarks through Llama 3 model development. For Llama 3, we conducted new in-depth sessions using objective based methodologies to assess the model risks along multiple attack vectors including the additional languages Llama 3 is trained on. We also partnered with content specialists to perform red teaming exercises assessing potentially violating content while taking account of market specific nuances or experiences.\n\n**3\\. Cyber Attacks:** For Llama 3.1 405B, our cyber attack uplift study investigated whether LLMs can enhance human capabilities in hacking tasks, both in terms of skill level and speed.\nOur attack automation study focused on evaluating the capabilities of LLMs when used as autonomous agents in cyber offensive operations, specifically in the context of ransomware attacks. This evaluation was distinct from previous studies that considered LLMs as interactive assistants. The primary objective was to assess whether these models could effectively function as independent agents in executing complex cyber-attacks without human intervention. Because Llama 3.2\u2019s 1B and 3B models are smaller and less capable models than Llama 3.1 405B, we broadly believe that the testing conducted for the 405B model also applies to Llama 3.2 models.\n\n### Community\n\n**Industry Partnerships:** Generative AI safety requires expertise and tooling, and we believe in the strength of the open community to accelerate its progress. We are active members of open consortiums, including the AI Alliance, Partnership on AI and MLCommons, actively contributing to safety standardization and transparency. We encourage the community to adopt taxonomies like the MLCommons Proof of Concept evaluation to facilitate collaboration and transparency on safety and content evaluations. Our Purple Llama tools are open sourced for the community to use and widely distributed across ecosystem partners including cloud service providers. We encourage community contributions to our [Github repository](https://github.com/meta-llama/PurpleLlama).\n\n**Grants:** We also set up the [Llama Impact Grants](https://llama.meta.com/llama-impact-grants/) program to identify and support the most compelling applications of Meta\u2019s Llama model for societal benefit across three categories: education, climate and open innovation. The 20 finalists from the hundreds of applications can be found [here](https://llama.meta.com/llama-impact-grants/#finalists).\n\n**Reporting:** Finally, we put in place a set of resources including an [output reporting mechanism](https://developers.facebook.com/llama_output_feedback) and [bug bounty program](https://www.facebook.com/whitehat) to continuously improve the Llama technology with the help of the community.\n\n## Ethical Considerations and Limitations\n\n**Values:** The core values of Llama 3.2 are openness, inclusivity and helpfulness. It is meant to serve everyone, and to work for a wide range of use cases. It is thus designed to be accessible to people across many different backgrounds, experiences and perspectives. Llama 3.2 addresses users and their needs as they are, without insertion unnecessary judgment or normativity, while reflecting the understanding that even content that may appear problematic in some cases can serve valuable purposes in others. It respects the dignity and autonomy of all users, especially in terms of the values of free thought and expression that power innovation and progress.\n\n**Testing:** Llama 3.2 is a new technology, and like any new technology, there are risks associated with its use. Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, Llama 3.2\u2019s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 3.2 models, developers should perform safety testing and tuning tailored to their specific applications of the model. Please refer to available resources including our [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development.\n"
    },
    {
      "@id": "ark:59852/model-facebook-wav2vec2-base-960h-0djuzrytms5",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "facebook/wav2vec2-base-960h",
      "description": "[Facebook's Wav2Vec2](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)",
      "author": "facebook",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "safetensors",
        "wav2vec2",
        "automatic-speech-recognition",
        "audio",
        "hf-asr-leaderboard",
        "en",
        "dataset:librispeech_asr",
        "arxiv:2006.11477",
        "license:apache-2.0",
        "model-index",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/librispeech_asr"
        }
      ],
      "usageInformation": "To transcribe audio files the model can be used as a standalone acoustic model as follows:\n\n```python\n from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n from datasets import load_dataset\n import torch",
      "contentUrl": "https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/model.safetensors",
      "url": "https://huggingface.co/facebook/wav2vec2-base-960h",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Wav2Vec2-Base-960h\n\n[Facebook's Wav2Vec2](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)\n\nThe base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. When using the model\nmake sure that your speech input is also sampled at 16Khz.\n\n[Paper](https://arxiv.org/abs/2006.11477)\n\nAuthors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli\n\n**Abstract**\n\nWe show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.\n\nThe original model can be found under https://github.com/pytorch/fairseq/tree/master/examples/wav2vec#wav2vec-20.\n\n\n# Usage\n\nTo transcribe audio files the model can be used as a standalone acoustic model as follows:\n\n```python\n from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n from datasets import load_dataset\n import torch\n \n # load model and tokenizer\n processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n     \n # load dummy dataset and read soundfiles\n ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n \n # tokenize\n input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n \n # retrieve logits\n logits = model(input_values).logits\n \n # take argmax and decode\n predicted_ids = torch.argmax(logits, dim=-1)\n transcription = processor.batch_decode(predicted_ids)\n ```\n \n ## Evaluation\n \n This code snippet shows how to evaluate **facebook/wav2vec2-base-960h** on LibriSpeech's \"clean\" and \"other\" test data.\n \n```python\nfrom datasets import load_dataset\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport torch\nfrom jiwer import wer\n\n\nlibrispeech_eval = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(\"cuda\")\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n\ndef map_to_pred(batch):\n    input_values = processor(batch[\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values\n    with torch.no_grad():\n        logits = model(input_values.to(\"cuda\")).logits\n\n    predicted_ids = torch.argmax(logits, dim=-1)\n    transcription = processor.batch_decode(predicted_ids)\n    batch[\"transcription\"] = transcription\n    return batch\n\nresult = librispeech_eval.map(map_to_pred, batched=True, batch_size=1, remove_columns=[\"audio\"])\n\nprint(\"WER:\", wer(result[\"text\"], result[\"transcription\"]))\n```\n\n*Result (WER)*:\n\n| \"clean\" | \"other\" |\n|---|---|\n| 3.4 | 8.6 |"
    },
    {
      "@id": "ark:59852/model-openai-whisper-small-b9umck3j3ix",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "openai/whisper-small",
      "description": "Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours",
      "author": "openai",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "safetensors",
        "whisper",
        "automatic-speech-recognition",
        "audio",
        "hf-asr-leaderboard",
        "en",
        "zh",
        "de",
        "es",
        "ru",
        "ko",
        "fr",
        "ja",
        "pt",
        "tr",
        "pl",
        "ca",
        "nl",
        "ar",
        "sv",
        "it",
        "id",
        "hi",
        "fi",
        "vi",
        "he",
        "uk",
        "el",
        "ms",
        "cs",
        "ro",
        "da",
        "hu",
        "ta",
        "no",
        "th",
        "ur",
        "hr",
        "bg",
        "lt",
        "la",
        "mi",
        "ml",
        "cy",
        "sk",
        "te",
        "fa",
        "lv",
        "bn",
        "sr",
        "az",
        "sl",
        "kn",
        "et",
        "mk",
        "br",
        "eu",
        "is",
        "hy",
        "ne",
        "mn",
        "bs",
        "kk",
        "sq",
        "sw",
        "gl",
        "mr",
        "pa",
        "si",
        "km",
        "sn",
        "yo",
        "so",
        "af",
        "oc",
        "ka",
        "be",
        "tg",
        "sd",
        "gu",
        "am",
        "yi",
        "lo",
        "uz",
        "fo",
        "ht",
        "ps",
        "tk",
        "nn",
        "mt",
        "sa",
        "lb",
        "my",
        "bo",
        "tl",
        "mg",
        "as",
        "tt",
        "haw",
        "ln",
        "ha",
        "ba",
        "jw",
        "su",
        "arxiv:2212.04356",
        "license:apache-2.0",
        "model-index",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "hasBias": "The primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only \u201cintended\u201d uses or to draw reasonable guidelines around what is or is not research.",
      "intendedUseCase": "Our studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf). \n\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.",
      "usageInformation": "To transcribe audio samples, the model has to be used alongside a [`WhisperProcessor`](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperProcessor).\n\nThe `WhisperProcessor` is used to:\n1. Pre-process the audio inputs (converting them to log-Mel spectrograms for the model)\n2. Post-process the model outputs (converting them from tokens to text)\n\nThe model is informed of which task to perform (transcription or translation) by passing the appropriate \"context tokens\". These context tokens \nare a sequence of tokens that are given to the decoder at the start of the decoding process, and take the following order:\n1. The transcription always starts with the `<|startoftranscript|>` token\n2. The second token is the language token (e.g. `<|en|>` for English)\n3. The third token is the \"task token\". It can take one of two values: `<|transcribe|>` for speech recognition or `<|translate|>` for speech translation\n4. In addition, a `<|notimestamps|>` token is added if the model should not include timestamp prediction\n\nThus, a typical sequence of context tokens might look as follows:\n```\n<|startoftranscript|> <|en|> <|transcribe|> <|notimestamps|>\n```\nWhich tells the model to decode in English, under the task of speech recognition, and not to predict timestamps.\n\nThese tokens can either be forced or un-forced. If they are forced, the model is made to predict each token at \neach position. This allows one to control the output language and task for the Whisper model. If they are un-forced, \nthe Whisper model will automatically predict the output langauge and task itself.\n\nThe context tokens can be set accordingly:\n\n```python\nmodel.config.forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")\n```\n\nWhich forces the model to predict in English under the task of speech recognition.\n\n## Transcription\n\n### English to English \nIn this example, the context tokens are 'unforced', meaning the model automatically predicts the output language\n(English) and task (transcribe).\n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n>>> model.config.forced_decoder_ids = None\n\n>>> # load dummy dataset and read audio files\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n>>> input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features \n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.<|endoftext|>']\n\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.']\n```\nThe context tokens can be removed from the start of the transcription by setting `skip_special_tokens=True`.\n\n### French to French \nThe following example demonstrates French to French transcription by setting the decoder ids appropriately. \n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"transcribe\")\n\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids)\n['<|startoftranscript|><|fr|><|transcribe|><|notimestamps|> Un vrai travail int\u00e9ressant va enfin \u00eatre men\u00e9 sur ce sujet.<|endoftext|>']\n\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Un vrai travail int\u00e9ressant va enfin \u00eatre men\u00e9 sur ce sujet.']\n```\n\n## Translation \nSetting the task to \"translate\" forces the Whisper model to perform speech translation.\n\n### French to English\n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"translate\")\n\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' A very interesting work, we will finally be given on this subject.']\n```\n\n## Evaluation\n\nThis code snippet shows how to evaluate Whisper Small on [LibriSpeech test-clean](https://huggingface.co/datasets/librispeech_asr):\n \n```python\n>>> from datasets import load_dataset\n>>> from transformers import WhisperForConditionalGeneration, WhisperProcessor\n>>> import torch\n>>> from evaluate import load\n\n>>> librispeech_test_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to(\"cuda\")\n\n>>> def map_to_pred(batch):\n>>>     audio = batch[\"audio\"]\n>>>     input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>>     batch[\"reference\"] = processor.tokenizer._normalize(batch['text'])\n>>> \n>>>     with torch.no_grad():\n>>>         predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n>>>     transcription = processor.decode(predicted_ids)\n>>>     batch[\"prediction\"] = processor.tokenizer._normalize(transcription)\n>>>     return batch\n\n>>> result = librispeech_test_clean.map(map_to_pred)\n\n>>> wer = load(\"wer\")\n>>> print(100 * wer.compute(references=result[\"reference\"], predictions=result[\"prediction\"]))\n3.432213777886737\n```\n\n## Long-Form Transcription\n\nThe Whisper model is intrinsically designed to work on audio samples of up to 30s in duration. However, by using a chunking \nalgorithm, it can be used to transcribe audio samples of up to arbitrary length. This is possible through Transformers \n[`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline) \nmethod. Chunking is enabled by setting `chunk_length_s=30` when instantiating the pipeline. With chunking enabled, the pipeline \ncan be run with batched inference. It can also be extended to predict sequence level timestamps by passing `return_timestamps=True`:\n\n```python\n>>> import torch\n>>> from transformers import pipeline\n>>> from datasets import load_dataset\n\n>>> device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n>>> pipe = pipeline(\n>>>   \"automatic-speech-recognition\",\n>>>   model=\"openai/whisper-small\",\n>>>   chunk_length_s=30,\n>>>   device=device,\n>>> )\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n\n>>> prediction = pipe(sample.copy(), batch_size=8)[\"text\"]\n\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.\"\n\n>>> # we can also return timestamps for the predictions\n>>> prediction = pipe(sample.copy(), batch_size=8, return_timestamps=True)[\"chunks\"]\n[{'text': ' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.',\n  'timestamp': (0.0, 5.44)}]\n```\n\nRefer to the blog post [ASR Chunking](https://huggingface.co/blog/asr-chunking) for more details on the chunking algorithm.\n\n## Fine-Tuning\n\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However, \nits predictive capabilities can be improved further for certain languages and tasks through *fine-tuning*. The blog \npost [Fine-Tune Whisper with \ud83e\udd17 Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step \nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\n\n### Evaluated Use\n\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only \u201cintended\u201d uses or to draw reasonable guidelines around what is or is not research.\n\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\n\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\n\n\n## Training Data\n\nThe models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages. \n\nAs discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.\n\n\n## Performance and Limitations\n\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf). \n\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\n\n\n## Broader Implications\n\nWe anticipate that Whisper models\u2019 transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box \u2013 their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\n\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\n\n\n### BibTeX entry and citation info\n```bibtex\n@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n```",
      "contentUrl": "https://huggingface.co/openai/whisper-small/resolve/main/model.safetensors",
      "url": "https://huggingface.co/openai/whisper-small",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# Whisper\n\nWhisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours \nof labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains **without** the need \nfor fine-tuning.\n\nWhisper was proposed in the paper [Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356) \nby Alec Radford et al from OpenAI. The original code repository can be found [here](https://github.com/openai/whisper).\n\n**Disclaimer**: Content for this model card has partly been written by the Hugging Face team, and parts of it were \ncopied and pasted from the original model card.\n\n## Model details\n\nWhisper is a Transformer based encoder-decoder model, also referred to as a _sequence-to-sequence_ model. \nIt was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. \n\nThe models were trained on either English-only data or multilingual data. The English-only models were trained \non the task of speech recognition. The multilingual models were trained on both speech recognition and speech \ntranslation. For speech recognition, the model predicts transcriptions in the *same* language as the audio. \nFor speech translation, the model predicts transcriptions to a *different* language to the audio.\n\nWhisper checkpoints come in five configurations of varying model sizes.\nThe smallest four are trained on either English-only or multilingual data.\nThe largest checkpoints are multilingual only. All ten of the pre-trained checkpoints \nare available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The \ncheckpoints are summarised in the following table with links to the models on the Hub:\n\n| Size     | Parameters | English-only                                         | Multilingual                                        |\n|----------|------------|------------------------------------------------------|-----------------------------------------------------|\n| tiny     | 39 M       | [\u2713](https://huggingface.co/openai/whisper-tiny.en)   | [\u2713](https://huggingface.co/openai/whisper-tiny)     |\n| base     | 74 M       | [\u2713](https://huggingface.co/openai/whisper-base.en)   | [\u2713](https://huggingface.co/openai/whisper-base)     |\n| small    | 244 M      | [\u2713](https://huggingface.co/openai/whisper-small.en)  | [\u2713](https://huggingface.co/openai/whisper-small)    |\n| medium   | 769 M      | [\u2713](https://huggingface.co/openai/whisper-medium.en) | [\u2713](https://huggingface.co/openai/whisper-medium)   |\n| large    | 1550 M     | x                                                    | [\u2713](https://huggingface.co/openai/whisper-large)    |\n| large-v2 | 1550 M     | x                                                    | [\u2713](https://huggingface.co/openai/whisper-large-v2) |\n\n# Usage\n\nTo transcribe audio samples, the model has to be used alongside a [`WhisperProcessor`](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperProcessor).\n\nThe `WhisperProcessor` is used to:\n1. Pre-process the audio inputs (converting them to log-Mel spectrograms for the model)\n2. Post-process the model outputs (converting them from tokens to text)\n\nThe model is informed of which task to perform (transcription or translation) by passing the appropriate \"context tokens\". These context tokens \nare a sequence of tokens that are given to the decoder at the start of the decoding process, and take the following order:\n1. The transcription always starts with the `<|startoftranscript|>` token\n2. The second token is the language token (e.g. `<|en|>` for English)\n3. The third token is the \"task token\". It can take one of two values: `<|transcribe|>` for speech recognition or `<|translate|>` for speech translation\n4. In addition, a `<|notimestamps|>` token is added if the model should not include timestamp prediction\n\nThus, a typical sequence of context tokens might look as follows:\n```\n<|startoftranscript|> <|en|> <|transcribe|> <|notimestamps|>\n```\nWhich tells the model to decode in English, under the task of speech recognition, and not to predict timestamps.\n\nThese tokens can either be forced or un-forced. If they are forced, the model is made to predict each token at \neach position. This allows one to control the output language and task for the Whisper model. If they are un-forced, \nthe Whisper model will automatically predict the output langauge and task itself.\n\nThe context tokens can be set accordingly:\n\n```python\nmodel.config.forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")\n```\n\nWhich forces the model to predict in English under the task of speech recognition.\n\n## Transcription\n\n### English to English \nIn this example, the context tokens are 'unforced', meaning the model automatically predicts the output language\n(English) and task (transcribe).\n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n>>> model.config.forced_decoder_ids = None\n\n>>> # load dummy dataset and read audio files\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n>>> input_features = processor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_tensors=\"pt\").input_features \n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.<|endoftext|>']\n\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.']\n```\nThe context tokens can be removed from the start of the transcription by setting `skip_special_tokens=True`.\n\n### French to French \nThe following example demonstrates French to French transcription by setting the decoder ids appropriately. \n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"transcribe\")\n\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids)\n['<|startoftranscript|><|fr|><|transcribe|><|notimestamps|> Un vrai travail int\u00e9ressant va enfin \u00eatre men\u00e9 sur ce sujet.<|endoftext|>']\n\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' Un vrai travail int\u00e9ressant va enfin \u00eatre men\u00e9 sur ce sujet.']\n```\n\n## Translation \nSetting the task to \"translate\" forces the Whisper model to perform speech translation.\n\n### French to English\n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"french\", task=\"translate\")\n\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset(\"common_voice\", \"fr\", split=\"test\", streaming=True)\n>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))[\"audio\"]\n>>> input_features = processor(input_speech[\"array\"], sampling_rate=input_speech[\"sampling_rate\"], return_tensors=\"pt\").input_features\n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n[' A very interesting work, we will finally be given on this subject.']\n```\n\n## Evaluation\n\nThis code snippet shows how to evaluate Whisper Small on [LibriSpeech test-clean](https://huggingface.co/datasets/librispeech_asr):\n \n```python\n>>> from datasets import load_dataset\n>>> from transformers import WhisperForConditionalGeneration, WhisperProcessor\n>>> import torch\n>>> from evaluate import load\n\n>>> librispeech_test_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n\n>>> processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n>>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to(\"cuda\")\n\n>>> def map_to_pred(batch):\n>>>     audio = batch[\"audio\"]\n>>>     input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n>>>     batch[\"reference\"] = processor.tokenizer._normalize(batch['text'])\n>>> \n>>>     with torch.no_grad():\n>>>         predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n>>>     transcription = processor.decode(predicted_ids)\n>>>     batch[\"prediction\"] = processor.tokenizer._normalize(transcription)\n>>>     return batch\n\n>>> result = librispeech_test_clean.map(map_to_pred)\n\n>>> wer = load(\"wer\")\n>>> print(100 * wer.compute(references=result[\"reference\"], predictions=result[\"prediction\"]))\n3.432213777886737\n```\n\n## Long-Form Transcription\n\nThe Whisper model is intrinsically designed to work on audio samples of up to 30s in duration. However, by using a chunking \nalgorithm, it can be used to transcribe audio samples of up to arbitrary length. This is possible through Transformers \n[`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline) \nmethod. Chunking is enabled by setting `chunk_length_s=30` when instantiating the pipeline. With chunking enabled, the pipeline \ncan be run with batched inference. It can also be extended to predict sequence level timestamps by passing `return_timestamps=True`:\n\n```python\n>>> import torch\n>>> from transformers import pipeline\n>>> from datasets import load_dataset\n\n>>> device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n>>> pipe = pipeline(\n>>>   \"automatic-speech-recognition\",\n>>>   model=\"openai/whisper-small\",\n>>>   chunk_length_s=30,\n>>>   device=device,\n>>> )\n\n>>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n>>> sample = ds[0][\"audio\"]\n\n>>> prediction = pipe(sample.copy(), batch_size=8)[\"text\"]\n\" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.\"\n\n>>> # we can also return timestamps for the predictions\n>>> prediction = pipe(sample.copy(), batch_size=8, return_timestamps=True)[\"chunks\"]\n[{'text': ' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.',\n  'timestamp': (0.0, 5.44)}]\n```\n\nRefer to the blog post [ASR Chunking](https://huggingface.co/blog/asr-chunking) for more details on the chunking algorithm.\n\n## Fine-Tuning\n\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However, \nits predictive capabilities can be improved further for certain languages and tasks through *fine-tuning*. The blog \npost [Fine-Tune Whisper with \ud83e\udd17 Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step \nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\n\n### Evaluated Use\n\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only \u201cintended\u201d uses or to draw reasonable guidelines around what is or is not research.\n\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\n\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\n\n\n## Training Data\n\nThe models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages. \n\nAs discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.\n\n\n## Performance and Limitations\n\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf). \n\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\n\n\n## Broader Implications\n\nWe anticipate that Whisper models\u2019 transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box \u2013 their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\n\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\n\n\n### BibTeX entry and citation info\n```bibtex\n@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n```\n\n"
    },
    {
      "@id": "ark:59852/model-nvidia-parakeet-rnnt-0-6b-opcqceelwf",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "nvidia/parakeet-rnnt-0.6b",
      "description": "<style>",
      "author": "nvidia",
      "keywords": [
        "nemo",
        "automatic-speech-recognition",
        "speech",
        "audio",
        "Transducer",
        "FastConformer",
        "Conformer",
        "pytorch",
        "NeMo",
        "hf-asr-leaderboard",
        "en",
        "dataset:librispeech_asr",
        "dataset:fisher_corpus",
        "dataset:Switchboard-1",
        "dataset:WSJ-0",
        "dataset:WSJ-1",
        "dataset:National-Singapore-Corpus-Part-1",
        "dataset:National-Singapore-Corpus-Part-6",
        "dataset:vctk",
        "dataset:voxpopuli",
        "dataset:europarl",
        "dataset:multilingual_librispeech",
        "dataset:mozilla-foundation/common_voice_8_0",
        "dataset:MLCommons/peoples_speech",
        "arxiv:2305.05084",
        "license:cc-by-4.0",
        "model-index",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/librispeech_asr"
        },
        {
          "@id": "https://huggingface.co/datasets/fisher_corpus"
        },
        {
          "@id": "https://huggingface.co/datasets/Switchboard-1"
        },
        {
          "@id": "https://huggingface.co/datasets/WSJ-0"
        },
        {
          "@id": "https://huggingface.co/datasets/WSJ-1"
        },
        {
          "@id": "https://huggingface.co/datasets/National-Singapore-Corpus-Part-1"
        },
        {
          "@id": "https://huggingface.co/datasets/National-Singapore-Corpus-Part-6"
        },
        {
          "@id": "https://huggingface.co/datasets/vctk"
        },
        {
          "@id": "https://huggingface.co/datasets/voxpopuli"
        },
        {
          "@id": "https://huggingface.co/datasets/europarl"
        },
        {
          "@id": "https://huggingface.co/datasets/multilingual_librispeech"
        },
        {
          "@id": "https://huggingface.co/datasets/mozilla-foundation/common_voice_8_0"
        },
        {
          "@id": "https://huggingface.co/datasets/MLCommons/peoples_speech"
        }
      ],
      "usageInformation": "The model is available for use in the NeMo toolkit [3], and can be used as a pre-trained checkpoint for inference or for fine-tuning on another dataset.\n\n### Automatically instantiate the model\n\n```python\nimport nemo.collections.asr as nemo_asr\nasr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(model_name=\"nvidia/parakeet-rnnt-0.6b\")\n```\n\n### Transcribing using Python\nFirst, let's get a sample\n```\nwget https://dldata-public.s3.us-east-2.amazonaws.com/2086-149220-0033.wav\n```\nThen simply do:\n```\noutput = asr_model.transcribe(['2086-149220-0033.wav'])\nprint(output[0].text)\n```\n\n### Transcribing many audio files\n\n```shell\npython [NEMO_GIT_FOLDER]/examples/asr/transcribe_speech.py \n pretrained_name=\"nvidia/parakeet-rnnt-0.6b\" \n audio_dir=\"<DIRECTORY CONTAINING AUDIO FILES>\"\n```\n\n### Input\n\nThis model accepts 16000 Hz mono-channel audio (wav files) as input.\n\n### Output\n\nThis model provides transcribed speech as a string for a given audio sample.",
      "url": "https://huggingface.co/nvidia/parakeet-rnnt-0.6b",
      "license": "cc-by-4.0",
      "isPartOf": [],
      "README": "\n# Parakeet RNNT 0.6B (en)\n\n<style>\nimg {\n display: inline;\n}\n</style>\n\n[![Model architecture](https://img.shields.io/badge/Model_Arch-FastConformer--Transducer-lightgrey#model-badge)](#model-architecture)\n| [![Model size](https://img.shields.io/badge/Params-0.6B-lightgrey#model-badge)](#model-architecture)\n| [![Language](https://img.shields.io/badge/Language-en-lightgrey#model-badge)](#datasets)\n\n\n`parakeet-rnnt-0.6b` is an ASR model that transcribes speech in lower case English alphabet. This model is jointly developed by [NVIDIA NeMo](https://github.com/NVIDIA/NeMo) and [Suno.ai](https://www.suno.ai/) teams.\nIt is an XL version of FastConformer Transducer [1] (around 600M parameters) model.\nSee the [model architecture](#model-architecture) section and [NeMo documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer) for complete architecture details.\n\n## NVIDIA NeMo: Training\n\nTo train, fine-tune or play with the model you will need to install [NVIDIA NeMo](https://github.com/NVIDIA/NeMo). We recommend you install it after you've installed latest PyTorch version.\n```\npip install nemo_toolkit['all']\n``` \n\n## How to Use this Model\n\nThe model is available for use in the NeMo toolkit [3], and can be used as a pre-trained checkpoint for inference or for fine-tuning on another dataset.\n\n### Automatically instantiate the model\n\n```python\nimport nemo.collections.asr as nemo_asr\nasr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(model_name=\"nvidia/parakeet-rnnt-0.6b\")\n```\n\n### Transcribing using Python\nFirst, let's get a sample\n```\nwget https://dldata-public.s3.us-east-2.amazonaws.com/2086-149220-0033.wav\n```\nThen simply do:\n```\noutput = asr_model.transcribe(['2086-149220-0033.wav'])\nprint(output[0].text)\n```\n\n### Transcribing many audio files\n\n```shell\npython [NEMO_GIT_FOLDER]/examples/asr/transcribe_speech.py \n pretrained_name=\"nvidia/parakeet-rnnt-0.6b\" \n audio_dir=\"<DIRECTORY CONTAINING AUDIO FILES>\"\n```\n\n### Input\n\nThis model accepts 16000 Hz mono-channel audio (wav files) as input.\n\n### Output\n\nThis model provides transcribed speech as a string for a given audio sample.\n\n## Model Architecture\n\nFastConformer [1] is an optimized version of the Conformer model with 8x depthwise-separable convolutional downsampling. The model is trained in a multitask setup with a Transducer decoder (RNNT) loss. You may find more information on the details of FastConformer here: [Fast-Conformer Model](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/asr/models.html#fast-conformer).\n\n## Training\n\nThe NeMo toolkit [3] was used for training the models for over several hundred epochs. These model are trained with this [example script](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/asr_transducer/speech_to_text_rnnt_bpe.py) and this [base config](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/conf/fastconformer/fast-conformer_transducer_bpe.yaml).\n\nThe tokenizers for these models were built using the text transcripts of the train set with this [script](https://github.com/NVIDIA/NeMo/blob/main/scripts/tokenizers/process_asr_text_tokenizer.py).\n\n### Datasets\n\nThe model was trained on 64K hours of English speech collected and prepared by NVIDIA NeMo and Suno teams.\n\nThe training dataset consists of private subset with 40K hours of English speech plus 24K hours from the following public datasets:\n\n- Librispeech 960 hours of English speech\n- Fisher Corpus\n- Switchboard-1 Dataset\n- WSJ-0 and WSJ-1\n- National Speech Corpus (Part 1, Part 6)\n- VCTK\n- VoxPopuli (EN)\n- Europarl-ASR (EN)\n- Multilingual Librispeech (MLS EN) - 2,000 hour subset\n- Mozilla Common Voice (v7.0)\n- People's Speech  - 12,000 hour subset\n\n## Performance\n\nThe performance of Automatic Speech Recognition models is measuring using Word Error Rate. Since this dataset is trained on multiple domains and a much larger corpus, it will generally perform better at transcribing audio in general.\n\nThe following tables summarizes the performance of the available models in this collection with the Transducer decoder. Performances of the ASR models are reported in terms of Word Error Rate (WER%) with greedy decoding. \n\n|**Version**|**Tokenizer**|**Vocabulary Size**|**AMI**|**Earnings-22**|**Giga Speech**|**LS test-clean**|**SPGI Speech**|**TEDLIUM-v3**|**Vox Populi**|**Common Voice**|\n|---------|-----------------------|-----------------|---------------|---------------|------------|-----------|-----|-------|------|------|\n| 1.22.0  | SentencePiece Unigram | 1024 | 17.55 | 14.78 | 10.07 | 1.63 | 3.06 | 3.47 | 3.86 | 6.05 | 8.07 |\n\nThese are greedy WER numbers without external LM. More details on evaluation can be found at [HuggingFace ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)\n\n## NVIDIA Riva: Deployment\n\n[NVIDIA Riva](https://developer.nvidia.com/riva), is an accelerated speech AI SDK deployable on-prem, in all clouds, multi-cloud, hybrid, on edge, and embedded. \nAdditionally, Riva provides: \n\n* World-class out-of-the-box accuracy for the most common languages with model checkpoints trained on proprietary data with hundreds of thousands of GPU-compute hours \n* Best in class accuracy with run-time word boosting (e.g., brand and product names) and customization of acoustic model, language model, and inverse text normalization \n* Streaming speech recognition, Kubernetes compatible scaling, and enterprise-grade support.\n\nAlthough this model isn\u2019t supported yet by Riva, the [list of supported models is here](https://huggingface.co/models?other=Riva).  \nCheck out [Riva live demo](https://developer.nvidia.com/riva#demos). \n\n## References\n[1] [Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition](https://arxiv.org/abs/2305.05084)\n\n[2] [Google Sentencepiece Tokenizer](https://github.com/google/sentencepiece)\n\n[3] [NVIDIA NeMo Toolkit](https://github.com/NVIDIA/NeMo)\n\n[4] [Suno.ai](https://suno.ai/)\n\n[5] [HuggingFace ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)\n\n\n## Licence\n\nLicense to use this model is covered by the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/). By downloading the public and release version of the model, you accept the terms and conditions of the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/) license."
    },
    {
      "@id": "ark:59852/model-facebookai-xlm-roberta-large-1otynsf7qt",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "FacebookAI/xlm-roberta-large",
      "description": "XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).",
      "author": "FacebookAI",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "onnx",
        "safetensors",
        "xlm-roberta",
        "fill-mask",
        "exbert",
        "multilingual",
        "af",
        "am",
        "ar",
        "as",
        "az",
        "be",
        "bg",
        "bn",
        "br",
        "bs",
        "ca",
        "cs",
        "cy",
        "da",
        "de",
        "el",
        "en",
        "eo",
        "es",
        "et",
        "eu",
        "fa",
        "fi",
        "fr",
        "fy",
        "ga",
        "gd",
        "gl",
        "gu",
        "ha",
        "he",
        "hi",
        "hr",
        "hu",
        "hy",
        "id",
        "is",
        "it",
        "ja",
        "jv",
        "ka",
        "kk",
        "km",
        "kn",
        "ko",
        "ku",
        "ky",
        "la",
        "lo",
        "lt",
        "lv",
        "mg",
        "mk",
        "ml",
        "mn",
        "mr",
        "ms",
        "my",
        "ne",
        "nl",
        "no",
        "om",
        "or",
        "pa",
        "pl",
        "ps",
        "pt",
        "ro",
        "ru",
        "sa",
        "sd",
        "si",
        "sk",
        "sl",
        "so",
        "sq",
        "sr",
        "su",
        "sv",
        "sw",
        "ta",
        "te",
        "th",
        "tl",
        "tr",
        "ug",
        "uk",
        "ur",
        "uz",
        "vi",
        "xh",
        "yi",
        "zh",
        "arxiv:1911.02116",
        "license:mit",
        "autotrain_compatible",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "intendedUseCase": "You can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation, you should look at models like GPT2.",
      "usageInformation": "You can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='xlm-roberta-large')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n\n[{'score': 0.10563907772302628,\n  'sequence': \"Hello I'm a fashion model.\",\n  'token': 54543,\n  'token_str': 'fashion'},\n {'score': 0.08015287667512894,\n  'sequence': \"Hello I'm a new model.\",\n  'token': 3525,\n  'token_str': 'new'},\n {'score': 0.033413201570510864,\n  'sequence': \"Hello I'm a model model.\",\n  'token': 3299,\n  'token_str': 'model'},\n {'score': 0.030217764899134636,\n  'sequence': \"Hello I'm a French model.\",\n  'token': 92265,\n  'token_str': 'French'},\n {'score': 0.026436051353812218,\n  'sequence': \"Hello I'm a sexy model.\",\n  'token': 17473,\n  'token_str': 'sexy'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\nmodel = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-large\")",
      "contentUrl": "https://huggingface.co/FacebookAI/xlm-roberta-large/resolve/main/model.safetensors",
      "url": "https://huggingface.co/FacebookAI/xlm-roberta-large",
      "license": "mit",
      "isPartOf": [],
      "README": "\n# XLM-RoBERTa (large-sized model) \n\nXLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr). \n\nDisclaimer: The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nXLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. \n\nRoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.\n\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.\n\nThis way, the model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the XLM-RoBERTa model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation, you should look at models like GPT2.\n\n## Usage\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='xlm-roberta-large')\n>>> unmasker(\"Hello I'm a <mask> model.\")\n\n[{'score': 0.10563907772302628,\n  'sequence': \"Hello I'm a fashion model.\",\n  'token': 54543,\n  'token_str': 'fashion'},\n {'score': 0.08015287667512894,\n  'sequence': \"Hello I'm a new model.\",\n  'token': 3525,\n  'token_str': 'new'},\n {'score': 0.033413201570510864,\n  'sequence': \"Hello I'm a model model.\",\n  'token': 3299,\n  'token_str': 'model'},\n {'score': 0.030217764899134636,\n  'sequence': \"Hello I'm a French model.\",\n  'token': 92265,\n  'token_str': 'French'},\n {'score': 0.026436051353812218,\n  'sequence': \"Hello I'm a sexy model.\",\n  'token': 17473,\n  'token_str': 'sexy'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\nmodel = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-large\")\n\n# prepare input\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\n\n# forward pass\noutput = model(**encoded_input)\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1911-02116,\n  author    = {Alexis Conneau and\n               Kartikay Khandelwal and\n               Naman Goyal and\n               Vishrav Chaudhary and\n               Guillaume Wenzek and\n               Francisco Guzm{\\'{a}}n and\n               Edouard Grave and\n               Myle Ott and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {Unsupervised Cross-lingual Representation Learning at Scale},\n  journal   = {CoRR},\n  volume    = {abs/1911.02116},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1911.02116},\n  eprinttype = {arXiv},\n  eprint    = {1911.02116},\n  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=xlm-roberta-base\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n"
    },
    {
      "@id": "ark:59852/model-patrickjohncyh-fashion-clip-gkqtbrauypv",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "patrickjohncyh/fashion-clip",
      "description": "[![Youtube Video](https://img.shields.io/badge/youtube-video-red)](https://www.youtube.com/watch?v=uqRSc-KSA1Y) [![HuggingFace Model](https://img.shields.io/badge/HF%20Model-Weights-yellow)](https://huggingface.co/patrickjohncyh/fashion-clip) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Z1hAxBnWjF76bEi9KQ6CMBBEmI_FVDrW?usp=sharing) [![Medium Blog Post](https://raw.githubusercontent.com/aleen42/badges/master/src/medium.svg)](https://towardsdatascience.com/teaching-clip-some-fashion-3005ac3fdcc3) [![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://huggingface.co/spaces/vinid/fashion-clip-app)",
      "author": "patrickjohncyh",
      "keywords": [
        "transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "clip",
        "zero-shot-image-classification",
        "vision",
        "language",
        "fashion",
        "ecommerce",
        "en",
        "license:mit",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "hasBias": "We acknowledge certain limitations of FashionCLIP and expect that it inherits certain limitations and biases present in the original CLIP model. We do not expect our fine-tuning to significantly augment these limitations: we acknowledge that the fashion data we use makes explicit assumptions about the notion of gender as in \"blue shoes for a woman\" that inevitably associate aspects of clothing with specific people.\n\nOur investigations also suggest that the data used introduces certain limitations in FashionCLIP. From the textual modality, given that most captions derived from the Farfetch dataset are long, we observe that FashionCLIP may be more performant in longer queries than shorter ones. From the image modality, FashionCLIP is also biased towards standard product images (centered, white background).\n\nModel selection, i.e. selecting an appropariate stopping critera during fine-tuning, remains an open challenge. We observed that using loss on an in-domain (i.e. same distribution as test) validation dataset is a poor selection critera when out-of-domain generalization (i.e. across different datasets) is desired, even when the dataset used is relatively diverse and large.",
      "intendedUseCase": "We acknowledge certain limitations of FashionCLIP and expect that it inherits certain limitations and biases present in the original CLIP model. We do not expect our fine-tuning to significantly augment these limitations: we acknowledge that the fashion data we use makes explicit assumptions about the notion of gender as in \"blue shoes for a woman\" that inevitably associate aspects of clothing with specific people.\n\nOur investigations also suggest that the data used introduces certain limitations in FashionCLIP. From the textual modality, given that most captions derived from the Farfetch dataset are long, we observe that FashionCLIP may be more performant in longer queries than shorter ones. From the image modality, FashionCLIP is also biased towards standard product images (centered, white background).\n\nModel selection, i.e. selecting an appropariate stopping critera during fine-tuning, remains an open challenge. We observed that using loss on an in-domain (i.e. same distribution as test) validation dataset is a poor selection critera when out-of-domain generalization (i.e. across different datasets) is desired, even when the dataset used is relatively diverse and large.",
      "contentUrl": "https://huggingface.co/patrickjohncyh/fashion-clip/resolve/main/model.safetensors",
      "url": "https://huggingface.co/patrickjohncyh/fashion-clip",
      "license": "mit",
      "isPartOf": [],
      "README": "\n[![Youtube Video](https://img.shields.io/badge/youtube-video-red)](https://www.youtube.com/watch?v=uqRSc-KSA1Y) [![HuggingFace Model](https://img.shields.io/badge/HF%20Model-Weights-yellow)](https://huggingface.co/patrickjohncyh/fashion-clip) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Z1hAxBnWjF76bEi9KQ6CMBBEmI_FVDrW?usp=sharing) [![Medium Blog Post](https://raw.githubusercontent.com/aleen42/badges/master/src/medium.svg)](https://towardsdatascience.com/teaching-clip-some-fashion-3005ac3fdcc3) [![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://huggingface.co/spaces/vinid/fashion-clip-app)\n\n# Model Card: Fashion CLIP\n\nDisclaimer: The model card adapts the model card from [here](https://huggingface.co/openai/clip-vit-base-patch32).\n\n## Model Details\n\nUPDATE (10/03/23): We have updated the model! We found that [laion/CLIP-ViT-B-32-laion2B-s34B-b79K](https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K) checkpoint (thanks [Bin](https://www.linkedin.com/in/bin-duan-56205310/)!) worked better than original OpenAI CLIP on Fashion. We thus fine-tune a newer (and better!) version of FashionCLIP (henceforth FashionCLIP 2.0), while keeping the architecture the same. We postulate that the perofrmance gains afforded by `laion/CLIP-ViT-B-32-laion2B-s34B-b79K` are due to the increased training data (5x OpenAI CLIP data). Our [thesis](https://www.nature.com/articles/s41598-022-23052-9), however, remains the same -- fine-tuning `laion/CLIP` on our fashion dataset improved zero-shot perofrmance across our benchmarks. See the below table comparing weighted macro F1 score across models.\n\n\n| Model             | FMNIST        | KAGL          | DEEP          | \n| -------------     | ------------- | ------------- | ------------- |\n| OpenAI CLIP       | 0.66          | 0.63          | 0.45          |\n| FashionCLIP       | 0.74          | 0.67          | 0.48          |\n| Laion CLIP        | 0.78          | 0.71          | 0.58          |\n| FashionCLIP 2.0   | __0.83__          | __0.73__          | __0.62__          |\n\n---\n\nFashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by [OpenAI](https://github.com/openai/CLIP), we train FashionCLIP on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks. FashionCLIP was not developed for model deplyoment - to do so, researchers will first need to carefully study their capabilities in relation to the specific context they\u2019re being deployed within.\n\n### Model Date\n\nMarch 2023\n\n### Model Type\n\nThe model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained, starting from a pre-trained checkpoint, to maximize the similarity of (image, text) pairs via a contrastive loss on a fashion dataset containing 800K products.\n\n\n### Documents\n\n- [FashionCLIP Github Repo](https://github.com/patrickjohncyh/fashion-clip)\n- [FashionCLIP Paper](https://www.nature.com/articles/s41598-022-23052-9)\n\n\n## Data\n\nThe model was trained on (image, text) pairs obtained from the Farfecth dataset[^1 Awaiting official release.], an English dataset comprising over 800K fashion products, with more than 3K brands across dozens of object types. The image used for encoding is the standard product image, which is a picture of the item over a white background, with no humans. The text used is a concatenation of the _highlight_ (e.g., \u201cstripes\u201d, \u201clong sleeves\u201d, \u201cArmani\u201d) and _short description_ (\u201c80s styled t-shirt\u201d)) available in the Farfetch dataset.\n\n\n\n## Limitations, Bias and Fiarness\n\nWe acknowledge certain limitations of FashionCLIP and expect that it inherits certain limitations and biases present in the original CLIP model. We do not expect our fine-tuning to significantly augment these limitations: we acknowledge that the fashion data we use makes explicit assumptions about the notion of gender as in \"blue shoes for a woman\" that inevitably associate aspects of clothing with specific people.\n\nOur investigations also suggest that the data used introduces certain limitations in FashionCLIP. From the textual modality, given that most captions derived from the Farfetch dataset are long, we observe that FashionCLIP may be more performant in longer queries than shorter ones. From the image modality, FashionCLIP is also biased towards standard product images (centered, white background).\n\nModel selection, i.e. selecting an appropariate stopping critera during fine-tuning, remains an open challenge. We observed that using loss on an in-domain (i.e. same distribution as test) validation dataset is a poor selection critera when out-of-domain generalization (i.e. across different datasets) is desired, even when the dataset used is relatively diverse and large.\n\n\n## Citation\n```\n@Article{Chia2022,\n    title=\"Contrastive language and vision learning of general fashion concepts\",\n    author=\"Chia, Patrick John\n            and Attanasio, Giuseppe\n            and Bianchi, Federico\n            and Terragni, Silvia\n            and Magalh{\\~a}es, Ana Rita\n            and Goncalves, Diogo\n            and Greco, Ciro\n            and Tagliabue, Jacopo\",\n    journal=\"Scientific Reports\",\n    year=\"2022\",\n    month=\"Nov\",\n    day=\"08\",\n    volume=\"12\",\n    number=\"1\",\n    abstract=\"The steady rise of online shopping goes hand in hand with the development of increasingly complex ML and NLP models. While most use cases are cast as specialized supervised learning problems, we argue that practitioners would greatly benefit from general and transferable representations of products. In this work, we build on recent developments in contrastive learning to train FashionCLIP, a CLIP-like model adapted for the fashion industry. We demonstrate the effectiveness of the representations learned by FashionCLIP with extensive tests across a variety of tasks, datasets and generalization probes. We argue that adaptations of large pre-trained models such as CLIP offer new perspectives in terms of scalability and sustainability for certain types of players in the industry. Finally, we detail the costs and environmental impact of training, and release the model weights and code as open source contribution to the community.\",\n    issn=\"2045-2322\",\n    doi=\"10.1038/s41598-022-23052-9\",\n    url=\"https://doi.org/10.1038/s41598-022-23052-9\"\n}\n```"
    },
    {
      "@id": "ark:59852/model-sentence-transformers-msmarco-distilbert-base-tas-b-pq8dfgckr5",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "sentence-transformers/msmarco-distilbert-base-tas-b",
      "description": "This is a port of the [DistilBert TAS-B Model](https://huggingface.co/sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco) to [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and is optimized for the task of semantic search.",
      "author": "sentence-transformers",
      "keywords": [
        "sentence-transformers",
        "pytorch",
        "tf",
        "onnx",
        "safetensors",
        "openvino",
        "distilbert",
        "feature-extraction",
        "sentence-similarity",
        "transformers",
        "en",
        "dataset:ms_marco",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-embeddings-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/ms_marco"
        }
      ],
      "usageInformation": "Using this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer, util\n\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]",
      "contentUrl": "https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b/resolve/main/model.safetensors",
      "url": "https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# sentence-transformers/msmarco-distilbert-base-tas-b\n\nThis is a port of the [DistilBert TAS-B Model](https://huggingface.co/sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco) to [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 768 dimensional dense vector space and is optimized for the task of semantic search.\n\n\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer, util\n\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n\n#Load the model\nmodel = SentenceTransformer('sentence-transformers/msmarco-distilbert-base-tas-b')\n\n#Encode query and documents\nquery_emb = model.encode(query)\ndoc_emb = model.encode(docs)\n\n#Compute dot score between query and all document embeddings\nscores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n\n#Combine docs & scores\ndoc_score_pairs = list(zip(docs, scores))\n\n#Sort by decreasing score\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n\n#Output passages & scores\nfor doc, score in doc_score_pairs:\n    print(score, doc)\n```\n\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n#CLS Pooling - Take output from first token\ndef cls_pooling(model_output):\n    return model_output.last_hidden_state[:,0]\n\n#Encode text\ndef encode(texts):\n    # Tokenize sentences\n    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n\n    # Compute token embeddings\n    with torch.no_grad():\n        model_output = model(**encoded_input, return_dict=True)\n\n    # Perform pooling\n    embeddings = cls_pooling(model_output)\n\n    return embeddings\n\n\n# Sentences we want sentence embeddings for\nquery = \"How many people live in London?\"\ndocs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/msmarco-distilbert-base-tas-b\")\nmodel = AutoModel.from_pretrained(\"sentence-transformers/msmarco-distilbert-base-tas-b\")\n\n#Encode query and docs\nquery_emb = encode(query)\ndoc_emb = encode(docs)\n\n#Compute dot score between query and all document embeddings\nscores = torch.mm(query_emb, doc_emb.transpose(0, 1))[0].cpu().tolist()\n\n#Combine docs & scores\ndoc_score_pairs = list(zip(docs, scores))\n\n#Sort by decreasing score\ndoc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n\n#Output passages & scores\nfor doc, score in doc_score_pairs:\n    print(score, doc)\n```\n\n\n\n## Full Model Architecture\n```\nSentenceTransformer(\n  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: DistilBertModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\n```\n\n## Citing & Authors\n\nHave a look at: [DistilBert TAS-B Model](https://huggingface.co/sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco)"
    },
    {
      "@id": "ark:59852/model-openai-clip-vit-base-patch16-jflnu5oweg",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "openai/clip-vit-base-patch16",
      "description": "Disclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md).",
      "author": "openai",
      "keywords": [
        "transformers",
        "pytorch",
        "jax",
        "clip",
        "zero-shot-image-classification",
        "vision",
        "arxiv:2103.00020",
        "arxiv:1908.04913",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [],
      "hasBias": "We find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with \u2018Middle Eastern\u2019 having the highest accuracy (98.4%) and \u2018White\u2019 having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.",
      "intendedUseCase": "The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n\nThe primary intended users of these models are AI researchers.\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP\u2019s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.",
      "contentUrl": "https://huggingface.co/openai/clip-vit-base-patch16/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/openai/clip-vit-base-patch16",
      "isPartOf": [],
      "README": "# Model Card: CLIP\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md).\n\n\n## Model Details\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they\u2019re being deployed within.\n\n\n### Model Date\nJanuary 2021\n\n\n### Model Type\nThe base model uses a ViT-B/16 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.\n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n- [Blog Post](https://openai.com/blog/clip/)\n- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n\n\n### Use with Transformers\n```python3\nfrom PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n\n#### Primary intended uses\nThe primary intended users of these models are AI researchers.\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n\n### Out-of-Scope Use Cases\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP\u2019s performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n## Data\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n\n### Data Mission Statement\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n## Performance and Limitations\n\n\n### Performance\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n\n## Limitations\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n\n### Bias and Fairness\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with \u2018Middle Eastern\u2019 having the highest accuracy (98.4%) and \u2018White\u2019 having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.\n\n\n## Feedback\n\n\n### Where to send questions or comments about the model\nPlease use [this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)\n"
    },
    {
      "@id": "ark:59852/model-sentence-transformers-paraphrase-minilm-l6-v2-sxiljz6i97",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "sentence-transformers/paraphrase-MiniLM-L6-v2",
      "description": "This is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.",
      "author": "sentence-transformers",
      "keywords": [
        "sentence-transformers",
        "pytorch",
        "tf",
        "onnx",
        "safetensors",
        "openvino",
        "bert",
        "feature-extraction",
        "sentence-similarity",
        "transformers",
        "arxiv:1908.10084",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-embeddings-inference",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [],
      "usageInformation": "Using this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch",
      "contentUrl": "https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2/resolve/main/model.safetensors",
      "url": "https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2",
      "license": "apache-2.0",
      "isPartOf": [],
      "README": "\n# sentence-transformers/paraphrase-MiniLM-L6-v2\n\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/paraphrase-MiniLM-L6-v2')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n\n\n## Full Model Architecture\n```\nSentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n)\n```\n\n## Citing & Authors\n\nThis model was trained by [sentence-transformers](https://www.sbert.net/). \n        \nIf you find this model helpful, feel free to cite our publication [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084):\n```bibtex \n@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n```"
    },
    {
      "@id": "ark:59852/model-emilyalsentzer-bioclinicalbert-yj9xpldecaa",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "emilyalsentzer/Bio_ClinicalBERT",
      "description": "The [Publicly Available Clinical BERT Embeddings](https://arxiv.org/abs/1904.03323) paper contains four unique clinicalBERT models: initialized with BERT-Base (`cased_L-12_H-768_A-12`) or BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`) & trained on either all MIMIC notes or only discharge summaries.",
      "author": "emilyalsentzer",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "bert",
        "fill-mask",
        "en",
        "arxiv:1904.03323",
        "arxiv:1901.08746",
        "license:mit",
        "endpoints_compatible",
        "deploy:azure",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "pytorch-bin",
      "trainingDataset": [],
      "usageInformation": "Load the model via the transformers library:\n```\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\nmodel = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n```",
      "contentUrl": "https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/pytorch_model.bin",
      "url": "https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT",
      "license": "mit",
      "isPartOf": [],
      "README": "\n# ClinicalBERT - Bio + Clinical BERT Model\n\nThe [Publicly Available Clinical BERT Embeddings](https://arxiv.org/abs/1904.03323) paper contains four unique clinicalBERT models: initialized with BERT-Base (`cased_L-12_H-768_A-12`) or BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`) & trained on either all MIMIC notes or only discharge summaries. \n\nThis model card describes the Bio+Clinical BERT model, which was initialized from [BioBERT](https://arxiv.org/abs/1901.08746) & trained on all MIMIC notes. \n\n## Pretraining Data\nThe `Bio_ClinicalBERT` model was trained on all notes from [MIMIC III](https://www.nature.com/articles/sdata201635), a database containing electronic health records from ICU patients at the Beth Israel Hospital in Boston, MA. For more details on MIMIC, see [here](https://mimic.physionet.org/). All notes from the `NOTEEVENTS` table were included (~880M words).\n\n## Model Pretraining \n\n### Note Preprocessing\nEach note in MIMIC was first split into sections using a rules-based section splitter (e.g. discharge summary notes were split into \"History of Present Illness\", \"Family History\", \"Brief Hospital Course\", etc. sections). Then each section was split into sentences using SciSpacy (`en core sci md` tokenizer). \n\n### Pretraining Procedures\nThe model was trained using code from [Google's BERT repository](https://github.com/google-research/bert) on a GeForce GTX TITAN X 12 GB GPU. Model parameters were initialized with BioBERT (`BioBERT-Base v1.0 + PubMed 200K + PMC 270K`).\n\n### Pretraining Hyperparameters\nWe used a batch size of 32, a maximum sequence length of 128, and a learning rate of 5 \u00b7 10\u22125 for pre-training our models. The models trained on all MIMIC notes  were trained for 150,000 steps. The dup factor for duplicating input data with different masks was set to 5. All other default parameters were used (specifically, masked language model probability = 0.15\nand max predictions per sequence = 20).\n\n## How to use the model\n\nLoad the model via the transformers library:\n```\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\nmodel = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n```\n\n## More Information\n\nRefer to the original paper, [Publicly Available Clinical BERT Embeddings](https://arxiv.org/abs/1904.03323) (NAACL Clinical NLP Workshop 2019) for additional details and performance on NLI and NER tasks.\n\n## Questions?\n\nPost a Github issue on the [clinicalBERT repo](https://github.com/EmilyAlsentzer/clinicalBERT) or email ealsentzer@stanford.edu with any questions.\n\n"
    },
    {
      "@id": "ark:59852/model-w11wo-indonesian-roberta-base-posp-tagger-s6vypmn88n",
      "@type": "https://w3id.org/EVI#MLModel",
      "name": "w11wo/indonesian-roberta-base-posp-tagger",
      "description": "<!-- This model card has been generated automatically according to the information the Trainer had access to. You",
      "author": "w11wo",
      "keywords": [
        "transformers",
        "pytorch",
        "tf",
        "tensorboard",
        "safetensors",
        "roberta",
        "token-classification",
        "generated_from_trainer",
        "ind",
        "dataset:indonlu",
        "base_model:flax-community/indonesian-roberta-base",
        "base_model:finetune:flax-community/indonesian-roberta-base",
        "license:mit",
        "model-index",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
      ],
      "version": "1.0",
      "framework": "pytorch",
      "modelFormat": "safetensors",
      "trainingDataset": [
        {
          "@id": "https://huggingface.co/datasets/indonlu"
        }
      ],
      "intendedUseCase": "More information needed",
      "baseModel": "flax-community/indonesian-roberta-base",
      "contentUrl": "https://huggingface.co/w11wo/indonesian-roberta-base-posp-tagger/resolve/main/model.safetensors",
      "url": "https://huggingface.co/w11wo/indonesian-roberta-base-posp-tagger",
      "license": "mit",
      "isPartOf": [],
      "README": "\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n# indonesian-roberta-base-posp-tagger\n\nThis model is a fine-tuned version of [flax-community/indonesian-roberta-base](https://huggingface.co/flax-community/indonesian-roberta-base) on the indonlu dataset.\nIt achieves the following results on the evaluation set:\n- Loss: 0.1395\n- Precision: 0.9625\n- Recall: 0.9625\n- F1: 0.9625\n- Accuracy: 0.9625\n\n## Model description\n\nMore information needed\n\n## Intended uses & limitations\n\nMore information needed\n\n## Training and evaluation data\n\nMore information needed\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 2e-05\n- train_batch_size: 16\n- eval_batch_size: 16\n- seed: 42\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- num_epochs: 10\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Precision | Recall | F1     | Accuracy |\n|:-------------:|:-----:|:----:|:---------------:|:---------:|:------:|:------:|:--------:|\n| No log        | 1.0   | 420  | 0.2254          | 0.9313    | 0.9313 | 0.9313 | 0.9313   |\n| 0.4398        | 2.0   | 840  | 0.1617          | 0.9499    | 0.9499 | 0.9499 | 0.9499   |\n| 0.1566        | 3.0   | 1260 | 0.1431          | 0.9569    | 0.9569 | 0.9569 | 0.9569   |\n| 0.103         | 4.0   | 1680 | 0.1412          | 0.9605    | 0.9605 | 0.9605 | 0.9605   |\n| 0.0723        | 5.0   | 2100 | 0.1408          | 0.9635    | 0.9635 | 0.9635 | 0.9635   |\n| 0.051         | 6.0   | 2520 | 0.1408          | 0.9642    | 0.9642 | 0.9642 | 0.9642   |\n| 0.051         | 7.0   | 2940 | 0.1510          | 0.9635    | 0.9635 | 0.9635 | 0.9635   |\n| 0.0368        | 8.0   | 3360 | 0.1653          | 0.9645    | 0.9645 | 0.9645 | 0.9645   |\n| 0.0277        | 9.0   | 3780 | 0.1664          | 0.9644    | 0.9644 | 0.9644 | 0.9644   |\n| 0.0231        | 10.0  | 4200 | 0.1668          | 0.9646    | 0.9646 | 0.9646 | 0.9646   |\n\n\n### Framework versions\n\n- Transformers 4.37.2\n- Pytorch 2.2.0+cu118\n- Datasets 2.16.1\n- Tokenizers 0.15.1\n"
    }
  ]
}